<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[opencv 图片基本操作]]></title>
      <url>/2017/07/23/opencv1/</url>
      <content type="html"><![CDATA[<p>对图片像素的操作实际是对图片读入后返回的numpy数组进行操作，记住数组格式是[height, width, channels]其中channels是BGR顺序排列。这些对像素的操作归到了对numpy数组的操作。考虑到速度可以使用numpy数组的img.item()或者img.itemset()方法来设置。在numpy中对数组的img.shape，img.size，img.dtype 操作都是可用的，且很有用。<br><a id="more"></a> </p>
<h3 id="图片加法"><a href="#图片加法" class="headerlink" title="图片加法"></a>图片加法</h3><p>图片数组(uint8)之间的加法，最好使用cv2.add(x,y)因为图片数组一般的大小在0到255之间，如果加的结果操作了255那么cv2.add会取255，如果是一般的numpy数组加法得到的就是4了。<br>图片加法还存在融合的形式，两个图片按照比例叠加在一起<code>cv2.addWeighted(img1, 0.7, img2, 0.3, 0)</code>最后是常数项$dst=\alpha <em> img1 + \beta </em> img2 + \gamma$。</p>
<h3 id="程序处理效率"><a href="#程序处理效率" class="headerlink" title="程序处理效率"></a>程序处理效率</h3><p>使用cv2.getTickCount()来获取开始和结束点，然后除频率可以得到程序执行时间。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">e1 = cv2.getTickCount()</div><div class="line">#some processing</div><div class="line">e2 = cv2.getTickCount()</div><div class="line">t = (e2-e1)/cv2.getTickFrequency()</div></pre></td></tr></table></figure>
<h3 id="图片尺寸改变"><a href="#图片尺寸改变" class="headerlink" title="图片尺寸改变"></a>图片尺寸改变</h3><p>使用<code>res = cv2.resize(img,None,fx=2, fy=2, interpolation = cv2.INTER_CUBIC)</code>或者<code>res = cv2.resize(img,(2*width, 2*height), interpolation = cv2.INTER_CUBIC)</code>都是可以将图片放大为原有尺寸的两倍。</p>
<h3 id="特征提取和描述"><a href="#特征提取和描述" class="headerlink" title="特征提取和描述"></a>特征提取和描述</h3><p>对于一个图片它独一无二的地方一般是它的边边角角，像是以其连成一块的色彩往往不能表示这个图片。接下来我们要做的就是提取图片的特征并给予它恰当的描述。这样根据特征和描述就可以对两张图片进行匹配等操作。       </p>
<h4 id="Harris-Corner-Detection"><a href="#Harris-Corner-Detection" class="headerlink" title="Harris Corner Detection"></a>Harris Corner Detection</h4><p>在一个图片上将大小固定的窗口在各个方向上移动如果窗口灰度出现大的变化，则这个很可能就是角点。<br>公式：$E(u,v)=\sum _(x,y) w(x,y)(I(x+u,y+v)-I(x,y))^2$<br>在opencv中<code>cv2.cornerHarris()</code>实现了这个功能，参数有。</p>
<ul>
<li><strong>img</strong>-输入图片，需要是float32类型的灰度图 </li>
<li><strong>blockSize</strong>-角点领区大小     </li>
<li><strong>ksize</strong>-Sobel的孔径参数(aperture parameter)，也就是Sobel核的半径，如1、3、5、7，用来求导的。</li>
<li><strong>k</strong>-k是一个经验参数用来调节阀值，一般取值在0.04-0.06之间。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">import cv2</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">filename = &apos;chessboard.jpg&apos;</div><div class="line">img = cv2.imread(filename)</div><div class="line">gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)</div><div class="line"></div><div class="line">gray = np.float32(gray)</div><div class="line">dst = cv2.cornerHarris(gray,2,3,0.04)</div><div class="line"></div><div class="line">#这里通过膨胀可以让点看起来更大些</div><div class="line">dst = cv2.dilate(dst,None)</div><div class="line"></div><div class="line">#选取一个dst中最大的点作为基准选择具体点设置为红色来显示。</div><div class="line">img[dst&gt;0.01*dst.max()]=[0,0,255]</div><div class="line"></div><div class="line">cv2.imshow(&apos;dst&apos;,img)</div><div class="line">if cv2.waitKey(0) &amp; 0xff == 27:</div><div class="line">    cv2.destroyAllWindows()</div></pre></td></tr></table></figure>
<p>优点：具有旋转不变性。<br>缺点：不具有尺度不变性。 </p>
<p><strong>注意</strong>：在显示一个图片定位的角点的时候一般会设置通道颜色。对于matplotlib是RGB的顺序，对于cv2是BGR的顺序。所以可能出现颜色不一致的情况。</p>
<h4 id="Shi-Tomasi-Corner-Detector"><a href="#Shi-Tomasi-Corner-Detector" class="headerlink" title="Shi-Tomasi Corner Detector"></a>Shi-Tomasi Corner Detector</h4><p>Shi-Tomasi Corner Detector是对Harris的优化，将以前的$R=\lambda _1\lambda _2 - k(\lambda _1+\lambda _2)^2$修改为$R=min(\lambda _1, \lambda _2)$。优化后可以提供更好的结果。<br>对应函数是<code>cv2.goodFeaturesToTrack()</code>它的参数是一张灰度图片，指定抽取的角点数，最小角点阀值(大于这个数字才进入候选)，角点最小距离(某个角点最小距离范围内如果有更强的角点则删除本角点)</p>
<p><img src="image/opencv/shi-tomasi.jpg" height="50%" width="50%/"></p>
<p><a href="http://www.cnblogs.com/ronny/p/4009425.html" target="_blank" rel="external">参考 Harris角点</a><br><a href="http://blog.csdn.net/lql0716/article/details/52628959" target="_blank" rel="external">参考  Harris角点检测(Python-OpenCV)</a></p>
]]></content>
      
        <categories>
            
            <category> tool </category>
            
        </categories>
        
        
        <tags>
            
            <tag> basic skill </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[dlib & opencv]]></title>
      <url>/2017/07/22/dlib-opencv/</url>
      <content type="html"><![CDATA[<p>安装dlib<br>下载源文件：<code>git clone https://github.com/davisking/dlib.git</code><br>进行编译：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">cd dlib/examples</div><div class="line">mkdir build</div><div class="line">cd build</div><div class="line">cmake .. -DUSE_SSE4_INSTRUCTIONS=ON</div><div class="line">cmake --build . --config Release</div></pre></td></tr></table></figure></p>
<a id="more"></a> 
<p>安装：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cd dlib</div><div class="line">sudo python setup.py install</div></pre></td></tr></table></figure>
<p>运行程序出现问题：<code>AttributeError: module &#39;dlib&#39; has no attribute &#39;image_window&#39;</code>。这个是因为dlib在mac下调用的显示窗口不存在。所以按照下面参考文档提示按照<a href="https://www.xquartz.org/" target="_blank" rel="external">X11</a>，安装完成后建立一个软链接，再重启电脑。完成源码编译以及python dlib安装即可。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cd /usr/local/opt</div><div class="line">ln -s /opt/X11 X11</div></pre></td></tr></table></figure>
<p><a href="http://noahsnail.com/2016/12/10/2016-12-11-Mac%E4%B8%8Bdlib%E5%AE%89%E8%A3%85/" target="_blank" rel="external">参考:Mac下dlib安装</a></p>
<h2 id="dlib-python接口"><a href="#dlib-python接口" class="headerlink" title="dlib python接口"></a>dlib python接口</h2><p><code>detector = dlib.get_frontal_face_detector()</code>:用来返回一个脸部提取器。<br><code>dets = detector(img, 1)</code>:脸部提取器本身在使用的时候需要传入一个<code>numpy</code>的<code>narray</code>，图像可以是8bit 灰度图或者是RGB图像(height,width,channels)。还有一个参数是<code>upsample_num_times&gt;=0</code>设置上采样倍数，可以用来放大图片以便侦测更小的头像。一般设置为1就可以了，表示方法一倍进行侦测。<br><code>dets = detector(img, 1)</code>:返回的dets是一个<code>rectangle</code>长方形集合<code>rectangles</code>这个集合中包含的对象是探测到的头像信息，比如头像位置在整个图片的上rectangle.top()下rectangle.bottom()左rectangle.left()右rectangle.right()点。通过这样<code>for i, d in enumerate(dets):</code>可以遍历探测到的所有人脸。<br><strong>显示图像和标出人脸</strong><br><code>win = dlib.image_window()</code>:用来创建一个图片显示窗口。<br><code>win.clear_overlay()</code>:清楚window上的覆盖，比如人脸红框。<br><code>win.set_image(img)</code>:加载并显示图片。<br><code>win.add_overlay(dets)</code>:将定位到人脸的方框，加载到window中去。        <code>dlib.hit_enter_to_continue()</code>:控制图片为显示状态，直到按下任意键再继续运行下面的程序。<br>如果想知道具体头像的得分可以使用<code>dets, scores, idx = detector.run(img, 1, -1)</code>其中第三个参数表示你设定的输出得分阀值，大于这个值的内容才会输出。返回值idx的意思不太明晰，可能表示匹配到头像的子提取器索引。<br><strong>获取人脸重要特征</strong><br><code>predictor = dlib.shape_predictor(face_predictor_model)</code>输入一个人脸特征模型，然后构造一个人脸特征提取器。<br><code>shape = predictor(img, d)</code>使用获取的图像，以及d图像中某个人脸的位置信息作为输入，输出为人脸特征模型下输出的人脸特征信息。     </p>
<p><a href="http://dlib.net/compile.html" target="_blank" rel="external">参考文档：Dlib.net</a></p>
<h2 id="OpenCV"><a href="#OpenCV" class="headerlink" title="OpenCV"></a>OpenCV</h2><p><a href="http://www.memorycrash.cn/2017/07/20/facerec/#more" target="_blank" rel="external">OpenCV 安装参考</a>。<br>在OpenCV的数组结构被转换为了Numpy的数组结构，所以Numpy的数组操作可以使用到OpenCV数组结构上。<br><code>img = cv2.imread(&#39;xx.jpg&#39;, cv2.IMREAD_COLOR)</code>：读入文件使用imread它的两个参数分别是读入图像文件路径以及显示文件的颜色模式。默认是<code>cv2.IMREAD_COLOR</code>将图片转换为RGB三通道，还有灰色模式<code>cv2.IMREAD_GRAYSCALE</code>将图片转换为单通道灰度图和<code>cv2.IMREAD_UNCHANGED</code>保留原始信息不作转话。     </p>
<h3 id="显示图片"><a href="#显示图片" class="headerlink" title="显示图片"></a>显示图片</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">cv2.imshow(&apos;image&apos;, img)</div><div class="line">cv2.waitKey(0)</div><div class="line">cv2.destroyAllWindows()</div></pre></td></tr></table></figure>
<p>imshow函数需要指定显示窗口名称，以及一个imread后返回的图片数组。<br>waitKey函数表示等待按键，参数0表示一致等待直到有任意键按下后再继续执行程序。这个函数返回的是捕捉到的键的ASCII值，可以通过判断返回值来识别按下的是什么键。ESC是27。字符和ASCII之间的转换可以通过ord()函数来进行。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">k = cv2.waitKey(0)</div><div class="line">if k == 27:</div><div class="line">  cv2.destroyAllWindows()</div><div class="line">elif k == ord(&apos;s&apos;):  </div><div class="line">    cv2.destroyAllWindows()</div><div class="line">    cv2.imwrite(&apos;./data/graygakki.jpg&apos;, gray_img)</div><div class="line">```    </div><div class="line"></div><div class="line">注意：</div><div class="line">1、waitKey识别按键时，必须将光标移动到图片上再按键。在终端界面进行按键识别不了。      </div><div class="line">2、如果是64位的机器需要如此获取捕捉的键`k = cv2.waitKey(0) &amp; 0xFF`</div><div class="line">3、`waitKey(35)`表示等待35ms看是否有按键，如果这个期间没有按键就继续执行下面的代码。     </div><div class="line"></div><div class="line"></div><div class="line">destroyAllWindows表示释放所有窗口资源，可以使用destroyWindow()输入窗口名称来指定释放的窗口资源。     </div><div class="line">在imshow函数使用前添加`cv2.namedWindow(&apos;image&apos;,cv2.WINDOW_NORMAL)`使得显示的图片能够调整大小，这个对于大的图片比较有用。     </div><div class="line"></div><div class="line">### 保存图片    </div><div class="line">`cv2.imwrite(&apos;graygakki.jpg&apos;, img)`即可。</div><div class="line"></div><div class="line">[参考文档：OpenCV-Python Tutorials](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_tutorials.html)</div><div class="line">小技巧：    </div><div class="line">在linux的系统可以通过`display xx.jpg`快速从终端打开一个图片。     </div><div class="line">在maxos的系统可以通过`open xx.jpg`快速从终端打开一个图片。   </div><div class="line">在maxos下使用`bzip2 -d`可以解压bz结尾的压缩文件。[参考](http://blog.sina.com.cn/s/blog_446247d30101d1x5.html)。       </div><div class="line">获取特点文件夹下的文件可以使用`import glob`非常方便。`for f in glob.glob(&apos;./data/*.jpg&apos;):`这样就可以获取所有的文件。</div><div class="line"></div><div class="line">### 视频显示    </div><div class="line">在显示视频的时候会使用到`cap = cv2.VideoCapture(para)`函数，它的参数可以是某个视频文件的名称或者是数字0或者1，2，3等。这些数字表示程序运行设备中摄像头装置标号，如果只有一个摄像装置那么可以使用0来表示它。     </div><div class="line">`ret, frame = cap.read()`:一帧一帧的读取视频，返回ret表示读取是否成功True／False，返回frame表示具体的帧，可以使用`cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)`将图片转换为灰色。灰色显示耗费少点。     </div><div class="line">`cap.release()`:记得不需要的时候释放视频对象。      </div><div class="line">`cap.set(propld, value)`可以设置视频显示的时候一些特性，具体[参考](http://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-get)比如`cap.set(3, 320)`设置视频显示宽度为320像素。</div><div class="line"></div><div class="line">### 视频保存</div><div class="line">视频保存会使用到`out = cv2.VideoWriter(&apos;output.avi&apos;,fourcc, 20.0, (640,480))`函数，其中第一个参数是保存文件，第二个参数是编码格式`fourcc = cv2.VideoWriter_fourcc(*&apos;XVID&apos;)`，第三个参数是每秒帧数，第四个参数是视频图像尺寸。第五个参数是设置颜色，默认是False表示灰色，设置为True表示彩色。使用`out.write(frame)`可以写入文件。`frame = cv2.flip(frame,0)`其中0是在垂直方向上翻转图像，1是水平方向翻转图像。视频保存完毕释放资源`out.release()`。      </div><div class="line"></div><div class="line">### 绘制</div><div class="line">要绘制一些东西可以使用`cv2.line()`绘制线条,`cv2.circle()`绘制圆形，`cv2.rectangle()`绘制矩形，`cv2.ellipse()`绘制椭圆，`cv2.putText()`设置文本。这些函数有些共同的参数，比如img 表示绘制的画布，color线条颜色BGR使用元组来表示(255,0,0)表示蓝色，全0是黑色，全255是白色。thickness：表示线宽单位是pixel，如果填-1表示填充封闭图像。lineType：表示线的类型。下面是一个绘制文字的例子。</div></pre></td></tr></table></figure>
<p>font = cv2.FONT_HERSHEY_SIMPLEX<br>cv2.putText(img,’OpenCV’,(10,500), font, 4,(255,255,255),2,cv2.LINE_AA)<br>```</p>
<h3 id="调用鼠标"><a href="#调用鼠标" class="headerlink" title="调用鼠标"></a>调用鼠标</h3><p>鼠标的操作是以回调函数的形式，回调函数的传入参数格式都是固定的。这里是<code>draw_circle(event, x, y, flags, param)</code>其中x和y是鼠标时间发生时的坐标。通过<code>events = [i for i in dir(cv2) if &#39;EVENT&#39; in i]</code>来查看有哪些事件。使用函数<code>cv2.setMouseCallback(&#39;image&#39;, draw_circle)</code>来将回调函数和图像窗口绑定。</p>
]]></content>
      
        <categories>
            
            <category> tool </category>
            
        </categories>
        
        
        <tags>
            
            <tag> basic skill </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[人脸识别简单实现]]></title>
      <url>/2017/07/20/facerec/</url>
      <content type="html"><![CDATA[<p>首先安装<code>dlib</code>来进行人脸图像的识别截取后续再使用这些数据进行神经网络人脸的分类训练。<br>安装<code>dlib</code>前先<code>sudo apt-get install libboost-python-dev cmake</code>安装配套文件。<br>然后使用<code>sudo pip3 install dlib</code>即可。<br><a id="more"></a> </p>
<h3 id="在mac下安装opencv"><a href="#在mac下安装opencv" class="headerlink" title="在mac下安装opencv"></a>在mac下安装opencv</h3><p>安装opencv使用：<code>sudo pip3 install opencv-python</code>，导入的时候为<code>import cv2</code>。<br>在mac上安装可以参考此<a href="http://www.pyimagesearch.com/2017/03/27/how-to-install-dlib/" target="_blank" rel="external">文章</a><br>简单说如下来安装     </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">brew install cmake</div><div class="line">brew install boost</div><div class="line">brew install boost-python --with-python3</div><div class="line">pip3 install dlib</div></pre></td></tr></table></figure>
<p>在mac上安装opencv需要先在系统中安装。<br><code>brew tap homebrew/science</code><br><code>brew install opencv3 --with-python3 --with-contrib --without-python</code><br>安装完成后添加执行此命令添加路径信息。      </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo &apos;export PATH=&quot;/usr/local/opt/opencv3/bin:$PATH&quot;&apos; &gt;&gt; ~/.bash_profile</div></pre></td></tr></table></figure>
<p>这样安装后仍然在<code>import cv2</code>的时候失败，然后将<code>cp cv2.cpython-35m-darwin.so numpy in /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages</code>执行，可解决问题。       </p>
<p><strong>注意</strong>：在python3上安装opencv需要添加参数<code>--without-python</code>否则会出现错误<code>Error: opencv3: Does not support building both Python 2 and 3 wrappers</code>。<code>--with-contrib</code>指代新加不稳定的库，如果不需要可以不添加此参数。</p>
<h3 id="在ubuntu下安装opencv"><a href="#在ubuntu下安装opencv" class="headerlink" title="在ubuntu下安装opencv"></a>在ubuntu下安装opencv</h3><p>安装依赖文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install --assume-yes build-essential cmake git</div><div class="line">sudo apt-get install --assume-yes pkg-config unzip ffmpeg qtbase5-dev python-dev python3-dev python-numpy python3-numpy</div><div class="line">sudo apt-get install --assume-yes libopencv-dev libgtk-3-dev libdc1394-22 libdc1394-22-dev libjpeg-dev libpng12-dev libtiff5-dev libjasper-dev</div><div class="line">sudo apt-get install --assume-yes libavcodec-dev libavformat-dev libswscale-dev libxine2-dev libgstreamer0.10-dev libgstreamer-plugins-base0.10-dev</div><div class="line">sudo apt-get install --assume-yes libv4l-dev libtbb-dev libfaac-dev libmp3lame-dev libopencore-amrnb-dev libopencore-amrwb-dev libtheora-dev</div><div class="line">sudo apt-get install --assume-yes libvorbis-dev libxvidcore-dev v4l-utils python-vtk</div><div class="line">sudo apt-get install --assume-yes liblapacke-dev libopenblas-dev checkinstall</div><div class="line">sudo apt-get install --assume-yes libgdal-dev</div></pre></td></tr></table></figure></p>
<p>下载源码:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/opencv/opencv.git</div><div class="line">git clone https://github.com/opencv/opencv_contrib.git</div></pre></td></tr></table></figure>
<p>完成编译配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/usr/local PYTHON3_EXECUTABLE = /usr/bin/python3 PYTHON_INCLUDE_DIR = /usr/include/python3.5 PYTHON_INCLUDE_DIR2 = /usr/include/x86_64-linux-gnu/python3.5m PYTHON_LIBRARY = /usr/lib/x86_64-linux-gnu-python3/libpython3.5m.so PYTHON3_NUMPY_INCLUDE_DIRS = /usr/local/lib/python3.5/dist-packages/numpy/core/include/ OPENCV_EXTRA_MODULES_PATH=/home/opencv_contrib-3.2.0/modules WITH_CUDA=ON WITH_CUBLAS=ON DCUDA_NVCC_FLAGS=&quot;-D_FORCE_INLINES&quot; CUDA_ARCH_BIN=&quot;6.1&quot; CUDA_ARCH_PTX=&quot;&quot; CUDA_FAST_MATH=ON WITH_TBB=ON WITH_V4L=ON WITH_GTK=ON WITH_OPENGL=ON BUILD_EXAMPLES=ON ..</div></pre></td></tr></table></figure>
<p>这里的<code>CUDA_ARCH_BIN=&quot;6.1&quot;</code>信息可以在nvidia的网站<a href="https://developer.nvidia.com/cuda-gpus" target="_blank" rel="external">查看</a></p>
<p>执行编译:<code>sudo make -j4</code>这的j4表示4核cpu</p>
<p>执行安装:<code>sudo make install</code>，完整完成后便可以使用。</p>
<p><a href="http://www.cnblogs.com/arkenstone/p/6490017.html" target="_blank" rel="external">参考:Ubuntu16.04下安装OpenCV3.2.0</a><br><a href="https://github.com/BVLC/caffe/wiki/OpenCV-3.2-Installation-Guide-on-Ubuntu-16.04" target="_blank" rel="external">参考：OpenCV 3.2 Installation Guide on Ubuntu 16.04</a></p>
<h3 id="获取个人图片"><a href="#获取个人图片" class="headerlink" title="获取个人图片"></a>获取个人图片</h3><p>使用编写的程序获取个人头像，然后再通过对图像的色调修改和对比度或者左右翻转来增加图片的量直到图片有1万张。在个人的macbook pro上执行获取个人图像和将图像随机进行色调，对比度，等的变化特别消耗时间。估计在10个小时范围。在ubuntu下可以使用<code>display xxx.jpg</code>来查看图像。</p>
<p>在程序中显示图片，使用<code>img = cv2.imread(&#39;path&#39;)</code>读取图片，创建窗口<code>cv2.namedWindow(&#39;Image&#39;)</code>显示图片<code>cv2.imshow(&#39;Image&#39;,img)</code>。等待<code>cv2.waitKey(0)</code>，显示完毕后关闭窗口<code>cv2.destroyAllWindows()</code>。</p>
<h3 id="生成tfrecord文件"><a href="#生成tfrecord文件" class="headerlink" title="生成tfrecord文件"></a>生成tfrecord文件</h3><p>将训练图像转换为tfrecord文件。</p>
<h3 id="读取tfrecord文件"><a href="#读取tfrecord文件" class="headerlink" title="读取tfrecord文件"></a>读取tfrecord文件</h3><p>读取tfrecords文件并做简单处理入将数值范围由0到255缩小为0到1。读取tfrecords中会选择使用<code>tf.train.shuffle_batch([image, label], batch_size, capacity, min_after_dequeue)</code>这里<code>[image, label]</code>样本以及其标签数据，batch_size表示输出的一个批次数据，capacity表示队列中数据的最大数，min_after_dequeue表示这个队列开始重新添加数据的最小值。</p>
<p>在初始化数据的时候似乎需要<code>tf.local_variables_initializer()</code>。<br>启动一个线程：<br><code>coord = tf.train.Coordinator()</code>.<br><code>threads = tf.train.strat_queue_runners(sess=sess, coord=coord)</code>.<br>使用<code>sess.run()</code>来读取数据.<br>数据使用完毕关闭线程：<br><code>coord.request_stop()</code>.<br><code>coord.join(threads)</code>.      </p>
<p>总结起来：</p>
<ul>
<li>生成tfrecords文件。    </li>
<li>定义解析tfrecords文件。    </li>
<li>构造批数据。    </li>
<li>初始化。     </li>
<li>启动QueueRunner。    </li>
</ul>
<h3 id="设计神经网络"><a href="#设计神经网络" class="headerlink" title="设计神经网络"></a>设计神经网络</h3><p>使用在tensorflow基础上进行封装的<code>prettytensor</code>来构建神经网络。有两个卷积层每个卷积层配套一个max_pool，然后是两个全连接层接着是输出两个值的softmax层。创建网络的时候设计为训练和测试可以复用的形式。</p>
<p>增加了一个加载CheckPoint的函数，可以在训练前加载先前保存的训练数据<code>saver = tf.train.Saver()</code>，这样的函数是在长时的训练过程中避免意外导致所以训练功亏一篑。</p>
<p>在创建网络的时候，分为两个一个是训练时的网络一个是测试时的网络。通过设置这个参数来进行切换。同时<code>with tf.variable_scope(&#39;network&#39;, reuse=not training):</code>这里如果<code>training=False</code>那么<code>reuse</code>就是True这个就复用训练的参数。毕竟我们在测试的时候，各个变量是不需要进行改变的。同时在<code>prettytensor</code>中使用<code>pt.defaults_scope(activation_fn=tf.nn.relu, phase=phase)</code>的phase来配置train和测试模式<code>pt.Phase.train</code>/<code>pt.Phase.infer</code>。</p>
<p><a href="https://zhuanlan.zhihu.com/p/26808093" target="_blank" rel="external">prettytensor可以参考</a></p>
<h3 id="训练和验证"><a href="#训练和验证" class="headerlink" title="训练和验证"></a>训练和验证</h3><p>在训练中需要注意将输入到模型label应该转换为one-hot数据。然后使用tfrecords数据来进行训练和测试，在验证测试集的时候选择一次读出指定的数据。最后每次使用最后一次保存的模型信息来进行具体的应用。在实际应用时采集到一张图片输入到神经网络进行判断时需要将采集到的数据添加一个图片索引维度<code>img = face[np.newaxis,:]</code>。面对多个项目的输出时，有时需要按照得分输出结果可以使用<code>index_sort = np.argsort(-cls_percent[0])</code>这个函数返回的是排序后索引。</p>
<h3 id="出现的问题"><a href="#出现的问题" class="headerlink" title="出现的问题"></a>出现的问题</h3><p>在编写读取tfrecords文件的程序时，进行<code>tf.train.shuffle_batch</code>操作时发生错误<code>All shapes must be fully defined:[TensorShape([Dimension(None)]),TensorShape([])]</code>这样的错误原因有很多。我这里的原因是进行<code>tf.reshpae(img,[64,64,3])</code>这个操作时，没有接收返回值，所以失败。改成这样就好了<code>img = tf.reshpae(img,[64,64,3])</code></p>
<p><a href="https://github.com/MemoryCrash/MachineLearningPractice/tree/master/recognizeface" target="_blank" rel="external">实现代码</a></p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[tensoflow练习]]></title>
      <url>/2017/07/14/tensorflowpractice/</url>
      <content type="html"><![CDATA[<p>使用<code>scp</code>在两台linux机器之间传送文件是一个常用的方式<code>scp localfilepath user@remoteip:filepath</code>如果发现出现了<code>premisson denied</code>权限的错误可以将远程目录的权限改为<code>sudo chmod 777 file</code>一般这样就可以了。<a href="http://www.jianshu.com/p/bb71b43e3389" target="_blank" rel="external">参考:SCP命令之Permission denied-已解决</a>。<br><a id="more"></a><br>这次我们使用的数据是<a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="external">CIFAR-10</a>它的数据格式是这样多的：</p>
<blockquote>
<p>data – a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image.</p>
</blockquote>
<p>CIFAR-10的原始数据是按照<code>[image_number，channel，height，width]</code>的形式存储的：留意tensorflow中在处理图像数据的时候要求的数据格式是<code>[image_number,height,width,channel]</code>不符合这样要求的格式需要去调整。这里可以使用transpose来进行调整一个<code>1*2*3</code>的矩阵A调整成<code>3*2*1</code>的矩阵可以使用<code>A_ = A.transpose([2,1,0])</code>这样来调整。元素按照这样来变化$\hat{A}_{k,j,i}=A_{i,j,k}$<a href="http://www.cnblogs.com/vin-yuan/p/5066922.html" target="_blank" rel="external">参考:numpy扎记</a>。这里为什么不用<code>reshape</code>呢，我理解<code>reshape</code>比较适合将一个一维的原始数据进行改变。但是如果要在多维数据上进行调整还是需要使用<code>transpose</code>因为它是在了解了原始数据多维结构的基础上进行的调整。</p>
<p><strong>注意</strong>：其实常困惑在一些运算中的参数axis或者dimension也就是维度信息。这个其实就按照维度来进行理解就可以，比如argmax(dimension=0)就是对维度为0上的元素进行取最大值索引操作如果比较的元素是列表就对元素内部的子元素进行逐一的计算。比如</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">a=[[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">4</span>,<span class="number">3</span>],[<span class="number">5</span>,<span class="number">6</span>]] </div><div class="line">tf.argmax(a, dimension=<span class="number">0</span>)</div><div class="line">&gt;&gt; <span class="number">2</span>,<span class="number">2</span></div></pre></td></tr></table></figure>
<h3 id="输入数据"><a href="#输入数据" class="headerlink" title="输入数据"></a>输入数据</h3><p>通过对原始数据的随机裁剪，水平翻转，调整色调、对比度和饱和度来扩充训练图像。对于测试数据只进行裁剪。<br>随机裁剪图片：<code>tf.random_crop</code><br>随机左右翻转图片：<code>tf.image.random_flip_left_right</code><br>随机色调调整：<code>tf.image.random_hue</code><br>随机对比度调整：<code>tf.image.random_contrast</code><br>随机亮度调整：<code>tf.image.random_brightness</code><br>随机饱和度调整：<code>tf.image.random_saturation</code><br>限制大小范围：<br><code>tf.minimum(image, 1.0)</code>：对大于1的数进行截断<br><code>tf.maximum(image, 0.0)</code>：对小于0的数进行截断<br>解压压缩包tar:<code>tar xvf FileName.tar</code>       </p>
<h3 id="GPU使用"><a href="#GPU使用" class="headerlink" title="GPU使用"></a>GPU使用</h3><p>tensorflow在设备有GPU的情况下会默认调用GPU</p>
<h3 id="tensorboard"><a href="#tensorboard" class="headerlink" title="tensorboard"></a>tensorboard</h3><p>tensorboard是tensorflow提供的一个可视化的工具，通过<code>pip3 install tensorboard</code>来安装。按照好了以后在终端中使用<code>sudo tensorborad --logdir=YouLogDir</code>来运行。然后在chrome浏览器中输入<code>localhost:6006</code>登陆，就可以看到在浏览器中可视化的界面，但是如果你没有在代码中设置需要可视化的对象并且没有运行代码的话也是看不到信息的。</p>
<p>首先我们可以可视化变量(HISTOGRAMS)就是模型中的权值，可视化常量(SCALARS)如模型中的损失值，还有神经网络的图表示等。<br><img src="/image/tensorboard/scalarstensoboard.png" height="50%" width="50%/"><br>如何可视化损失的变化呢？<br><code>with tf.name_scope(&#39;input&#39;):</code>：指定名称。这样的层次关系可以让可视化看起来更清晰。<br><code>tf.summary.scalar(&#39;loss&#39;,loss)</code>:设置观察的常量。<br><code>tf.summary.histogram(&#39;xxx&#39;,xxx)</code>:设置变量的观察点。<br><code>tf.summary.image(&#39;input&#39;, image_shaped_input, 10)</code>:显示10张图片。<br><code>summary = tf.summary.merge_all()</code>:整合所有需要关注的内容。<br><code>summary = tf.summary.FileWriter(logdir, sess.grahp)</code>:指定保持可视化文件的路径。这个操作应该放在<code>sess = tf.Session()</code>后面。<br>选择一个需要进行记录的位置，比如迭代过程中的时刻，每一百次记录一下。<br><code>summary_str = sess.run(summary, feed_dict=feedict)</code>:summary也是需要传入feed数据的而且就是训练的神经网络的数据。<br><code>summary_write.add_summary(summary_str, step)</code>:将这个步骤的可视化数据和迭代位置传入。<br><code>summary_write.Flush()</code>:刷新缓冲区，写入文件。<br><img src="/image/tensorboard/graphtensorboard.png" height="50%" width="50%/"></p>
<p><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py" target="_blank" rel="external">参考文件:mnist_with_summaries.py</a>   </p>
<h3 id="checkpoint和restore"><a href="#checkpoint和restore" class="headerlink" title="checkpoint和restore"></a>checkpoint和restore</h3><p>因为在神经网络的训练中通常的训练时间都比较长，所以设定迭代位置定期存储模型信息就显得尤为重要。在tensorflow中使用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">save_dir = <span class="string">'/checkpoint'</span></div><div class="line">save_path = os.path.join(save_dir, <span class="string">'modelname'</span>)</div><div class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(save_dir):</div><div class="line">	os.mkdirs(save_dir)</div><div class="line">saver = tf.train.Saver()</div><div class="line"><span class="comment">#保存模型,global_step表示迭代位置</span></div><div class="line">saver.save(session, save_path=save_path, global_step=global_step)</div><div class="line"><span class="comment">#恢复数据，获取最新的checkpoint</span></div><div class="line">last_chk_path = tf.train.latest_checkpoint(checkpoint_dir=save_dir)</div><div class="line">saver.restore(session, save_path=last_chk_path)</div></pre></td></tr></table></figure>
<h3 id="训练中出现的问题"><a href="#训练中出现的问题" class="headerlink" title="训练中出现的问题"></a>训练中出现的问题</h3><p>出现这样的错误：<code>CUDNN_STATUS_INTERNAL_ERROR</code>我这里的原因是运行脚本的时候没有加上<code>sudo</code>。<br>试验了一个网络训练的例子，教程中4核cpu需要15小时训练15万个迭代，使用单个的1080ti的gpu只需要1个小时即可。但是机箱温度上升很快，机箱没有配置自带的散热风扇可能也是温度升高快的原因。<br><code>shutdown -h now</code>: 现在立即关机。<br><code>shutdown -r now</code>: 现在立即重启。<br>希望将ubuntu终端中显示的绝对路径修改当前路径，只需要将<code>.bashrc</code>文件中<code>PS1</code>的的<code>\w</code>改为<code>\W</code>再<code>source .bashrc</code>即可。<br>运行mnist的<code>mninst_with_summaries.py</code>时出现错误<code>cant open CUDA library libcupti.so.8.0</code>错误。解决方法是将<code>／usr/local/CUDA/extras/CUPTI/lib64</code>下的<code>libcupti.so.8</code>3个文件拷贝到<code>/usr/local/lib/</code>目录下去。 </p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[jupyter notebook & pandas & numpy]]></title>
      <url>/2017/07/13/JupyterPandasNumpy/</url>
      <content type="html"><![CDATA[<p>以下简单介绍 jupyter notebook、pandas 数据处理包、numpy 科学计算库。</p>
<h2 id="jupyter-notebook"><a href="#jupyter-notebook" class="headerlink" title="jupyter notebook"></a>jupyter notebook</h2><p>安装jupyter notebook：对于python3是这样<code>pip3 install jupyter</code>。<br>运行jupyter notebook：客户端输入<code>jupyter notebook</code>。<br><a id="more"></a><br>我的运行环境是macbook，在实际运行jupyter的时候出现<code>execution error: &quot;http://localhost:8888/tree不理解open location信息。 (-1708)</code>的错误。解决办法是首先运行这个命令<code>jupyter notebook --generate-config</code>生成<code>~/.jupyter</code>配置文件目录。然后找到<code>jupyter_notebook_config.py</code>文件，并做如下修改:    </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">c.NotebookApp.browser = <span class="string">u'Safari'</span></div><div class="line">c.NotebookApp.token = <span class="string">''</span></div><div class="line">c.NotebookApp.password = <span class="string">''</span></div></pre></td></tr></table></figure>
<p>接下来就可以使用了。使用<code>ctrl+c</code>来发送退出信号，“是否退出”-选择yes完成退出。<br>jupyter notebook 练习参考<a href="http://www.codingpy.com/article/getting-started-with-jupyter-notebook-part-1/?utm_source=tuicool&amp;utm_medium=referral" target="_blank" rel="external">Jupyter Notebook 快速入门（上）</a>，<a href="http://www.codingpy.com/article/getting-started-with-jupyter-notebook-part-2/" target="_blank" rel="external">Jupyter Notebook 快速入门（下）</a><br>jupyter notebook 同样支持 markdown 也就支持了 laTex 公式的编辑和使用 HTML 插入图片。习惯使用 markdown 编写文档的话在 jupyter notebook 上也能无缝衔接。<br>嵌入<code>matplotlib</code>进行绘图。写文章做笔记的时候很困扰的一件事件就是将绘制的图形附在文章中。现在jupyter notebook提供了将matplotlib嵌入到笔记中的方式。在单元格中填写<code>%matplotlib inline</code>然后新增一个单元格来进行绘图即可。<br>以下内容参考<a href="http://blog.csdn.net/tina_ttl/article/details/51031113" target="_blank" rel="external">Python·Jupyter Notebook各种使用方法记录·持续更新</a><br>将代码倒入到某一个cell使用<code>%load tesp.py</code>然后运行<code>shift+enter</code>。<br>使用<code>%run file.py</code>可以在 jupyter notebook 中运行 python 文件。 </p>
<h2 id="numpy"><a href="#numpy" class="headerlink" title="numpy"></a>numpy</h2><p>安装<code>pip3 install numpy</code>        </p>
<h3 id="创建numpy-array"><a href="#创建numpy-array" class="headerlink" title="创建numpy array"></a>创建numpy array</h3><p>使用numpy创建array并指定元素类型。array.dtype可以查看类型信息<br>array = np.array([[1,2,3],[2,3,4]], dtype=np.int)<br>print(array)<br>打印维度<code>print(&#39;number of dim:&#39;,array.ndim)</code><br>打印形状<code>print(&#39;shape:&#39;,array.shape)</code><br>打印大小<code>print(&#39;size:&#39;,array.size)</code>       </p>
<p>生成全0的矩阵<code>a = np.zeros((3,4))</code><br>生成全1的矩阵<code>b = np.ones((3,4),dtype=np.int)</code><br>生成empty矩阵元素值接近0<code>c = np.empty((3,4))</code><br>生成10到20之间的数(不包括20)步长是2<code>d = np.arange(10,20,2)</code><br>调整shape<code>e = np.arange(12).reshape((3,4))</code><br>生成数值从1到10产生5个<code>f = np.linspace(1,10,5)</code><br>随机生成2x2的0到1之间的随机数。<code>a = np.random.random((2,4))</code>       </p>
<h3 id="简单运算"><a href="#简单运算" class="headerlink" title="简单运算"></a>简单运算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">a = np.array([<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span>])       </div><div class="line">b = np.arange(<span class="number">4</span>)       </div><div class="line">c = a + b       </div><div class="line">c = a - b    </div><div class="line">```   </div><div class="line"></div><div class="line">平方`c = b**<span class="number">2</span>`       </div><div class="line">求sin`c = <span class="number">10</span>*np.sin(a)`       </div><div class="line">判断矩阵元素值[<span class="keyword">True</span>, <span class="keyword">True</span>, <span class="keyword">False</span>, <span class="keyword">False</span>] b&lt;<span class="number">3</span>  </div><div class="line">     </div><div class="line">```python</div><div class="line">a = np.array([[<span class="number">1</span>,<span class="number">1</span>],</div><div class="line">             [<span class="number">0</span>,<span class="number">1</span>]])       </div><div class="line">b = np.arange(<span class="number">4</span>).reshape((<span class="number">2</span>,<span class="number">2</span>))</div></pre></td></tr></table></figure>
<p>矩阵逐个乘法<code>c = a * b</code><br>矩阵乘法<code>c_dot = np.dot(a, b)</code>,<code>c_dot2 = a.dot(b)</code>       </p>
<p>求取矩阵的和<code>np.sum(a)</code>通过axis指定求和的范围，axis=1表示将维度是1的内容进行求和，axis=0表示将维度是0的元素进行这个元素可能是标量也可能是另一个列表求和.<code>np.sum(a,axis=1)</code><br>求取矩阵中的最小值<code>np.min(a)</code><br>求取矩阵中的最大值<code>np.max(a)</code>       </p>
<h3 id="索引操作"><a href="#索引操作" class="headerlink" title="索引操作"></a>索引操作</h3><p>A = np.arange(2, 14).reshape((3,4))<br>矩阵A的最小值索引<code>np.argmin(A)</code><br>矩阵A的最大值索引<code>np.argmax(A)</code><br>矩阵平均值<code>np.mean(A)</code>,<code>A.mean()</code>,<code>np.average(A)</code><br>中位数<code>np.median(A)</code><br>逐位累加<code>np.cumsum(A)</code><br>获取非0，给出非0元素的横坐标列表和纵坐标列表<code>np.nonzero(A)</code><br>矩阵的转置<code>np.transpose(A)</code>或者<code>A.T</code><br>截断数组大于9截断为9小于5截断为5<code>np.clip(A, 5, 9)</code>       </p>
<p>A = np.arange(3,15).reshape((3,4))<br>索引1行1列<code>A[1][1]</code>,<code>A[1,1]</code><br>索引某行所有数据<code>A[1,:]</code><br>索引某列所有数据<code>A[:,1]</code><br>for循环默认迭代行<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> row <span class="keyword">in</span> A:       </div><div class="line">    print(row)</div></pre></td></tr></table></figure></p>
<p>for循环迭代列<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> column <span class="keyword">in</span> A.T:</div><div class="line">    print(column)</div></pre></td></tr></table></figure></p>
<p>迭代每个项目，将数据变成一行的迭代器<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> item <span class="keyword">in</span> A.flat:</div><div class="line">    print(item)</div></pre></td></tr></table></figure></p>
<p>A = np.array([1,1,1])<br>B = np.array([2,2,2])<br>合并-上下的方式合并<code>np.vstack((A,B))</code><br>合并-左右的方式合并<code>np.hstack((A,B))</code><br>横向数据变为纵向–在列上加一个维度[1 1]-&gt;[[1][1]]<code>A[:,np.newaxis]</code><br>纵向数据变为横向–在横上加一个维度<code>A[np.newaxis,:]</code>       </p>
<p>多个矩阵的合并，并在列的维度进行合并。这横向和纵向是组成新的矩阵时原有矩阵的排列方式。<code>C = np.concatenate((A, B, B, A), axis=0)</code>       </p>
<h3 id="分割"><a href="#分割" class="headerlink" title="分割"></a>分割</h3><p>A = np.arange(12).reshape((3,4))<br>按照行进行分割成3块<code>np.split(A, 3, axis=0)</code>,<code>np.vsplit(A, 3)</code><br>按照列进行分割成2块<code>np.split(A, 2, axis=1)</code>,<code>np.hsplit(A, 2)</code><br>不等的分割<code>np.array_split(A,3,axis=1)</code>       </p>
<h3 id="深拷贝"><a href="#深拷贝" class="headerlink" title="深拷贝"></a>深拷贝</h3><p>python里面对象是使用引用来存储的所以要考虑到深拷贝还是浅拷贝的问题。<br>a = np.arange(4)<br>这样就是深拷贝，新创建对象出来<code>b = a.copy()</code><br>这样就是浅拷贝引入同样的对象<code>c = a</code>       </p>
<h2 id="pandas"><a href="#pandas" class="headerlink" title="pandas"></a>pandas</h2><p>安装<code>pip3 install pandas</code>     </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">像是字典形式的numpy     </div><div class="line">s = pd.Series([<span class="number">1</span>,<span class="number">3</span>,<span class="number">6</span>,np.nan,<span class="number">44</span>,<span class="number">1</span>])     </div><div class="line">print(s)     </div><div class="line"></div><div class="line">dates = pd.date_range(<span class="string">'20160101'</span>, periods=<span class="number">6</span>)     </div><div class="line">print(dates)     </div><div class="line">给数据加上了行列标签     </div><div class="line">df = pd.DataFrame(np.random.randn(<span class="number">6</span>,<span class="number">4</span>),index=dates,columns=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])     </div><div class="line">print(df)</div></pre></td></tr></table></figure>
<p>按照列给出每列数据的类型<code>df.dtypes</code><br>给出行名字<code>df.index</code><br>给出列名字<code>df.columns</code><br>给出值<code>df.values</code><br>给出描述比如mean等，运算数值信息<code>df.describe()</code><br>矩阵的转置<code>df.T</code><br>axis以列标签为单位进行排序，并且是逆序<code>df.sort_index(axis=1,ascending=False)</code><br>按照值E这列的值来进行排序<code>df.sort_values(by=&#39;E&#39;)</code>     </p>
<h3 id="选择数据"><a href="#选择数据" class="headerlink" title="选择数据"></a>选择数据</h3><p>输出对应列<code>df[&#39;A&#39;]</code>,<code>df.A</code><br>输出行<code>df[0:3]</code>     </p>
<p>使用标签输出,行标签<code>df.loc[&#39;20130104&#39;]</code><br>使用标签输出，列标签<code>df.loc[:,[&#39;A&#39;,&#39;B&#39;]]</code>     </p>
<p>通过位置输出     </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">df.iloc[<span class="number">3</span>]</div><div class="line">df.iloc[<span class="number">3</span>,<span class="number">1</span>]</div><div class="line">df.iloc[[<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>],<span class="number">1</span>:<span class="number">3</span>]</div></pre></td></tr></table></figure>
<p>标签和索引一起使用<code>df.ix[:3,[&#39;A&#39;,&#39;C&#39;]]</code><br>条件筛选<code>df[df.A&gt;8]</code>     </p>
<h3 id="赋值"><a href="#赋值" class="headerlink" title="赋值"></a>赋值</h3><p>df.iloc[2,2] = 1111<br>df.A[df.A&gt;4] = 0<br>添加一个空列<code>df[&#39;F&#39;] = np.nan</code><br>注意如果是要赋具体值，行标签要和添加对象保持一致。<code>df[&#39;E&#39;] =pd.Series([1,2,3,4,5,6],index=pd.date_range(&#39;20130101&#39;,periods=6))</code>     </p>
<h3 id="处理没有数据的模块"><a href="#处理没有数据的模块" class="headerlink" title="处理没有数据的模块"></a>处理没有数据的模块</h3><p>df.iloc[0,1]=np.nan<br>df.iloc[1,2]=np.nan     </p>
<p>丢掉出现nan的行，how=any只有有nan就丢，how=all就是全部为nan丢掉     <code>df.dropna(axis=0,how=&#39;any&#39;)</code>     </p>
<p>填充nan数据<code>df.fillna(value=0)</code><br>检查是否有nan,缺失位置为true<code>df.isnull()</code><br>判断是否有nan<code>np.any(df.isnull())==True</code>     </p>
<h3 id="数据导入导出"><a href="#数据导入导出" class="headerlink" title="数据导入导出"></a>数据导入导出</h3><p>一般使用csv文件<code>data = pd.read_csv(&#39;student.csv&#39;)</code><br>存储为pickle格式<code>data.to_pickle(&#39;student.pickle&#39;)</code>     </p>
<h3 id="合并data-frame"><a href="#合并data-frame" class="headerlink" title="合并data frame"></a>合并data frame</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">df1 = pd.DataFrame(np.ones((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">0</span>,columns=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</div><div class="line">df2 = pd.DataFrame(np.ones((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">1</span>,columns=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</div><div class="line">df3 = pd.DataFrame(np.ones((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">2</span>,columns=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</div><div class="line">df4 = pd.DataFrame(np.ones((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">3</span>,columns=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</div></pre></td></tr></table></figure>
<p>上下合并 axis=0表示以行为单位进行合并。增加行数<code>res = pd.concat([df1,df2,df3],axis=0)</code><br>左右合并 axis=1表示以列为单位进行合并。增加列数,ignore_index=True忽略原有的行索引<code>res = pd.concat([df1,df2,df3],axis=0,ignore_index=True)</code>     </p>
<p>这样直接合并会出现问题因为索引和标签不能完全对应     </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">df1 = pd.DataFrame(np.ones((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">0</span>,columns=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>],index=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</div><div class="line">df2 = pd.DataFrame(np.ones((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">1</span>,columns=[<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>,<span class="string">'e'</span>],index=[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</div></pre></td></tr></table></figure>
<p>使用join=’inner’会选择共有的数据。其它数据放弃。<code>res = pd.concat([df1,df2],join=&#39;inner&#39;,ignore_index=True)</code><br>以df1.index来进行合并<code>res = pd.concat([df1,df2],axis=1,join_axes=[df1.index])</code><br>竖向的加数据<code>res = df1.append([df2,df3], ignore_index=True)</code><br>添加一条序列数据     </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">l = pd.Series([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],index=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</div><div class="line">res = df1.append(l, ignore_index=<span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<h3 id="merge-进行数据合并"><a href="#merge-进行数据合并" class="headerlink" title="merge 进行数据合并"></a>merge 进行数据合并</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">left = pd.DataFrame(&#123;<span class="string">'key'</span>:[<span class="string">'K0'</span>,<span class="string">'K1'</span>,<span class="string">'K2'</span>,<span class="string">'K3'</span>],</div><div class="line">                    <span class="string">'A'</span>:[<span class="string">'A0'</span>,<span class="string">'A1'</span>,<span class="string">'A2'</span>,<span class="string">'A3'</span>],</div><div class="line">                    <span class="string">'B'</span>:[<span class="string">'B0'</span>,<span class="string">'B1'</span>,<span class="string">'B2'</span>,<span class="string">'B3'</span>]&#125;)</div><div class="line"></div><div class="line">right = pd.DataFrame(&#123;<span class="string">'key'</span>:[<span class="string">'K0'</span>,<span class="string">'K1'</span>,<span class="string">'K2'</span>,<span class="string">'K3'</span>],</div><div class="line">                    <span class="string">'C'</span>:[<span class="string">'C0'</span>,<span class="string">'C1'</span>,<span class="string">'C2'</span>,<span class="string">'C3'</span>],</div><div class="line">                    <span class="string">'D'</span>:[<span class="string">'D0'</span>,<span class="string">'D1'</span>,<span class="string">'D2'</span>,<span class="string">'D3'</span>]&#125;)</div></pre></td></tr></table></figure>
<p>基于key这列来进行merge<code>res = pd.merge(left,right,on=&#39;key&#39;)</code>     </p>
<p>参考：<a href="https://www.youtube.com/watch?v=R6oAP8A2lNQ&amp;list=PLXO45tsB95cKKyC45gatc8wEc3Ue7BlI4&amp;index=11" target="_blank" rel="external">numpy pandas教程</a>     </p>
]]></content>
      
        <categories>
            
            <category> tool </category>
            
        </categories>
        
        
        <tags>
            
            <tag> basic skill </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[最大似然估计]]></title>
      <url>/2017/07/10/mechinabasic/</url>
      <content type="html"><![CDATA[<p><strong>独立同分布(IID)</strong>：在概率统计理论中，如果变量序列或者其他随机变量有相同的概率分布，并且互相独立，那么这些随机变量是独立同分布。随机变量独立是指X1的取值不影响X2的取值，X2的取值不影响X1的取值。<br><strong>最大似然估计</strong>：在已知试验结果的情况下，用来估计满足这些样本分布的参数，把可能性最大的参数作为真实的参数估计。最大似然估计中采样，需要假设采样结果都是独立同分布。f是已知模型，$x_1$,$x_2$,…,$x_3$是独立同分布的采样数据。$\theta$是未知的模型参数。<br>$f(x_1,x_2,…x_n|\theta )=f(x_1|\theta )*f(x_2|\theta )…f(x_n|\theta )$<br><a id="more"></a><br>$L(\theta |x_1,x_2…,x_n)=f(x_1,…,x_n|\theta )=\prod_{i=1}^{n}f(x_i|\theta )$<br>取对数将乘法转换为加法方便计算。<br>$ln(\theta |x_1,x_2…,x_n)=\sum_{i=1}^{n}lnf(x_i|\theta)$<br>取对数似然平均值。<br>$\hat{l}=\frac{1}{n}lnL$<br>最大似然估计就是取使得$\hat{l}$获得最大值的$\theta$<br>$\theta _{mle}=argmax\hat{l}(\theta |x_1,…,x_n)$</p>
<p>最大似然估计一般求解过程：</p>
<ol>
<li>写出似然函数；</li>
<li>对似然函数取对数；</li>
<li>求导数；</li>
<li>解似然方程；</li>
</ol>
<p><strong>举例求解高斯分布的最大似然估计</strong>：<br>高斯分布概率密度函数：<br>$f(x)=\frac{1}{\sqrt{2\pi}\sigma }e^{\frac{(x-\mu )^2}{2\sigma ^2}}$<br>将采样的$x_i$带入到似然函数得到：<br>$L(x)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}\sigma }e^{\frac{(x-\mu )^2}{2\sigma ^2}}$<br>两边求对数l(x)表示log(L(x))：<br>$l(x)=\sum_{i=1}^{n}log\frac{1}{\sqrt{2\pi}\sigma }e^{\frac{(x-\mu )^2}{2\sigma ^2}}$<br>进一步化简：<br>$l(x)=(\sum_{i}log\frac{1}{\sqrt{2\pi }\sigma})+(\sum_{i}-\frac{(x_i-\mu)^2}{2\sigma ^2})$<br>左侧第一项是常数项目因为只有$x_i$是变量，$\mu$和$\sigma$是固定但是未知的数据。<br>$l(x)=-\frac{n}{2}log(2\pi \sigma ^2)-\frac{1}{2\sigma ^2}\sum_{i}(x_i-\mu )^2$ 公式1<br>在公式1基础上对$\mu $和$\sigma$求偏导数并令其为0求解到(lnx求导为$\frac{1}{x}$)。<br>$\mu = \frac{1}{n}\sum_{i}x_i$ 公式2<br>$\sigma ^2=\frac{1}{n}\sum_{i}(x_i-\mu)^2$ 公式3</p>
<p>公式2和公式3也是很符合均值和方差的直观感觉。</p>
<p><a href="http://www.cnblogs.com/liliu/archive/2010/11/22/1883702.html" target="_blank" rel="external">参考1:最大似然估计(Maximum likelihood estimation)</a><br><a href="https://zh.wikipedia.org/zh-hans/最大似然估计" target="_blank" rel="external">参考2 wikipedia:最大似然估计</a><br><a href="http://blog.csdn.net/cheng1988shu/article/details/7905308" target="_blank" rel="external">参考3 什么是独立同分布</a></p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> basic skill </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[《deep learning》--梯度]]></title>
      <url>/2017/07/07/machinelearningbasic/</url>
      <content type="html"><![CDATA[<p>在机器学习中的一些运算需要考虑是否会出现上溢或者下溢的情况，比较好的是一般的框架会处理这些问题。<br><strong>导数</strong>：在深度学习中一般我们会获取到一个目标函数或者叫损失函数。然后通过最小化它们来进行优化。优化的过程是迭代进行。${f}’(x)$表示函数f(x)在点x处的斜率。它表明如何缩放输入的小变化才能在输出获得相应的变化：$f(x+\epsilon )\approx f(x)+\epsilon{f}’(x)$。然后我们可以通过这样的方式迭代的向一个函数的局部最小值或者最大值进行移动。当到达${f}’(x)=0$时，这个时候就可能到达了一个极值点，也可能在一个鞍点上。<br><a id="more"></a><br><strong>偏导数</strong>：偏导数的概念在多维输入的函数中引入。偏导数$\frac{\partial}{\partial x_{i}}f(x)$衡量点x处只有$x_{i}$增加时f(x)如何变化。梯度是相对一个向量求导的导数：f的导数是包括所有偏导数的向量，记为$\triangledown _xf(x)$。梯度的第i个元素是f关于$x_{i}$的偏导数。在多维的情况下，临界点是梯度中所有元素都为零的点。<br><strong>梯度</strong>：是一个矢量，其方向上的导数最大，其大小正好是此最大方向导数。<a href="http://www.matongxue.com/madocs/222.html#/madoc" target="_blank" rel="external">参考：如何直观形象的理解方向导数与梯度以及它们之间的关系</a> 。偏导数是多元函数沿着坐标轴方向的变化率<a href="https://zhuanlan.zhihu.com/p/24913912" target="_blank" rel="external">参考：为什么梯度反方向是函数值下降最快的方向</a>以下内容基本摘录自这个参考内容。 </p>
<ul>
<li>方向导数是各个方向上的导数  </li>
<li>偏导数连续才有梯度存在</li>
<li>梯度的方向是方向导数中取到最大值的方向，梯度的值是方向导数的最大值。</li>
</ul>
<p>注意：梯度的方向是函数增长最快的方向，所以我们在使用梯度下降算法的时候实际是取梯度的反方向。<br>以上我们说了梯度的定义，其上的方向导数最大并且梯度的值就是这个最大方向导数的值。但是和我们的偏导数还没有建立关系，因为使用梯度的经验告诉我们求解梯度的时候都是对多维度函数求偏导数。<br>我们的目的是寻找某个位置变化最快的方向以及变化率，这个可以通过方向导数告诉我们。方向导数就是求解在多维函数的某个点上方向导数的方向上函数的变化率。而这个点当然会有很多个方向。我们要找到变化最大的方向。<br>方向导数：$\underset{t\rightarrow 0}{lim}\frac{f(x_0+tcos\theta ,y_0+tsin\theta)-f(x_0,y_0)}{t}$<br>单位向量：$u=cos\theta i+sin\theta j$<br>方向导数的偏微分表示：$f_x(x,y)cos\theta +f_y(x,y)sin\theta$ 公式1<br>使用方向导数的偏微分表示来寻找什么位置的方向导数具有最大值。假设$A=(f_x(x,y),f_y(x,y))$表示偏微分组成的向量，$I=(cos\theta, sin\theta)$表示方向向量。那么公式1可以化简为A和I的点积$A\cdot I=|A|*|I|cos\alpha$。这个时候可以知道当$\alpha$是0时对应公式1获得最大值。这个时候的方向导数就被叫做了梯度。而其正好就是偏微分形成的向量说指向的方向，其值也是偏微分形成的向量的模。</p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> basic skill </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[《deep learning》--概率论]]></title>
      <url>/2017/07/06/svd/</url>
      <content type="html"><![CDATA[<p>一种概率和发生的频率相关，称为频率派概率。还有一种涉及到确信的程度(1表示一定发生，0表示一定不发生)称为贝叶斯概率。</p>
<h3 id="基本概率概念"><a href="#基本概率概念" class="headerlink" title="基本概率概念"></a>基本概率概念</h3><p><strong>随机变量</strong>：可以随机取不同值的变量，一个随机变量只是对可能的状态的描述，它必须伴随着一个概率分布来指定每个状态的可能性。<br><strong>概率分布</strong>：用来描述随机变量在每一个可能取到的状态的可能性大小。<br>离线型的变量的概率分布使用概率质量函数来表示符号P。X=x的概率为P(x)完成从随机变量取的某个状态到随机变量取的某个状态的概率的映射。概率质量函数同时作用于多个随机变量P(X=x,Y=y)表示X=x，Y=y同时发生的概率，也称为联合概率分布。<br><a id="more"></a><br>连续型的概率分布使用概率密度函数p表示，概率密度函数在在随机变量的作用域下的积分为1。<br><strong>边缘概率分布</strong>：当我们了解到一个联合概率分布后希望知道其中一个子集的分布，这个子集的分布就叫做边缘概率分布。比如对于一个离线型的随机变量x，y知道了P(x,y)可以通过$P(x)=\sum _{y}P(x,y)$来求解，对于连续型的就是通过积分的形式求解。<br><strong>条件概率</strong>：在一个事件给定的情况下另外一个事件发生的概率。$P(X=x|Y=y)=\frac{P(X=x,Y=y)}{P(Y=y)}$这个公式通过集合的方式来理解很方便。假设有一个Y和一个X集合它们有交集$Y\cup X$这个交集就可以理解为P(Y=y,X=x)这个联合概率分布(因为是同时发生)。然后现在要求的是在条件概率在Y发生的情况下X发生的概率。所以就是将这个交集除Y这个集合就可以了。对一个起来就是上面的条件概率公式。<br><strong>条件概率的链式法则</strong>：任何一个多变量的联合概率分布都可与转换为只有一个变量的条件概率相乘的形式。如P(a,b,c)=P(a|b,c)P(b|c)P(c)。<br><strong>期望</strong>：一个函数f(x)作用于P(x)产生的x，这样得到的f(x)的平均值就叫做f(x)关于P(x)的期望。$E[f(x)]=\sum _{x}P(x)f(x)$连续分布这个公式就对应为积分。<br><strong>方差</strong>：衡量的是当我们对x依据它的概率分布来进行采样的时候随机变量x的函数值呈现的差异大小。$Var(f(x))=E[(f(x)-E[f(x)])^2]$注意这里是对某个随机变量的概率分布来进行的采样。方差的平方根称为标准差。<br><strong>协方差</strong>：给出两个变量线性相关性以及这些变量的尺度。$Cov(f(x),g(y))=E[(f(x)-E[f(x)])(g(x)-E[g(x)])]$当x变大y也变大这个时候协方差是正，当x变大y变小这个时候协方差值为负。协方差的绝对值如果很大则意味着变量的变化很大并且它们同时距离各自的均值很远。    </p>
<h3 id="常见概率分布"><a href="#常见概率分布" class="headerlink" title="常见概率分布"></a>常见概率分布</h3><p><strong>Bernoulli分布</strong>：单个二值随机分布。类似抛一次硬币，正面向上有一个概率，背面向上有一个概率。不一定是50%对50%的概率。<br><strong>Multinoulli分布</strong>：类似扔骰子，但是只扔一次。Multinomial分布(多项分布)可能是扔多次的情况。<br><strong>高斯分布</strong>：又叫做正态分布，公式是$N(x;\mu ,\sigma ^2)=\sqrt{\frac{1}{2\pi \sigma ^2}}exp(-\frac{1}{2\sigma ^2}(x-\mu )^2)$这里$\sigma$表示标准差，它的平方也就是方差，方差越大正太分布的图形越宽。方差越小正太分布的图像越尖锐。$\mu$表示均值。一般使用正太分布的建模的情况比较多，一个是根据中心极限定理，很多独立随机变量的和近似服从正太分布；二是在具有相同方差的所有可能概率分布中，正太分布在实数上具有最大的不确定性，也就是需要的先验知识少。    </p>
<h3 id="常用函数的有用性质"><a href="#常用函数的有用性质" class="headerlink" title="常用函数的有用性质"></a>常用函数的有用性质</h3><p><strong>logistc sigmoid</strong>：公式为：$\sigma (x)=\frac{1}{1+exp(-x)}$其取值范围是0到1之间。在x是一个很大或者很小值的时候函数曲线变的平缓，对于变化感知不灵敏。在机器学习中也说是进入的饱和状态。<br>$\sigma (x)=\frac{exp(x)}{exp(x)+exp(0)}$<br>$\frac{d}{dx}\sigma (x)=\sigma (x)(1-\sigma (x))$<br>$1-\sigma (x)=\sigma (-x)$<br><strong>贝叶斯规则</strong>：在知道P(y|x)时求解P(x|y)公式为$P(x|y)=\frac{P(x)P(y|x)}{P(y)}$通过全概率公式来得到$P(y)=\sum _{x}P(y|x)P(x)$所以事先不需要知道P(y)的信息。<a href="http://www.jianshu.com/p/c59851b1c0f3" target="_blank" rel="external">参考:条件概率，全概率，贝叶斯公式理解</a></p>
<h3 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h3><p><strong>香农熵</strong>：使用香农熵来对整个概率分布中的不确定总量进行量化，香农熵越大不确定性也越大：$H(x)=-E_{x\sim P}[logP(x)]$。<br><strong>KL散度</strong>：如果我们对于同一个随机变量x有两个单独的概率分布P(x)和Q(x)，我们可以使用KL散度来衡量这两个分布的差异(在对抗生成网络GAN中有使用到KL散度来度量生成的数据和实际数据所属分布之间的距离不过效果不好)$D_{KL}(P||Q)=E_{x\sim P}[logP(x)-logQ(x)]$。<br><strong>交叉熵</strong>：交叉熵也用来衡量两个分布的相似度在机器学习中一般配合softmax来进行最后的损失函数的计算$H(p,q)=E_{p}[-logq]=H(p)+D_{KL}(p||q)$对于离散数据公式为$H(p,q)=-\sum _{x}p(x)logq(x)$。</p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> basic skill </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[《deep learning》--线性代数]]></title>
      <url>/2017/07/04/deeplearningNote/</url>
      <content type="html"><![CDATA[<p>以下内容是《deep learning》简单的线性代数部分的摘录或者笔记。<br><strong>向量</strong>：表示为一列数。<br><strong>张量(tensor)</strong>：一个数组的元素分布在若干维。<br><strong>转置</strong>：一个矩阵围绕从左上角到右下角这个主对角线做的镜像。<br><strong>矩阵乘法</strong>：两个矩阵A，B满足$A^{n\cdot m},B^{m\cdot l}$才能相乘得到n<em>l的矩阵C：$C_{i,j}=\sum _{k}A_{i,k}B_{k,j}$，注意矩阵乘法并不是两个矩阵对应元素相乘。当然这里也有叫做Hadamard乘积的运算用来表示矩阵对应元素相乘。<br><a id="more"></a><br><strong>点积</strong>：对于两个维度相同的向量x，y它们的点积可以表示为矩阵$x^{T}$和y的乘积。<br><strong>单位矩阵</strong>：只有主对角线的元素是1，其余元素都是0。<br><strong>矩阵的逆</strong>：矩阵的逆和矩阵乘积为单位矩阵，矩阵的逆是通过这样的方式来定义。<br><strong>范数</strong>：在机器学习中有时需要去衡量向量的大小，一般使用范数这样的概念，L2表示2范数，就是欧几里得距离。<br>两个向量的点积也可以使用范数来表示：$x^{T}y=\left | x \right |_{2}\left | y \right |_{2}cos\theta $。<br><strong>特征分解</strong>：我们可以通过将一个整数分解为质数的乘积来观察一些它隐藏的性质比如12=2\</em>3*3这样可以看出12可以被3整除。同样我们可以对一个矩阵进行特征分解来获取矩阵元素表示成数组时不明显的函数性质。方阵A的特征向量是指与A相乘后等于对该向量进行缩放的非零向量。$Av=v\lambda$这里$\lambda$称为特征值。因为经过缩放后的特征向量也是A的特征向量并且特征值不变，所以一般就考虑单位特征向量。单位特征向量就是其2范数值为1的特征向量。假设A有n个特征向量$V={v^{(1)},v^{(2)},…,v^{(n)}}$，同时对应了n个特征值。那么以矩阵的形式可以表示为$A=Vdiag\lambda V^{-1}$这里的diag是将$\lambda $表示为只有正对角线有元素其它为0的矩阵。每个实对称矩阵都可以分解出实特征向量和实特征值。特征值都是正数的称为正定矩阵，特征值为非负数的称为半正定。<br><strong>奇异值分解(SVD)</strong>：奇异值分解可以得到和特征分解相似的信息，分别是奇异向量和奇异值。所有实数矩阵都能进行奇异值分解，它应用面更广，因为特征分解要求更苛刻，能进行奇异分解不一定能进行特征分解。<br><strong>迹</strong>：运算是返回矩阵对角元素的和。<br><strong>行列式</strong>：det(A)是将A映射到实数域的函数。行列式等同于特征值的乘积，行列式的绝对值可以用来衡量矩阵参与矩阵乘法后空间扩大或者缩小了多少。<br><strong>主成分分析(PCA)</strong>：<br>这篇<a href="http://blog.codinglabs.org/articles/pca-tutorial.html" target="_blank" rel="external">PCA的数学原理(作者 张洋)</a>给出了很详细的说明。以下是根据这篇文章总结或者摘录的一些内容。主成分分析主要用来给数据进行降维操作。在进行机器学习过程中使用的数据往往维度很高，我们要做的事情是将维度降低以此降低数据处理的难度同时对于内存的需求也同步降低。</p>
<p>假设我们有一组数据分别用来表示得分、颜色、金额、可以理解最能够表现这一组数据特点的应该变化范围也就是方差较为大的一项，因为如果多组数据的某项比如颜色完全就没有变化始终是保持一致那就并不能提供信息很多信息给我们。即是去掉它对于我们分析数据的影响也不大。</p>
<p>同时现实中数据和数据之间可能是有关系的，比如得分越高的时候金额也越高，那这个时候是不是可以考虑只保留得分和金额中的一项数据。这个两项数据之间的关系度可以通过协方差来度量。现在我们需要对数据安装方差大和协方差为0（数据间相互独立）的要求去处理。</p>
<p>上文中作者提到了一种将二维数据降低到一维的情形非常形象。</p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> basic skill </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[ubuntu上安装tensorflow笔记]]></title>
      <url>/2017/06/27/reinforcementLearning/</url>
      <content type="html"><![CDATA[<p>大概有5天的时候花在购置的组装设备的软件安装上，有点费时间。主机的硬件配置如下。<br>显卡：GTX 1080Ti AORUS<br>系统：ubuntu 16.04.2<br>内存：16G*2（安装两倍显存来配置）Kingston Fury DDR4 2400 16G<br>机械硬盘：SEAGATE 2TB 7200 转 64M SATA3<br>固态硬盘：SAMSUNG 850 EVO 250G SATA3<br>主板：z270-Gaming<br>cpu：i7-7700k<br>机箱：技嘉和谐号黑色（机箱没有自带散热扇）<br>cpu散热器：DeepCool 大霜塔<br><a id="more"></a><br>总价：14000RMB。因为各个部件的价格随着时间变化会比较大比如GPU，内存条，固态硬盘，所以也就没有贴出各个零部件的单价只是贴出了总价。零部件购置回来后请店家帮助完成装机后就自己来安装系统和软件</p>
<h3 id="ubuntu系统安装"><a href="#ubuntu系统安装" class="headerlink" title="ubuntu系统安装"></a>ubuntu系统安装</h3><p>使用UltralOS制作U盘引导盘，详细可以百度。<br>在安装系统的时候我选择系统默认分区。并没有自己去分区。安装好系统后需要主机修改启动引导也就是将让BIOS从安装了系统的硬盘启动对于我来说就是从SAMSUNG的那个固态硬盘去启动具体方法是在启动的时候按F12进入boot menu 选择SETUP然后将有系统的硬盘放置在第一个优先启动位置</p>
<h3 id="机械硬盘挂载"><a href="#机械硬盘挂载" class="headerlink" title="机械硬盘挂载"></a>机械硬盘挂载</h3><p>安装好系统后对<a href="http://www.cnblogs.com/hnrainll/archive/2012/02/27/2369331.html" target="_blank" rel="external">机械硬盘挂载</a>使用<code>sudo blkid</code>获取挂载硬盘名称，类型，uuid。这个在设置系统启动时自动挂载硬盘有用。<br>以下说明引用自<a href="https://www.linuxdashen.com/how-to-automount-file-systems-on-linux" target="_blank" rel="external">Linux系统如何开机自动挂载硬盘分区</a>。       </p>
<blockquote>
<p>交换分区的挂载点要设为none。<br>defaults挂载选项将给予用户读取和写入的权限。<br>dump的值通常是数字0。<br>Linux在开机时会用fsck工具检查文件系统，如果你需要在开机时检查你的硬盘分区，那么将pass的值设为2。根分区的pass值为1。这就是说，开机时，首先检查根分区文件系统，然后检查其他pass值为2的分区。交换分区不需要检查，所以它的pass值为数字0。根分区和交换分区之外的分区，通常pass值为2。如果你不需要检查文件系统，可以将pass值设为数字0。    </p>
</blockquote>
<h3 id="开启SSH服务"><a href="#开启SSH服务" class="headerlink" title="开启SSH服务"></a>开启SSH服务</h3><p>安装ssh服务可以方便远程连接主机在ubuntu下使用<code>sudo apt-get install openssh-server</code>命令，通过<code>ps -e | grep ssh</code>查看ssh是否启动，如果显示<code>sshd</code>便是已经成功启动。使用<code>ssh HostName@IPAddress</code>的格式去登陆，如果出现<code>WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!</code> 的错误，可以选择将known_hosts文件,把里面与所要连接IP相关的内容删掉即可。</p>
<h3 id="其它设置"><a href="#其它设置" class="headerlink" title="其它设置"></a>其它设置</h3><p>查看主机名称：hostname<br>可能需要设置主机名称，临时修改使用<code>hostname YOURHOSTNAME</code>即可，永久修改主机名称对此文件<code>/etc/hostname</code>进行修改。<br>修改ubuntu终端目录颜色<br>将<code>export LS_COLORS=${LS_COLORS}&#39;di=01;36&#39;:</code>输入到<code>~/.bashrc</code>中设置目录颜色为清蓝色。<br>菜单栏位置<br>把菜单栏从左侧变换到底部的命令是：<code>gsettings set com.canonical.Unity.Launcher launcher-position Bottom</code><br>把菜单栏从底部变换到左侧的命令是：<code>gsettings set com.canonical.Unity.Launcher launcher-position Left</code><br>显示器亮度调节<br>在ubuntu的系统下是不能通过系统设置来进行屏幕亮度调节，后来找了好些方法基本上是适用于笔记本的比如<code>Fn + 方向键</code>调节之类的。后来找到解决方便时，通过调节台式机显示器的物理按键来调节亮度，是不是很意外呀。<br>使用 sudo apt install + 安装软件，如果是deb包使用 dpkg -i file.deb来安装。</p>
<p>安装sublime后编辑文件准备保存的时候需要获取管理员权限，可以通过<code>which 软件名</code>或者<code>whereis 软件名</code>找到sublime的安装目录，然后使用sudo运行它，这样就给它管理员权限了。</p>
<p>安装sublime<a href="blog.csdn.net/zhaoyu106/article/details/52858962">参考</a><br>安装chrome<a href="blog.csdn.net/qq_30164225/article/details/54632634">参考</a>   </p>
<h3 id="GPU驱动安装"><a href="#GPU驱动安装" class="headerlink" title="GPU驱动安装"></a>GPU驱动安装</h3><p>第一步禁用 nouveau 驱动打开终端，输入<code>$ sudo gedit /etc/modprobe.d/blacklist-nouveau.conf</code>,输入：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">blacklist nouveau</div><div class="line">options nouveau modset=0</div></pre></td></tr></table></figure></p>
<p>保存退出，执行<code>$ sudo update-initramfs -u</code>，禁用结束。重启电脑。验证驱动是否禁用成功输入<code>sudo lspci | grep nouveau</code>如果没有内容，则禁用成功。</p>
<p>第二步 lspci | grep -i nvidia 通过此命令查看nvidia显卡的类型，在没有安装驱动的时候也应该可以查看到显卡型号信息。如果是1080ti的显卡：<code>01:00。0 VGA compatible controller:NVIDIA Corporation GP102[GeForce GTX 1080 Ti] (rev a1)</code><br>如果没有出现这些信息可以尝试<code>sudo update-pciids</code>来更新下PCI试试。此处<a href="http://docs.nvidia.com/cuda/cuda-installation-guide-linux/#axzz4VZnqTJ2A" target="_blank" rel="external">参考</a></p>
<p>第三步在<a href="http://www.geforce.cn/drivers" target="_blank" rel="external">nvidia官网</a>查看对应型号GPU的驱动程序，我这里使用的是381。当然我参考的其它安装教程也有使用378的，我也试过也可以使用但是安装后使用<code>nvidia-smi</code>来查看显卡信息时，显卡型号在378驱动下显示的是<code>GraphicDevice</code>而381可以显示<code>1080ti</code>。<br>显卡驱动安装381<br>先执行<code>sudo add-apt-repository ppa:graphics-drivers/ppa</code>添加源，再<br><code>sudo apt update</code>更新源。<br><code>sudo apt upgrade</code>根据差异进行软件更新。<br><code>sudo apt install nvidia-381</code>安装对应型号的驱动。<br>查看显卡驱动信息可以使用<code>nvidia-smi</code>查看显卡驱动是否安装好可以使用<code>glxinfo | head</code>得到输出信息中<code>direct rendering：Yes</code> 表示安装好了。如果对于安装不满意可以使用下面的命令卸载:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sudo apt-get remove --purge nvidia*</div><div class="line">sudo apt-get autoremove</div><div class="line">sudo reboot</div></pre></td></tr></table></figure></p>
<p>我是在第三步中先关闭图形界面<code>sudo service lightdm stop</code>完成显卡安装后再打开<code>sudo service lightdm start</code>。在安装了显卡以后需要安装<a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="external">cuda</a>和<a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="external">cudnn</a>其中cuda安装cuda 8.0然后cudnn安装cudnn 5.1。<br>cuda下载下来后使用<code>sudo sh cuda_8.0.61_375.26_linux.run</code>进行安装开始有协议条款阅读可以使用<code>q</code>结束阅读开始安装，在安装过程中遇到让你选择是否安装显卡驱动时记得选择否，因为我们已经安装了显卡驱动。安装完cuda后将这些路径添加到~/.bashrc文件中去<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">export PATH=/usr/local/cuda-8.0/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</div><div class="line">export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</div></pre></td></tr></table></figure></p>
<p>再执行<code>source ~/.bashrc</code><br>如果有patch包，就将其下载然后同样适用<code>sudo sh cuda_8.0.61.2_linux.run</code><br>安装好后可以参考<a href="http://www.52nlp.cn/深度学习主机环境配置-ubuntu16-04-geforce-gtx1080-tensorflow" target="_blank" rel="external">深度学习主机环境配置: Ubuntu16.04+GeForce GTX 1080+TensorFlow</a>做一些cuda的例子的运行。</p>
<p><code>tar -zxvf cudnn-8.0-linux-x64-v5.1.tgz</code>解压cudnn压缩包，然后使用下面的命令进行拷贝：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">sudo cp cuda/include/cudnn.h /usr/local/cuda/include/</div><div class="line">sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/</div><div class="line">sudo chmod a+r /usr/local/cuda/include/cudnn.h</div><div class="line">sudo chmod a+r /usr/local/cuda/lib64/libcudnn*</div></pre></td></tr></table></figure></p>
<h3 id="安装tensorflow"><a href="#安装tensorflow" class="headerlink" title="安装tensorflow"></a>安装tensorflow</h3><p>因为ubuntu 16.04.2已经自带了python3.5所以我就不需要重复安装了.选择使用bazel编译tensorflow源码来进行安装的形式，安装的bazel是通过pip3的形式来安装这个和一些教程不太一样。当安装好后可以试试<code>import tensorflow as tf</code>在一些教程中提到这样会提示<code>successfully opened CUDA library</code>但是我安装好以后并没有出现这个提示，主要就是纠结在这里，然后使用<code>watch nvidia-smi</code>查看在运行tensorflow源码中的mnist例子时gpu的使用情况，发现会有28%左右的显存被使用，应该是在利用gpu。后来看到一个tensorflow源码github库上有个issue【<a href="https://github.com/tensorflow/tensorflow/issues/10827" target="_blank" rel="external">version 1.2 doesn’t show CUDA and cuDNN information</a>】似乎是当前这个1.2版本的tensorflow都有这个情况。所以我暂时也就没有管了。后来使用<code>git checkout r1.0</code>使用1.0版本来进行安装就能出现<code>successfully opened CUDA library</code>提示信息。</p>
<p>参考此<a href="http://blog.csdn.net/uselym/article/details/73556511" target="_blank" rel="external">文章</a></p>
<pre><code>更新软连接，这里最好ls一下lib64的内容，防止版本不一致：   
$cd /usr/local/cuda/lib64/  
$sudo chmod +r libcudnn.so.5.1.10    
$sudo ln -sf libcudnn.so.5.1.10 libcudnn.so.5     
$sudo ln -sf libcudnn.so.5 libcudnn.so     
更新设置：      
$sudo ldconfig
</code></pre><p>后续在运行一个GAN的样例的时候出现<code>ImportError:libcudart.so.8.0:cannot open shared...</code>的错误。我找到的处理方法是<a href="http://blog.csdn.net/u010454261/article/details/71268325?utm_source=itdadao&amp;utm_medium=referral" target="_blank" rel="external">cuda执行出错解决方法</a>大概是这样.     </p>
<p>执行以下命令，将相应的库文件复制到/usr/lib<br><code>sudo cp /usr/local/cuda-8.0/lib64/libcudart.so.8.0 /usr/local/lib/libcudart.so.8.0 &amp;&amp; sudo ldconfig</code>    </p>
<p>结果是可以正常运行。   </p>
<h3 id="使用virtualenv安装tensorflow"><a href="#使用virtualenv安装tensorflow" class="headerlink" title="使用virtualenv安装tensorflow"></a>使用virtualenv安装tensorflow</h3><p>在安装好了基于python3的gpu版本的tensorflow后，我再使用virtualenv安装了基于python2.7的cpu版本tensorflow。<br>首先安装python2.7适配的virtualenv：<br><code>sudo apt-get install python-pip python-dev python-virtualenv</code></p>
<p>然后创建工作目录<code>targetDirectory</code>这个目录我选择的是~/tensorflow.<br><code>virtualenv --system-site-packages targetDirectory</code></p>
<p>激活虚拟环境:<br><code>source ~/tensorflow/bin/activate</code><br>去激活使用 deactivate</p>
<p>安装tensorflow:<br><code>(tensorflow)$ pip install --upgrade tensorflow</code>   </p>
<p><strong>注意</strong>：在虚拟环境下运行tensorflow的例子时，<code>sudo python demo.py</code>和<code>python demo.py</code>不一样，<code>sudo</code>一般代表系统默认的<code>python</code>环境。对于在<code>virtualenv</code>环境下使用<code>python demo.py</code>。参考此<a href="https://segmentfault.com/q/1010000008425727" target="_blank" rel="external">文章</a></p>
<h3 id="gpu版本指定gpu"><a href="#gpu版本指定gpu" class="headerlink" title="gpu版本指定gpu"></a>gpu版本指定gpu</h3><p><a href="http://www.cnblogs.com/darkknightzh/p/6591923.html" target="_blank" rel="external">参考:（原）tensorflow中使用指定的GPU及GPU显存</a><br>在实际应用gpu版本的tensorflow时间，如果存在多个gpu，tensorflow会默认将所有的gpu都占用。这个可能不是你所想要的。你可以通过<code>echo {$CUDA_VISIBLE_DEVICES}</code>来查看环境变量<code>CUDA_VISIBLE_DEVICES</code>的值，使用<code>export CUDA_VISIBLE_DEVICES=0</code>表示设置使用0号gpu设备。设置<code>export CUDA_VISIBLE_DEVICES=0,2,3</code>表示使用第0，2，3号gpu设备。如果<code>export CUDA_VISIBLE_DEVICES=&quot;&quot;</code>表示不使用gpu设备。</p>
<p>参考文章:<br><a href="https://www.tensorflow.org/install/install_linux" target="_blank" rel="external">Installing TensorFlow on Ubuntu</a><br><a href="http://www.52nlp.cn/深度学习主机环境配置-ubuntu16-04-geforce-gtx1080-tensorflow" target="_blank" rel="external">深度学习主机环境配置: Ubuntu16.04+GeForce GTX 1080+TensorFlow</a><br><a href="https://blog.slavv.com/the-1700-great-deep-learning-box-assembly-setup-and-benchmarks-148c5ebe6415" target="_blank" rel="external">The $1700 great Deep Learning box: Assembly, setup and benchmarks</a></p>
]]></content>
      
        <categories>
            
            <category> tool </category>
            
        </categories>
        
        
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[word embedding学习笔记]]></title>
      <url>/2017/06/17/wordembedding/</url>
      <content type="html"><![CDATA[<p>我们在进行神经网络的训练时，输入的数据可以是图片，也可以是文字。对于图片来讲，图片拥有的所有的信息都已经存在，就是那些色彩通道值和像素点。我们可以将一个图片的信息放置在一个高维空间作为一个点，而很多个类似的图片可能在空间中就会出现某个固定的分布。但是对于文字呢？文字是人创造出来的符号。这个符号背后的意义是我们人赋予的，但是机器并不了解，或者说单个的词本身并没有含义。就像我们看到一个非母语词汇也会完全摸不到头脑一样。<br><a id="more"></a></p>
<h2 id="one-hot"><a href="#one-hot" class="headerlink" title="one-hot"></a>one-hot</h2><p>我们要做事情是第一让机器可以理解词，其中一个方法就是使用词向量，将一个词表示为一个向量的意思。一种最简单的方式是one-hot。实现方式是建立一个词表V，|V|表示词表的大小。用长度是|V|的且第n个位置是1词向量来表示一个在词表V中索引是n的单词。但是这种方法并不能体现出词与词之间的关系，因为它们都各自占据了一个位置完全没有交集，比如说“黑猫”，“白猫”在使用one-hot表示体现不出这两个词都表示猫这样的相似性。而且这样的表示对空间的需求量极大。</p>
<h2 id="word2vector"><a href="#word2vector" class="headerlink" title="word2vector"></a>word2vector</h2><p>word2vector是word embedding思想的一种实现(以下以CBOW举例)，我们的目的就是让一个词向量可以表示一个词更多的信息。然后这个获得的过程可以通过训练得到，过程是这样的。首先我们创建两个矩阵$\nu \in R^{n\times |V|}$和$\upsilon \in R^{|V|\times n}$其中n表示词向量的维度。$\nu$是输入词矩阵，$\upsilon$是输出词矩阵。</p>
<ol>
<li>我们从输入的上下文中产生one-hot词向量，m:($x^(c-m),…x^(c-1),x^(c+1),…x^(c+m)\in R^{|V|}$).</li>
<li>获得前入词向量($v_{c-m}=\nu x^(c-m),v_{c-m+1}=\nu x^(c-m+1),…,v_{c+m}=\nu x^(c+m)\in R^{n}$).</li>
<li>取这些向量的平均值$\hat{v}=\frac{v_{c-m}+v_{c-m+1}…v_{c+m}}{2m}\in R^n$.</li>
<li>参生一个得分矩阵$z=\upsilon \hat{v}\in R^{|V|}$.相似的向量的点积结果更大，相似的单词得分也会相近也就会倾向分布在同一个区域。</li>
<li>将得分进行转换为概率$\hat{y}=softmax(z)\in R^|V|$.</li>
<li>希望这样产生的概率$\hat{y}\in R^|V|$，和真实的概率，$y\in R^|V|$匹配。而真实的概率就是对应词的one-hot向量。</li>
</ol>
<p>对应的图形是：<br><img src="/image/wordembd/cbow.png" width="60%" height="60%/">  </p>
<p>在训练过程中我们还需要一个损失函数，很自然的就可以想到交叉熵损失函数。<br>$$H(\hat{y},y)=-\sum_{j=1}^{|V|}y_{j}log(\hat{y}_{j})$$<br>因为y是一个one-hot向量实际只有一个位置是1，所以上面的式子可以进一步化简为。<br>$$H(\hat{y},y)=-y_{j}log(\hat{y}_{j})$$<br>接下来我们最小化的对象就变成了如下形式：<br>$min J=-logP(w_c|w_{c-m},…,w_{c-1},w_{c+1},…,w_{c+m})$<br>$=-logP(u_c|\hat{v})$<br>$=-log\frac{exp(u^{T}_{c}\hat{v})}{\sum_{j=1}^{V}exp(u^{T}_{j}\hat{v})}$<br>$=-u^{T}_{c}\hat{v}+log\sum_{j=1}^{|V|}exp(u^{T}_{j}\hat{v})$<br>我们可以使用随机梯度下降来进行计算更新$u_{c}$和$v_{j}$。$u_{c}$就是我们需要的词向量。    </p>
<p>参考文件：<br><a href="http://web.stanford.edu/class/cs224n/lecture_notes/cs224n-2017-notes1.pdf" target="_blank" rel="external">[1] cs224n-2017-notes1</a><br><a href="http://blog.csdn.net/han_xiaoyang/article/details/51567822" target="_blank" rel="external">[2] 深度学习与自然语言处理(1)_斯坦福cs224d Lecture 1</a></p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> word embedding </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[GANs对抗生成网络学习笔记]]></title>
      <url>/2017/06/16/GANs/</url>
      <content type="html"><![CDATA[<p>对抗生成网络主要由两部分组成，分别是Generator(生成器)和Discriminator(判别器)。提供一个随机噪声文件给G以后就会生成图片出来当然也可能是生成文字等其它信息，这里我们以生成图片为列子。然后D去判断这个生成的图片是真的还是假的。<br><a id="more"></a></p>
<h2 id="GANs的训练"><a href="#GANs的训练" class="headerlink" title="GANs的训练"></a>GANs的训练</h2><p>对于生成器和判别器来说我们都可以使用神经网络来实现。对于生成器来说我们希望它生成的图片越真越好，对于判别器来说我们希望它尽可能的区分生成的图片和真实的图片。这样生成器和判别器在这样的博弈中逐渐达到了一个判别器对于生成图片无法判断真假的情况便可以结束训练。</p>
<p>经过上面的描述只是一个感性的认识，但是就像我们其它的神经网络训练有一个损失函数来进行度量训练的程度，GANs的训练我们也应该有一个这样的损失函数。以图片为列子，一种类型的图片比如说卡通头像，一张图片可以看作是在高维空间中的一个点，而一个类型的图片在高维空间中应该是满足了某种分布的。而我们在训练的生成器的时候就是在将我们输入的随机噪声经过神经网络训练成和目标真实图片同样的高维空间的点，并且使我们生成的高维点的分布满满和目标真实图片的分布一致。</p>
<p>那这里我们可以通过去度量高维空间中的两个分布之间的距离表示我们训练的过程。距离越远生成数据越假，距离越近生成数据越真。在一般意义的GANs使用的是KL散度和JS散度来度量，后面改进后的W-GAN使用的是Wasserstein距离来度量。</p>
<h2 id="JS散度用来度量的问题"><a href="#JS散度用来度量的问题" class="headerlink" title="JS散度用来度量的问题"></a>JS散度用来度量的问题</h2><p>原始GANs的公式是:<br>$$\underset{G}{min} \underset{D}{max}V(D,G)=E_{x\sim pdata(x)}[logD(x)]+E_{z\sim pz(z)}[1-logD(G(z))]$$  <strong>公式1</strong><br>在这个公式中，D(x)是在对真实的图片进行判断，D(G(z))是在对生成的图片进行判断。那么当固定了G(z)的时候我们希望的就是D(x)越接近1越好。D(G(z))越接近0越好。这样就很好的分割了真确和错误。因为D(x)本身作为判断来讲的范围就是在0到1之间。所以以上的过程实际在给定G的条件下调整D来获取公式1最大值。</p>
<p>对于G(z)来说希望的是让D将其判断为1，所以对于<strong>公式1</strong>是最小化的过程。这里注意公式1中的$E_{x\sim pdata(x)}[logD(x)]$和G无关所以这个最小化过程不需要考虑它。就剩下了$E_{z\sim pz(z)}[1-logD(G(z))]$。<br>下面的内容引用自参考[2]</p>
<blockquote>
<p>根据原始GAN定义的判别器loss，我们可以得到最优判别器的形式；而在最优判别器下，我们可以把原始GAN定义的生成器loss等价变换为最小真实分布$P_{r}$与生成分布$P_{g}$之间的JS散度。我们越训练判别器，它就越接近最优，最小化生成器的loss就会越近似最小化$P_{r}$和$P_{g}$之间的JS散度。</p>
</blockquote>
<p>化简后与JS散度相关的度量$2JS(P_{r}||P_{g})-2log2$ <strong>公式2</strong><br>我们希望的是通过JS散度度量两个分布的距离然后根据这个距离将两个分布不断拉近，但是JS散度对于没有重合部分或者重合部分可以忽略的两个分布是起不到度量作用的。</p>
<h2 id="Wasserstein的优势"><a href="#Wasserstein的优势" class="headerlink" title="Wasserstein的优势"></a>Wasserstein的优势</h2><p>Wasserstein距离又叫<code>Earth-Mover推土机</code>的优势是即是两个分布之间没有重合的部分但是也能度量它们之间的距离并且随着距离的变化Wasserstein也是随着变化的。定义如下：<br>$$B(\gamma )=\sum_{x_{g},x_{data}}\gamma (x_{g},x_{data})\left | x_{g}-x_{data} \right |$$<br>这个公式表示将$x_{g}$分布移动到$x_{data}$和它们两个之间距离的乘积。对应推土机的比喻就是把一堆土移动到另外一堆和这两堆土之间的距离。我们要做的就是将这个当成一个最短路径规划问题，找到一个最快捷和不费事的方式完成。<br>$$W(g,data)=\underset{\gamma\in \prod }{min}B(\gamma)$$<br>下图来自参考[4]，这张图表示来JS散度和Wasserstein距离之间的差别。<br><img src="/image/GANs/earth-mover.png" width="70%" height="70%/"></p>
<p>Wasserstein距离来度量距离时使用如下公式：<br>$$W(P_{data},P_{G})=\underset{D\in 1-Lipschitz}{max} {E_{x\sim P_{data}}[D(x)]-E_{x\sim P_{G}}[D(x)]}$$<br>这里$\left | D(x_{1})-D(x_{2}) \right |\leq K\left | x_{1}-x_{2} \right |$当K=1时就是1-Lipschitz。主要作用就是限制D的变化范围不要太大了。在实际训练的时候采取的方式是对更新后的权值在一个小范围内进行截断。</p>
<h3 id="W-GAN的训练过程"><a href="#W-GAN的训练过程" class="headerlink" title="W-GAN的训练过程"></a>W-GAN的训练过程</h3><p><img src="/image/GANs/w-gans.jpg"><br>说明下这个内容。总的来说是先固定生成器，然后调整生成器参数使得代价函数获得最大值。然后固定判别器参数，调整生成器参数使得代价函数获得最小值。<br>$\alpha$表示学习率这个在更新神经网络的权值时会使用到。<br>c表示截断参数应用在判别器训练上，用来限制更新权值的范围如果更新后的权值大于c则赋为c小于-c则赋为-c。<br>m表示从原始数据中采样多少图片来作为一个batch的训练集。这里很重要的时候我们是采样数据来感知一个原始数据的分布情况，预先我们既不知道它们的分布。<br>$n_{critic}$表示我们在训练生成器的时候内部需要循环几次。   </p>
<h2 id="W-GAN的改进"><a href="#W-GAN的改进" class="headerlink" title="W-GAN的改进"></a>W-GAN的改进</h2><p>对W-GAN改进主要体现在对使用截断来进行限制权值更新上。后来提出添加一个惩罚项在loss函数中。具体讲$D\in 1-Lipschitz$等价于说$\left | \triangledown _xD(x)) \right | \leq 1$那么原公式可以优化为：<br>$$W(P_{data},P_{g})\approx \underset{D}{max}{E_{x\sim pdata}[D(x)]-E_{x\sim pg}-\lambda E_{x\sim penalty}(\left | \triangledown _{x}D(x) \right |-1)^2}$$<br>其中$P_{penalty}$表示在分布$P_{data}$和$P_{P_{g}}$之间点连线的采样点。下图来自参考[4]。<br><img src="/image/GANs/penalty.png" width="70%" height="70%/"></p>
<p>参考文件列表：<br><a href="https://zhuanlan.zhihu.com/p/24767059" target="_blank" rel="external">[1] GAN学习指南：从原理入门到制作生成Demo</a><br><a href="https://zhuanlan.zhihu.com/p/25071913" target="_blank" rel="external">[2] 令人拍案叫绝的Wasserstein GAN</a><br><a href="https://www.youtube.com/watch?v=0CKeqXl5IY0" target="_blank" rel="external">[3] MLDS Lecture 9: Generative Adversarial Network</a><br><a href="https://www.youtube.com/watch?v=KSN4QYgAtao" target="_blank" rel="external">[4] MLDS Lecture 10: Improved Generative Adversarial Network</a>  </p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> gans </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Batch Normalization 学习笔记]]></title>
      <url>/2017/06/15/BatchNorma/</url>
      <content type="html"><![CDATA[<p>参考文件列表：<br>[0] <a href="https://www.youtube.com/watch?v=-5hESl-Lj-4" target="_blank" rel="external">什么是 Batch Normalization 批标准化 (深度学习 deep learning)</a><br>[1] <a href="https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：反向传播笔记</a><br>[2] <a href="https://www.zhihu.com/question/38102762" target="_blank" rel="external">深度学习中 Batch Normalization为什么效果好？-魏秀参答</a><br>[3] <a href="http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html" target="_blank" rel="external">Must Know Tips/Tricks in Deep Neural Networks (by Xiu-Shen Wei)</a><br>[4] <a href="http://blog.csdn.net/hjimce/article/details/50866313" target="_blank" rel="external">深度学习（二十九）Batch Normalization 学习笔记</a><br>[5] <a href="http://www.jianshu.com/p/0312e04e4e83" target="_blank" rel="external">谈谈Tensorflow的Batch Normalization</a><br>[6] <a href="https://r2rt.com/implementing-batch-normalization-in-tensorflow.html" target="_blank" rel="external">Implementing Batch Normalization in Tensorflow</a><br>[7] <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html" target="_blank" rel="external">Understanding the backward pass through Batch Normalization Layer</a><br>[8] <a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="external">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a><br> <a id="more"></a><br>感激上面的这些参考文件，对于了解和学习Batch Normalization起到了很大的帮助。如果有人意外的看到了我这边笔记，我的建议是可以先看上面的这些参考文件。下面的内容主要是为了方便自己查阅Batch Normalization的内容而做的相关笔记。</p>
<h2 id="神经网络训练"><a href="#神经网络训练" class="headerlink" title="神经网络训练"></a>神经网络训练</h2><p>我们基本上可以对神经网络的形成一个印象，就是其训练过程对超参数配置有较为严格的要求，比如一般需要一个比较小的学习率，但是学习率小了也意味着训练时间的变长。同时需要警惕梯度消散的问题，因此添加了drop out或者l2正则化来减少影响，选择合适的激活函数比如relu。同时也包括对于输入数据的预处理比如“白化”。可以看出这个过程充满了各种技巧。</p>
<p>后面就有人去思考一些简单的方法来改变这个复杂的训练过程，其中一个就是Batch Normalization。这个可以同数据预处理中的归一化处理联系起来，一般我们经过归一化处理后的数据输入数据可以得到一个更好的训练结果(我猜)，既然可以对输入数据做这样的处理，那是不是也可以对卷积层，全连接层也做这样的处理呢？</p>
<h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>在对数据进行预处理的时候，我们将它们进行归一化后，这样如果是一个有效的变化区间在-1到1之间的tanh激活函数就很有利。如果没有这样操作输入的数据如果过大能直接落到了激活函数对于梯度感觉不明显的区域后续的训练就会变的很慢。所以实际上我们也是将这样的处理方式推广到了卷积层或者全连接层。</p>
<p>在训练的过程中数据是一批批的进入。所以我们也是对数据进行一批批的归一化处理。具体来说是对每个神经元接受的数据进行归一化，而且是在数据进入到激活函数前完成归一化操作。也就是对$Wx+b$这个阶段。Batch Normalization涉及到的主要公式如下：<br><img src="/image/batchNorm/batchnoram.jpg" weight="50%" height="50%/"><br>$$\mu _{\beta }\leftarrow \frac{1}{m}\sum_{i=1}^{m}x_{i}$$ <strong>公式1.</strong><br>$$\sigma ^{2}_{\beta }\leftarrow \frac{1}{m}\sum_{i=1}^{m}(x_{i}-\mu_{ \beta})^{2}$$ <strong>公式2.</strong><br>$$\hat{x}_{i}\leftarrow \frac{x_{i}-\mu _{\beta }}{\sqrt{\sigma ^{2}_{\beta }+\epsilon }}$$ <strong>公式3.</strong><br>$$y_{i}\leftarrow \gamma x_{i}+\beta $$ <strong>公式4.</strong><br>注意在公式中的$\epsilon$表示为了避免出现除0出现添加的一个很小的值，比如0.001。前三个公式表示了公式1求取平均值，公式2表示求取方差，公式3进行归一化将均值归为0将方差归为1。这里需要特别注意下公式4的作用是设置两个可学习参数$\beta$和$\gamma$，神经网络通过对者两个参数的学习将归一化的值回退为归一化前的值，这样增加了添加Batch Normalization之后的兼容性。</p>
<h3 id="Batch-Normalization-的计算"><a href="#Batch-Normalization-的计算" class="headerlink" title="Batch Normalization 的计算"></a>Batch Normalization 的计算</h3><p>其计算过程依然是普通的反向传播过程，参考[7]有完整的链式求导推导过程。以及配合参考[1]学习到使用计算图的方式理解反向传播的方式。</p>
<h3 id="Batch-Normalization-的实现"><a href="#Batch-Normalization-的实现" class="headerlink" title="Batch Normalization 的实现"></a>Batch Normalization 的实现</h3><p>其全连接层实现以及如何得到测试的时候的均值和方差可以参考[6]，关于在卷积层上的实现可以参考[4]。简单说在测试集使用的均值和方差来自训练时候对各个训练batch中均值和方差的平均值。而在卷积层上的时候是配套参数共享的特点将每个卷积层中的特征map视为一个神经元对其输入求均值和方差。</p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> batch norm </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[matplotlib笔记]]></title>
      <url>/2017/06/07/matplot/</url>
      <content type="html"><![CDATA[<p>在机器学习或者深度学习中，matplotlib都是一个可视化数据的好工具。它可以辅助开发者了解数据的情况，下面我们就简单的以一些列子来学习matplotlib。<br>以下例子中的代码和图形引用自此<a href="https://zhuanlan.zhihu.com/p/26660699" target="_blank" rel="external">文章</a></p>
<a id="more"></a>
<h2 id="显示多子图"><a href="#显示多子图" class="headerlink" title="显示多子图"></a>显示多子图</h2><p>在matplotlib中，整个图像为一个Figure对象。在Figure对象中可以包含一个，或者多个Axes对象。每个Axes对象都是一个拥有自己坐标系统的绘图区域。</p>
<p><code>ax.imshow(images[i].reshape(img_shape), cmap=&#39;binary&#39;)</code>这个函数是在axes中绘制图像，第一个参数是传入来整理过形状的绘制数据，cmap是color map可以<a href="http://matplotlib.org/examples/color/colormaps_reference.html" target="_blank" rel="external">对照</a>选择你喜欢的颜色，这里选择的 binary 就是普通的黑白。在python3中很多使用来<code>format</code>来进行格式化可以参考此<a href="关于format的用法可以参考http://blog.xiayf.cn/2013/01/26/python-string-format/">文章</a></p>
<p>绘图举例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_images</span><span class="params">(images, cls_true, cls_pred=None)</span>:</span></div><div class="line">    <span class="keyword">assert</span> len(images) == len(cls_true) == <span class="number">9</span></div><div class="line">    <span class="comment"># 创建一个拥有子图的图片，返回图片对象和轴</span></div><div class="line">    fig, axes = plt.subplots(<span class="number">3</span>, <span class="number">3</span>)</div><div class="line">    <span class="comment"># 调整子图在整个图片中的宽和高的间隔</span></div><div class="line">    fig.subplots_adjust(hspace=<span class="number">0.2</span>, wspace=<span class="number">0.3</span>)</div><div class="line">    <span class="comment"># 使用flat属性来提取子对象，flat数组元素的迭代器</span></div><div class="line">    <span class="comment"># enumerate 返回位置索引和该位置上的对象</span></div><div class="line">    <span class="keyword">for</span> i, ax <span class="keyword">in</span> enumerate(axes.flat):</div><div class="line">        <span class="comment"># 在ax中展示图形。输入第一个参数是子图片的尺寸。cmap是color map</span></div><div class="line">        <span class="comment"># 选择你喜欢的颜色，这里选择的 binary 就是普通的黑白</span></div><div class="line">        ax.imshow(images[i].reshape(img_shape), cmap=<span class="string">'binary'</span>)</div><div class="line">        <span class="comment"># ax.imshow(images[i].reshape(img_shape), cmap='summer')</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> cls_pred <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            xlabel = <span class="string">"True:&#123;0&#125;"</span>.format(cls_true[i])</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            xlabel = <span class="string">"True:&#123;0&#125;, Pred:&#123;1&#125;"</span>.format(cls_true[i], cls_pred[i])</div><div class="line">        <span class="comment"># 在x方向设置label</span></div><div class="line">        ax.set_xlabel(xlabel)</div><div class="line">        <span class="comment"># 设定x和y轴的标签，这里[]是空意思就是不现实这些标签。</span></div><div class="line">        ax.set_xticks([])</div><div class="line">        ax.set_yticks([])</div><div class="line">    <span class="comment"># 展示图片</span></div><div class="line">    plt.show()</div></pre></td></tr></table></figure>
<h2 id="绘制混淆矩阵"><a href="#绘制混淆矩阵" class="headerlink" title="绘制混淆矩阵"></a>绘制混淆矩阵</h2><p>在机器学习或者深度学习中我们也存在需要去查看绘制的一个混淆矩阵的需求，用来可视化预测值和实际值之间的差异。方法是先<a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html" target="_blank" rel="external">生成一个混淆矩阵</a>，再将它绘制出来。下面的例子中涉及到一个<code>plt.tight_layout</code>是用来对图片进行<a href="https://stackoverflow.com/questions/9603230/how-to-use-matplotlib-tight-layout-with-figure" target="_blank" rel="external">细微调整</a>。同时说明下<code>plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=plt.cm.Blues)</code>这里的cmap可以填写字符串<code>Blues</code>也可以填写<code>plt.cm.Blues</code>。具体颜色可以参考此<a href="http://matplotlib.org/examples/color/colormaps_reference.html" target="_blank" rel="external">表</a>。</p>
<p>举例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_confusion_matrix</span><span class="params">()</span>:</span></div><div class="line">    cls_true = data.test.cls</div><div class="line"></div><div class="line">    cls_pred = session.run(y_pred_cls, feed_dict=feed_dict_test)</div><div class="line">    <span class="comment"># 输入分布是实际的标签值，和模型预测的标签值。</span></div><div class="line">    cm = confusion_matrix(y_true=cls_true, y_pred=cls_pred)</div><div class="line">    <span class="comment"># 绘制图片interpolation='nearest'是图像处理的一种方式</span></div><div class="line">    <span class="comment"># 选择你喜欢的颜色，这里选择的 binary 就是普通的黑白</span></div><div class="line">    plt.imshow(cm, interpolation=<span class="string">'nearest'</span>, cmap=plt.cm.Blues)</div><div class="line">    <span class="comment"># 自动调整图片如果是多个图片的话会自动调整图片的间距具体可以查看此链接</span></div><div class="line">    plt.tight_layout()</div><div class="line">    <span class="comment"># 添加一个色条</span></div><div class="line">    plt.colorbar()</div><div class="line">    tick_marks = np.arange(num_classes)</div><div class="line">    <span class="comment"># 将tick_marks对应位置表示为num_classes，也就是画出标尺</span></div><div class="line">    plt.xticks(tick_marks, range(num_classes))</div><div class="line">    plt.yticks(tick_marks, range(num_classes))</div><div class="line"></div><div class="line">    <span class="comment"># 设置x轴标签为预测</span></div><div class="line">    plt.xlabel(<span class="string">'Predicted'</span>)</div><div class="line">    <span class="comment"># 设置y轴标签为真实</span></div><div class="line">    plt.ylabel(<span class="string">'True'</span>)</div><div class="line">    <span class="comment"># 展示图片</span></div><div class="line">    plt.show()</div></pre></td></tr></table></figure>
<p><img src="/image/confusion.png"></p>
<p>举例：<br>下图是可视化权值的分布情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 绘制权值变化</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_weights</span><span class="params">()</span>:</span></div><div class="line">    w = session.run(weights)</div><div class="line">    <span class="comment"># 获取权值的最大和最小值作为后续画图的范围使用。</span></div><div class="line">    w_min = np.min(w)</div><div class="line">    w_max = np.max(w)</div><div class="line"></div><div class="line">    fig, axes = plt.subplots(<span class="number">3</span>, <span class="number">4</span>)</div><div class="line">    fig.subplots_adjust(hspace=<span class="number">0.3</span>, wspace=<span class="number">0.3</span>)</div><div class="line"> </div><div class="line">    <span class="keyword">for</span> i, ax <span class="keyword">in</span> enumerate(axes.flat):</div><div class="line">        <span class="keyword">if</span> i&lt;<span class="number">10</span>:</div><div class="line">            <span class="comment"># 一个数字一个子图</span></div><div class="line">            image = w[:, i].reshape(img_shape)</div><div class="line"> </div><div class="line">            ax.set_xlabel(<span class="string">"Weights: &#123;0&#125;"</span>.format(i))</div><div class="line">            <span class="comment"># 可以在上文中的颜色列表中看到'seismic'是一种一半蓝色一半红色的渐变颜色</span></div><div class="line">            <span class="comment"># 用来做对比很合适，这里vmin=w_min, vmax=w_max也设定了每个子图的变化</span></div><div class="line">            <span class="comment"># 都是在一个范围的。</span></div><div class="line">            ax.imshow(image, vmin=w_min, vmax=w_max, cmap=<span class="string">'seismic'</span>)</div><div class="line"></div><div class="line">        ax.set_xticks([])</div><div class="line">        ax.set_yticks([]) </div><div class="line"></div><div class="line">    plt.show()</div></pre></td></tr></table></figure>
<p><img src="/image/plotweight.png"></p>
]]></content>
      
        <categories>
            
            <category> tool </category>
            
        </categories>
        
        
        <tags>
            
            <tag> matplot </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[迁移学习Tensorflow]]></title>
      <url>/2017/06/06/TransLearnByTF/</url>
      <content type="html"><![CDATA[<p>参考：<a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/08_Transfer_Learning.ipynb" target="_blank" rel="external">Transfer Learning</a><br>在先前的<a href="http://www.memorycrash.cn/2017/05/21/TransLearning/" target="_blank" rel="external">文章</a>有涉及到使用Pytorch实现迁移学习。迁移学习的思路是基于一个在大量数据上训练好的神经网络，通过它作为一个特征提取器，将提取出来的特征输入一个新的神经网络中进一步训练得到我们需要的结果。这样做比起完全的从头开始训练一个神经网络可以节约很大一部分的时间。<br><a id="more"></a></p>
<h2 id="基于Tensorflow"><a href="#基于Tensorflow" class="headerlink" title="基于Tensorflow"></a>基于Tensorflow</h2><p>我们准备基于<code>inception v3</code>的迁移学习再去训练<code>cifar10</code>的分类，如何去实现？我们首先选择<code>inception v3</code> 这个基于imageNet的大量文件训练好的神经网络。这个网络并不会在tensorflow中提供，需要单独下载。参考文件中提供了一个<a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/inception.py" target="_blank" rel="external">inception.py</a>文件里面集成了<code>inception v3</code>的下载地址和解析加载模型的方法。下面给出inception v3的大致结构图。<br><img src="/image/transLearnTF/inceptionV3Trans.png"><br>从这个图中根据图例我们可以看出来最后是选择了<code>AvgPool</code>的输出作为特征进行重新训练。<br><strong>注意1</strong>：graph.get_tensor_by_name(name)可以根据名称来获得输出Tensor。我们基于这个函数实现特征的获取。<a href="https://stackoverflow.com/questions/35336648/list-of-tensor-names-in-graph-in-tensorflow/" target="_blank" rel="external">参考stackoverflow</a> 。<br><strong>注意2</strong>：我们需要获取的层是除开作为输出softmax层和全连接层后的最后一层。通过graph.get_operations()来获取层名称信息，我们需要的层是<code>pool3:0</code>。<a href="https://stackoverflow.com/questions/35336648/list-of-tensor-names-in-graph-in-tensorflow/" target="_blank" rel="external">参考1 stackoverflow</a><a href="https://github.com/AKSHAYUBHAT/VisualSearchServer/blob/master/notebooks/notebook_network.ipynb" target="_blank" rel="external">参考2 inception v3模型结构信息</a>。</p>
<h3 id="训练数据处理"><a href="#训练数据处理" class="headerlink" title="训练数据处理"></a>训练数据处理</h3><p>训练数据就是<code>cifar10</code>这里参考文件中同样提供来一个<a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/cifar10.py" target="_blank" rel="external">cifar10.py</a>文件来实现<code>cifar10</code>数据的下载。<code>cifar10</code>是一个拥有6万数据量的10个分类的数据集，相比于<code>imageNet</code>的数据少了很多。我们要做的事下载数据后将这些数据进行特征提取也就是输入到<code>inception v3</code>中去处理，将<code>pool3:0</code>位置的tensor数据取出来并进行保存。这些保存好的数据就是我们迁移到新的神经网络上的输入数据。   </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> cifar10 <span class="comment"># 这是在利用自行编辑的数据下载文件</span></div><div class="line"><span class="keyword">from</span> cifar10 <span class="keyword">import</span> num_classes</div><div class="line"><span class="comment"># 下载数据并解压</span></div><div class="line">cifar10.maybe_download_and_extract()</div><div class="line"><span class="comment"># 获取分类名</span></div><div class="line">class_names = cifar10.load_class_names()</div><div class="line"><span class="comment"># 分别加载训练和验证数据</span></div><div class="line">images_train, cls_train, labels_train = cifar10.load_training_data()</div><div class="line">images_test, cls_test, labels_test = cifar10.load_test_data()</div></pre></td></tr></table></figure>
<h3 id="加载模型"><a href="#加载模型" class="headerlink" title="加载模型"></a>加载模型</h3><p>利用<code>inception.py</code>文件加载模型数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> incepton <span class="comment"># 这是在利用自行编辑的inception下载处理文件</span></div><div class="line"><span class="keyword">from</span> inception <span class="keyword">import</span> transfer_values_cache</div><div class="line"></div><div class="line"><span class="comment"># 下载inception文件，具体实现细节可以看上文中的链接了解</span></div><div class="line">inception.maybe_download()</div><div class="line"><span class="comment"># 创建incepton模型对象</span></div><div class="line">model = inception.Inception()</div><div class="line"><span class="comment"># 设置transfer-value(可以理解为上文中特征提取值)的保存路径</span></div><div class="line">file_path_cache_train = os.path.join(cifar10.data_path, <span class="string">'inception_cifar10_train.pkl'</span>)</div><div class="line">file_path_cache_test = os.path.join(cifar10.data_path, <span class="string">'inception_cifar10_test.pkl'</span>)</div><div class="line"></div><div class="line"><span class="comment"># Scale images because Inception needs pixels to be between 0 and 255,</span></div><div class="line"><span class="comment"># while the CIFAR-10 functions return pixels between 0.0 and 1.0</span></div><div class="line">images_scaled = images_train * <span class="number">255.0</span></div><div class="line"></div><div class="line"><span class="comment"># If transfer-values have already been calculated then reload them,</span></div><div class="line"><span class="comment"># otherwise calculate them and save them to a cache-file.</span></div><div class="line">transfer_values_train = transfer_values_cache(cache_path=file_path_cache_train,</div><div class="line">                                              images=images_scaled,</div><div class="line">                                              model=model)</div><div class="line">                                              </div><div class="line">images_scaled = images_test * <span class="number">255.0</span></div><div class="line"></div><div class="line"><span class="comment"># If transfer-values have already been calculated then reload them,</span></div><div class="line"><span class="comment"># otherwise calculate them and save them to a cache-file.</span></div><div class="line">transfer_values_test = transfer_values_cache(cache_path=file_path_cache_test,</div><div class="line">                                             images=images_scaled,</div><div class="line">                                             model=model)</div></pre></td></tr></table></figure>
<h3 id="Transfer-values分析"><a href="#Transfer-values分析" class="headerlink" title="Transfer-values分析"></a>Transfer-values分析</h3><h4 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h4><p>使用PCA去分析并绘制出<code>cifar10</code>经过特征提取后的文件分布情况是否存在规律。因为特征提取后的文件依然是个高纬度的数据，要在图形画出来我们将维度降低到2维。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div><div class="line"></div><div class="line">pca = PCA(n_components=<span class="number">2</span>)</div><div class="line"><span class="comment"># 只选取3000个数据来观察</span></div><div class="line">transfer_values = transfer_values_train[<span class="number">0</span>:<span class="number">3000</span>]</div><div class="line">cls = cls_train[<span class="number">0</span>:<span class="number">3000</span>]</div><div class="line">transfer_values_reduced = pca.fit_transform(transfer_values)</div></pre></td></tr></table></figure>
<p>但是这样绘制出来的效果似乎并不好。</p>
<h4 id="t-SNE-进行分析"><a href="#t-SNE-进行分析" class="headerlink" title="t-SNE 进行分析"></a>t-SNE 进行分析</h4><p>还有一种方法是使用t-SNE来进行分析，但是这个方法运行速度比较慢，所以可以先使用PCA预降维，再使用t-SNE进行处理。后可以看出来数据的分布基本很有规律。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</div><div class="line"></div><div class="line">pca = PCA(n_components=<span class="number">50</span>)</div><div class="line">transfer_values_50d = pca.fit_transform(transfer_values)</div><div class="line">tsne = TSNE(n_components=<span class="number">2</span>)</div><div class="line">transfer_values_reduced = tsne.fit_transform(transfer_values_50d)</div></pre></td></tr></table></figure>
<p><img src="/image/transLearnTF/tSNEAnly.png"></p>
<p><strong>注意</strong>：以上详细代码请<a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/08_Transfer_Learning.ipynb" target="_blank" rel="external">参考</a>。</p>
<h3 id="构建新的神经网络"><a href="#构建新的神经网络" class="headerlink" title="构建新的神经网络"></a>构建新的神经网络</h3><p>这里使用Placeholder变量的方式来输入训练数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">transfer_len = model.transfer_len</div><div class="line">x = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, transfer_len], name=<span class="string">'x'</span>)</div><div class="line">y_true = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, num_classes], name=<span class="string">'y_true'</span>)</div></pre></td></tr></table></figure>
<p>然后使用<code>Pretty Tensor</code>(可以通过pip安装)来构建一个新的神经网络。这样的构建方式更加简单方便。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment"># Wrap the transfer-values as a Pretty Tensor object.</span></div><div class="line">x_pretty = pt.wrap(x)</div><div class="line"></div><div class="line"><span class="keyword">with</span> pt.defaults_scope(activation_fn=tf.nn.relu):</div><div class="line">    y_pred, loss = x_pretty.\</div><div class="line">        fully_connected(size=<span class="number">1024</span>, name=<span class="string">'layer_fc1'</span>).\</div><div class="line">        softmax_classifier(num_classes=num_classes, labels=y_true)</div></pre></td></tr></table></figure>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>使用下面的代码随机的选择数据批量的数据进行训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_batch</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># Number of images (transfer-values) in the training-set.</span></div><div class="line">    num_images = len(transfer_values_train)</div><div class="line"></div><div class="line">    <span class="comment"># Create a random index.</span></div><div class="line">    <span class="comment"># 下面的这个函数将随机生成train_batch_size个数据</span></div><div class="line">    idx = np.random.choice(num_images,</div><div class="line">                           size=train_batch_size,</div><div class="line">                           replace=<span class="keyword">False</span>)</div><div class="line"></div><div class="line">    <span class="comment"># Use the random index to select random x and y-values.</span></div><div class="line">    <span class="comment"># We use the transfer-values instead of images as x-values.</span></div><div class="line">    x_batch = transfer_values_train[idx]</div><div class="line">    y_batch = labels_train[idx]</div><div class="line"></div><div class="line">    <span class="keyword">return</span> x_batch, y_batch</div></pre></td></tr></table></figure>
<p><strong>注意</strong>：对应英文文档还有一个比较好的<a href="https://zhuanlan.zhihu.com/p/27093918" target="_blank" rel="external">中文翻译</a>可以对照学习。</p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> tensorflow </tag>
            
            <tag> transfer learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Tensorflow学习笔记]]></title>
      <url>/2017/06/04/tensorflow/</url>
      <content type="html"><![CDATA[<h2 id="Tensorflow基本使用"><a href="#Tensorflow基本使用" class="headerlink" title="Tensorflow基本使用"></a>Tensorflow基本使用</h2><ul>
<li>使用图(graph)来表示计算任务。</li>
<li>在被称之为<code>会话(Session)的上下文(context)</code>中执行图。</li>
<li>使用tensor来表示数据。</li>
<li>通过<code>变量(Variable)</code>维护状态。</li>
<li>通过feed和fetch可以为任意的操作(arbitrary operation)赋值或者从其中获取数据。<a id="more"></a>
在Tensorflow图由它的操作和变量/常量构成。操作就比如普通的矩阵乘法<code>matmul</code>等，变量指那些可以用来保存状态并进行更新的值<code>Variable</code>，常量是<code>constant</code>。这个图可以理解为一个管道结构，我们的目的是根据这个图来计算更新变量或者得到这个图的输出值，但是仅仅靠一个图结构还不能完成这个工作。我们还需要输入数据，类比着看就是有了管道以后还需要向管道中通水。</li>
</ul>
<p>这就引出了Tensorflow的<code>Session</code>我们的图结构需要在这个<code>Session</code>中运行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="comment"># 创建一个1X2的常量矩阵</span></div><div class="line">matrix1 = tf.constant([[<span class="number">3.</span>, <span class="number">3.</span>]])</div><div class="line"><span class="comment"># 创建一个2X1的常量矩阵</span></div><div class="line">matrix2 = tf.constant([[<span class="number">2.</span>], [<span class="number">2.</span>]])</div><div class="line"><span class="comment"># 执行矩阵乘法</span></div><div class="line">product = tf.matmul(matrix1, matrix2)</div><div class="line"><span class="comment"># 创建session这里使用了with as 的方法，如果没有的话记得sess.close()关闭session</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">	<span class="comment"># 执行图</span></div><div class="line">	result = sess.run(product)</div><div class="line">	<span class="comment"># 打印结果</span></div><div class="line">	print(result)</div></pre></td></tr></table></figure>
<p>tensorflow会在运行的机器上寻找GPU如果有的话会优先使用GPU，如果没有会找CPU使用。如果有多个GPU可用，tensorflow会默认使用第一个找到的GPU，当然你可以指定。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">	<span class="keyword">with</span> tf.device(<span class="string">"/gpu:1"</span>):</div><div class="line">	...</div><div class="line">	...</div></pre></td></tr></table></figure></p>
<ul>
<li><code>&quot;/cpu:0&quot;</code>:机器的CPU。</li>
<li><code>&quot;/gpu:0&quot;</code>:机器的第一个GPU。</li>
<li><code>&quot;/gpu:1&quot;</code>:机器的第二个GPU。</li>
</ul>
<p>对于变量还需要特别说明下，变量在使用前都需要进行初始化。当在图中使用一个创建一个变量的时候我们会给这个变量指定一个初始值或者初始化的方式，但是要记住的是这个时候只是一个管道结构而已实际上这个变量还并没有执行初始化操作，我们需要在使用变量前通过<code>tf.initialize_all_variables() sess.run(init_op)</code>操作将所有的变量进行一次执行初始化操作。</p>
<p>有时候也存在使用一个已经定义的变量来初始化另外一个变量的情况，这个时候需要使用<code>initialized_value()</code><br>举例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create a variable with a random value.</span></div><div class="line">weights = tf.Variable(tf.random_normal([<span class="number">784</span>, <span class="number">200</span>], stddev=<span class="number">0.35</span>),name=<span class="string">"weights"</span>)</div><div class="line"><span class="comment"># Create another variable with the same value as 'weights'.</span></div><div class="line">w2 = tf.Variable(weights.initialized_value(), name=<span class="string">"w2"</span>)</div><div class="line"><span class="comment"># Create another variable with twice the value of 'weights'</span></div><div class="line">w_twice = tf.Variable(weights.initialized_value() * <span class="number">0.2</span>, name=<span class="string">"w_twice"</span>)</div></pre></td></tr></table></figure></p>
<h2 id="搭建神经网络注意项"><a href="#搭建神经网络注意项" class="headerlink" title="搭建神经网络注意项"></a>搭建神经网络注意项</h2><p>具体搭建过程参考这篇<a href="http://www.tensorfly.cn/tfdoc/tutorials/mnist_pros.html" target="_blank" rel="external">文章</a>，<br>tf.Session() 和 tf.InteractiveSession()的区别：Session()的方式可以使用python中with…as的形式，这样就可以不用单独执行close()操作，但是在每次需要执行图计算的时候需要以sess.run()的形式进行，对于tf.InteractiveSession()来说，不能使用with…as的形式，所以在最后都需要进行显式的close()操作，但是它可以让每次要进行图计算的时候只进行Tensor.eval()即可，这样更灵活。</p>
<p>在tensorflow中使用tensor来表示所有数据，这点和pytorch不同，tensorflow不需要通过一个变量来单独定义tensor，因为在tensorflow看来这些数据都是tensor。</p>
<h3 id="占位符"><a href="#占位符" class="headerlink" title="占位符"></a>占位符</h3><p>tf.placeholder作用是为一个将被频繁使用的tensor插入一个占位符。这个占位符实际也是一个tensor只是不能直接使用。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">placeholder(dtype, shape=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</div></pre></td></tr></table></figure></p>
<p><strong>注意</strong>直接使用placeholder定义的tensor会返回错误，你需要在<code>Session.run()</code>，<code>Tensor.eval()</code>或者<code>Operation.run()</code>通过<code>feed_dict</code>参数传入符合placeholder定义的tensor才行。<br>举例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">x = tf.placeholder(tf.float32, shape=(<span class="number">1024</span>, <span class="number">1024</span>))</div><div class="line">y = tf.matmul(x, x)</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  print(sess.run(y))  <span class="comment"># ERROR: will fail because x was not fed.</span></div><div class="line"></div><div class="line">  rand_array = np.random.rand(<span class="number">1024</span>, <span class="number">1024</span>)</div><div class="line">  print(sess.run(y, feed_dict=&#123;x: rand_array&#125;))  <span class="comment"># Will succeed.</span></div></pre></td></tr></table></figure>
<p>参数：  </p>
<ul>
<li><code>dtype</code>：tensor的元素类型。      </li>
<li><code>shape</code>：tensor的尺寸，如果将某个大小填写为None表示这个位置由系统根据条件给出。类似reshape操作中如果是tf.reshape(a, [-1,9])中-1表示系统自行根据元素个数决定行数，只要保证列数位9即可。   </li>
<li><code>name</code>：操作的名称</li>
</ul>
<h3 id="部分函数"><a href="#部分函数" class="headerlink" title="部分函数"></a>部分函数</h3><p><code>tf.reduce_sum()</code>作为求和函数<br><code>tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)</code>表示一个使用梯度下降的优化方法。优化的步长是0.01，优化的对象是最小化cross_entropy函数，这个函数cross_entropy需要你自行定义。</p>
<p>举例：这里表现的是神经网络中进行训练的代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cross_entropy)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</div><div class="line">  <span class="comment"># 将输入数据按照batch来拆分</span></div><div class="line">  batch = mnist.train.next_batch(<span class="number">50</span>)</div><div class="line">  <span class="comment"># 将一个batch的输入输入到train操作中</span></div><div class="line">  train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>]&#125;)</div></pre></td></tr></table></figure></p>
<p><strong>注意</strong> feed_dict这个不仅可以用来作为占位符的具体输入，也可以用来替换其它tensor。</p>
<p><code>tf.argmax(input, axis=None, name=None, dimension=None)</code>可以用来获取最大值所在的位置，如果输入是一个向量，axis填写位0。axis为1返回每行的最大值位置索引。通过这个函数可以实现对模型的正确率的评估。</p>
<h3 id="权值初始化"><a href="#权值初始化" class="headerlink" title="权值初始化"></a>权值初始化</h3><p>因为是一个神经网络将权值初始化为0学习过程将无法开始，这里我们使用<code>tf.truncated_normal(shape, mean=0.0, stddev=1.0)</code>(这里只列出需要的两个参数)来进行初始化，因为希望初始化后的值小点，所以将stddev标准差设置为0.1也就是使用更加集中在0附近的点的进行初始化。在pytorch中似乎是不需要进行我们进行权值的初始化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></div><div class="line">  initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</div><div class="line">  <span class="keyword">return</span> tf.Variable(initial)</div></pre></td></tr></table></figure>
<h3 id="卷积层和池化层"><a href="#卷积层和池化层" class="headerlink" title="卷积层和池化层"></a>卷积层和池化层</h3><p>使用<code>tf.nn.conv2d()</code>来定义卷积层，主要用来获取特征。<br>使用<code>tf.nn.max_pool()</code>来定义池化层，池化层的作用主要用来进行下采样，进一步降低数据量。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">conv2d(input, filter, strides, padding)这里只介绍这几个参数。</div></pre></td></tr></table></figure></p>
<ul>
<li>input：Tensor。必须是：half，float，float64 类型。一个4D tensor。</li>
<li>filter：Tensor。类型和input一致。一个4D tensor，大小满足[filter_height, filter_width, in_channels, out_channels]</li>
<li>strides：ints的列表。长度为4的1D tensor。表示输入的每个维度的滑动距离。对应这里[image-number, filter_height, filter_width, input-channel]其中mage-number,input-channel设置为1的话表示一个图片一个通道的去处理。</li>
<li>padding：类型字符层，可选“SAME”“VALID”，比如6的长度，窗口是4每次滑动4。如果是“SAME”就会补差的值，如果是“VALID”就会抛弃差的值。</li>
</ul>
<h3 id="变量的保存和加载"><a href="#变量的保存和加载" class="headerlink" title="变量的保存和加载"></a>变量的保存和加载</h3><p><code>tf.train.Saver()</code>创建一个<code>Saver</code>来管理模型中的所有变量。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">saver = tf.train.Saver()</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  <span class="comment"># Save the variables to disk.</span></div><div class="line">  save_path = saver.save(sess, <span class="string">"/tmp/model.ckpt"</span>)</div></pre></td></tr></table></figure></p>
<p>使用同一个<code>Saver</code>对象来恢复变量。注意，当你从文件中恢复变量时，不需要事先对它们做初始化。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">saver = tf.train.Saver()</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  saver.restore(sess, <span class="string">"/tmp/model.ckpt"</span>)</div></pre></td></tr></table></figure></p>
<h2 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h2><h3 id="读取csv文件"><a href="#读取csv文件" class="headerlink" title="读取csv文件"></a>读取csv文件</h3><p><a href="http://www.tensorfly.cn/tfdoc/how_tos/reading_data.html" target="_blank" rel="external">参考文件</a>有些情况下我们会收到.csv的数据文件。现在得考虑如何将这些数据读入到模型中以便开始训练。<br>一般读取文件的步骤：</p>
<ul>
<li>生成文件名列表</li>
<li>根据文件名列表生成文件名队列</li>
<li>选择合适的阅读器</li>
<li>选择合适的解析器</li>
</ul>
<p>中间会将文件名队列放在另外的线程中。这个线程负责将文件逐次输入到文件名称队列中。这个过程和阅读器工作独立。<br><strong>生成文件列表</strong>：使用列表生成式，或者手动写入<code>[&quot;file0&quot;, &quot;file1&quot;]</code>。<br><strong>根据文件名列表生成文件名队列</strong>：<code>tf.train.string_input_producer()</code>如果设置<code>shuffle=True</code>将对文件名进行乱序处理。<br><strong>选择合适的阅读器</strong>：reader = tf.TextLineReader()，reader输入文件队列，输出key，value。key对应文件，value对应文件的内容。<br><strong>选择合适的阅读器</strong>：tf.decode_csv()解析csv文件，输入value<br><strong>启Coordinator</strong>：coord=tf.train.Coordinator()作用是在出现错误的时可以正确的关闭下面的线程<br><strong>启动队列runner</strong>：threads=tf.train.start_queue_runners(coord=coord)这样文件名队列才开始入队数据，后续才能进行出队操作。<br><strong>进行操作</strong>：session.run()<br><strong>关闭线程</strong>：申请停止线程coord.request_stop()，等待threads线程结束coord.join(threads)<br>举例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 创建文件队列</span></div><div class="line">filename_queue = tf.train.string_input_producer([<span class="string">"file0.csv"</span>, <span class="string">"file1.csv"</span>])</div><div class="line"><span class="comment"># 创建阅读器</span></div><div class="line">reader = tf.TextLineReader()</div><div class="line"><span class="comment"># 阅读器读取文件返回key 和 value</span></div><div class="line">key, value = reader.read(filename_queue)</div><div class="line"></div><div class="line"><span class="comment"># 设置读取文件每一行记录的时候如果有位置为空时的默认值</span></div><div class="line">record_defaults = [[<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">1</span>]]</div><div class="line"><span class="comment"># csv解析器解析value</span></div><div class="line">col1, col2, col3, col4, col5 = tf.decode_csv(value, record_defaults=record_defaults)</div><div class="line"><span class="comment"># 将col1, col2, col3, col4按照0行进行连接为新的特征</span></div><div class="line"><span class="comment"># 举例a=[[1]] b=[[2]]按照行连接后为[[1],[2]]</span></div><div class="line">features = tf.concat(<span class="number">0</span>, [col1, col2, col3, col4])</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  <span class="comment">#创建线程协调</span></div><div class="line">  coord = tf.train.Coordinator()</div><div class="line">  <span class="comment"># 开启线程</span></div><div class="line">  threads = tf.train.start_queue_runners(coord=coord)</div><div class="line"></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1200</span>):</div><div class="line">    <span class="comment"># 返回特征和标签</span></div><div class="line">    example, label = sess.run([features, col5])</div><div class="line"></div><div class="line">  <span class="comment"># 停止线程</span></div><div class="line">  coord.request_stop()</div><div class="line">  <span class="comment"># 等待线程停止</span></div><div class="line">  coord.join(threads)</div></pre></td></tr></table></figure>
<h3 id="通过TFRecord读取数据"><a href="#通过TFRecord读取数据" class="headerlink" title="通过TFRecord读取数据"></a>通过TFRecord读取数据</h3><p><a href="http://www.tensorfly.cn/tfdoc/how_tos/reading_data.html" target="_blank" rel="external">参考文件</a>还有种更节约空间的Tensorflow自己的文件格式TFRecord。我们要完成的事情包括将原始文件转换为TFRecord和从TFRecord种读取数据出来。下面是以图片csv文件转换为TFRecord来说明。下面的代码来自参考文件。<br><strong>转换为TFRecord文件</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_to</span><span class="params">(data_set, name)</span>:</span></div><div class="line">  <span class="string">"""Converts a dataset to tfrecords."""</span></div><div class="line">  images = data_set.images</div><div class="line">  labels = data_set.labels</div><div class="line">  num_examples = data_set.num_examples</div><div class="line"></div><div class="line">  <span class="keyword">if</span> images.shape[<span class="number">0</span>] != num_examples:<span class="keyword">raise</span> ValueError(<span class="string">'Images size %d does not match label size %d.'</span> %(images.shape[<span class="number">0</span>], num_examples))</div><div class="line">  rows = images.shape[<span class="number">1</span>]</div><div class="line">  cols = images.shape[<span class="number">2</span>]</div><div class="line">  depth = images.shape[<span class="number">3</span>]</div><div class="line"></div><div class="line">  filename = os.path.join(FLAGS.directory, name + <span class="string">'.tfrecords'</span>)</div><div class="line">  print(<span class="string">'Writing'</span>, filename)</div><div class="line">  writer = tf.python_io.TFRecordWriter(filename)</div><div class="line">  <span class="keyword">for</span> index <span class="keyword">in</span> range(num_examples):</div><div class="line">    <span class="comment"># 将图片转换为字符串</span></div><div class="line">    image_raw = images[index].tostring()</div><div class="line">    <span class="comment"># 将图片信息转换到example</span></div><div class="line">    example = tf.train.Example(features=tf.train.Features(feature=&#123;</div><div class="line">        <span class="string">'height'</span>: _int64_feature(rows),</div><div class="line">        <span class="string">'width'</span>: _int64_feature(cols),</div><div class="line">        <span class="string">'depth'</span>: _int64_feature(depth),</div><div class="line">        <span class="string">'label'</span>: _int64_feature(int(labels[index])),</div><div class="line">        <span class="string">'image_raw'</span>: _bytes_feature(image_raw)&#125;))</div><div class="line">    <span class="comment"># 写入文件</span></div><div class="line">    writer.write(example.SerializeToString())</div><div class="line">  writer.close()</div></pre></td></tr></table></figure></p>
<p><strong>从TFRecord文件读取</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_and_decode</span><span class="params">(filename_queue)</span>:</span></div><div class="line">  <span class="comment"># 创建阅读器</span></div><div class="line">  reader = tf.TFRecordReader()</div><div class="line">  _, serialized_example = reader.read(filename_queue)</div><div class="line">  <span class="comment"># 创建解析器</span></div><div class="line">  features = tf.parse_single_example(</div><div class="line">      serialized_example,</div><div class="line">      <span class="comment"># Defaults are not specified since both keys are required.</span></div><div class="line">      features=&#123;</div><div class="line">          <span class="string">'image_raw'</span>: tf.FixedLenFeature([], tf.string),</div><div class="line">          <span class="string">'label'</span>: tf.FixedLenFeature([], tf.int64),</div><div class="line">      &#125;)</div><div class="line"></div><div class="line">  <span class="comment"># Convert from a scalar string tensor (whose single string has</span></div><div class="line">  <span class="comment"># length mnist.IMAGE_PIXELS) to a uint8 tensor with shape</span></div><div class="line">  <span class="comment"># [mnist.IMAGE_PIXELS].</span></div><div class="line">  image = tf.decode_raw(features[<span class="string">'image_raw'</span>], tf.uint8)</div><div class="line">  image.set_shape([mnist.IMAGE_PIXELS])</div><div class="line"></div><div class="line">  <span class="comment"># OPTIONAL: Could reshape into a 28x28 image and apply distortions</span></div><div class="line">  <span class="comment"># here.  Since we are not applying any distortions in this</span></div><div class="line">  <span class="comment"># example, and the next step expects the image to be flattened</span></div><div class="line">  <span class="comment"># into a vector, we don't bother.</span></div><div class="line"></div><div class="line">  <span class="comment"># Convert from [0, 255] -&gt; [-0.5, 0.5] floats.</span></div><div class="line">  image = tf.cast(image, tf.float32) * (<span class="number">1.</span> / <span class="number">255</span>) - <span class="number">0.5</span></div><div class="line"></div><div class="line">  <span class="comment"># Convert label from a scalar uint8 tensor to an int32 scalar.</span></div><div class="line">  label = tf.cast(features[<span class="string">'label'</span>], tf.int32)</div><div class="line"></div><div class="line">  <span class="keyword">return</span> image, label</div></pre></td></tr></table></figure></p>
<p>参考文档:<a href="http://www.tensorfly.cn/tfdoc/get_started/basic_usage.html" target="_blank" rel="external">Tensorflow官方文档中文版</a></p>
]]></content>
      
        <categories>
            
            <category> frame </category>
            
        </categories>
        
        
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Translation with a Sequence to Sequence Network and Attention(翻译)]]></title>
      <url>/2017/05/26/TransRnn/</url>
      <content type="html"><![CDATA[<p>原文链接：<a href="http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html" target="_blank" rel="external">Translation with a Sequence to Sequence Network and Attention</a><br>作者：<a href="https://github.com/spro/practical-pytorch" target="_blank" rel="external">Sean Robertson</a></p>
<p>在这个项目中我们将训练一个神经网络将法语翻译为英语。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">[KEY: &gt; input, = target, &lt; output]</div><div class="line"></div><div class="line">&gt; il est en train de peindre un tableau .</div><div class="line">= he is painting a picture .</div><div class="line">&lt; he is painting a picture .</div><div class="line"></div><div class="line">&gt; pourquoi ne pas essayer ce vin delicieux ?</div><div class="line">= why not try that delicious wine ?</div><div class="line">&lt; why not try that delicious wine ?</div><div class="line"></div><div class="line">&gt; elle n est pas poete mais romanciere .</div><div class="line">= she is not a poet but a novelist .</div><div class="line">&lt; she not not a poet but a novelist .</div><div class="line"></div><div class="line">&gt; vous etes trop maigre .</div><div class="line">= you re too skinny .</div><div class="line">&lt; you re all alone .</div></pre></td></tr></table></figure></p>
<p>…不同程度的成功。<br><a id="more"></a><br>一个简单但是有力的<a href="http://arxiv.org/abs/1409.3215" target="_blank" rel="external">序列到序列网络</a>的想法让翻译变的可能，它由两个循环神经网络组成一起将一个序列翻译为另外一个序列。一个编码网络将输入序列凝聚为向量，一个解码网络将向量张开为新的序列。</p>
<p><img src="/image/seq2seq/seq2seq.png"></p>
<p>为了改善上面的模型我们将使用<a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="external">注意力机制</a>，它将使得解码器学会将注意力集中在输入学历的特定部分。</p>
<h2 id="Recommended-Reading-推荐阅读"><a href="#Recommended-Reading-推荐阅读" class="headerlink" title="Recommended Reading(推荐阅读)"></a>Recommended Reading(推荐阅读)</h2><p>我假设你目前至少安装来PyTorch，了解Python，并且理解Tensor：  </p>
<ul>
<li><a href="http://pytorch.org" target="_blank" rel="external">http://pytorch.org</a>从这里获取安装指导</li>
<li><a href="http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="external">Deep Learning with PyTorch: A 60 Minute Blitz</a>一般从这里开始PyToch</li>
<li><a href="http://pytorch.org/tutorials/beginner/pytorch_with_examples.html" target="_blank" rel="external">Learning PyTorch with Examples</a>获取更有广度和深度的概述</li>
<li><a href="http://pytorch.org/tutorials/beginner/former_torchies_tutorial.html" target="_blank" rel="external">PyTorch for former Torch users</a>如果你是Lua Torch用户</li>
</ul>
<p>了解序列到序列神经网络和它们是如何工作对后面的学习将非常有用。</p>
<ul>
<li><a href="http://arxiv.org/abs/1406.1078" target="_blank" rel="external">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a></li>
<li><a href="http://arxiv.org/abs/1409.3215" target="_blank" rel="external">Sequence to Sequence Learning with Neural Networks</a></li>
<li><a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="external">Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
<li><a href="http://arxiv.org/abs/1506.05869" target="_blank" rel="external">A Neural Conversational Model</a></li>
</ul>
<p>你将会发现前面的指导<a href="http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html" target="_blank" rel="external">Classifying Names with a Character-Level RNN</a>和<a href="http://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html" target="_blank" rel="external">Generating Names with a Character-Level RNN</a>很有用因为这两篇文章的一些概念和本文编码解码模型中的概念非常相似。</p>
<p>关于这个主题更多的信息，可以阅读下面的这些论文：</p>
<ul>
<li><a href="http://arxiv.org/abs/1406.1078" target="_blank" rel="external">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a></li>
<li><a href="http://arxiv.org/abs/1409.3215" target="_blank" rel="external">Sequence to Sequence Learning with Neural Networks</a></li>
<li><a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="external">Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
<li><a href="http://arxiv.org/abs/1506.05869" target="_blank" rel="external">A Neural Conversational Model</a></li>
</ul>
<p><strong>Requirements(依赖)</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</div><div class="line"><span class="keyword">from</span> io <span class="keyword">import</span> open</div><div class="line"><span class="keyword">import</span> unicodedata</div><div class="line"><span class="keyword">import</span> string</div><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">import</span> random</div><div class="line"></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"></div><div class="line">use_cuda = torch.cuda.is_available()</div></pre></td></tr></table></figure></p>
<h2 id="Loading-data-files-加载文件"><a href="#Loading-data-files-加载文件" class="headerlink" title="Loading data files(加载文件)"></a>Loading data files(加载文件)</h2><p>这个项目的数据是数以千计的英语到法语翻译组合的数据集。</p>
<p>在问答平台<a href="http://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages" target="_blank" rel="external">Open Data Stack Exchange</a>得到了一个开源翻译网站<a href="http://tatoeba.org/" target="_blank" rel="external">http://tatoeba.org/</a>可以下载获取数据在这个路径<a href="http://tatoeba.org/eng/downloads" target="_blank" rel="external">http://tatoeba.org/eng/downloads</a>-更好的是，一些人做了些额外的工作将切分语言组合到独立的文件：<br><a href="http://www.manythings.org/anki/" target="_blank" rel="external">http://www.manythings.org/anki/</a></p>
<p>英语到法语对因为太大不便放到库中，因此在继续开始前下载<code>data/eng-fra.txt</code>文件。这个文件使用tab键来分割翻译对：</p>
<blockquote>
<p>I am cold.    Je suis froid.</p>
</blockquote>
<p><strong>Note</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">从[这里](https://download.pytorch.org/tutorial/data.zip)下载文件并解压到当前路径。</div></pre></td></tr></table></figure></p>
<p>同在字符级RNN指导中字符编码相似，我们表示语言中的词汇使用的是one-hot向量，或者是特大的只有一个位置为1(表示词汇出现的位置)其余为0的向量。同语言中可能出现的字符相比，这里会存在更多的词汇，因此编码的向量将非常庞大。然而我们将做些简化处理裁剪一些词汇每个语言只使用几千个词语。</p>
<p><img src="/image/seq2seq/word-encoding.png"></p>
<p>我们需要每个词汇有个单独的索引用来作为神经网络的输入和目标。为了保持对这些索引的跟踪我们会使用一个很有用的类<code>lang</code>包含了词-&gt;索引(<code>word2index</code>)和索引-&gt;词(index2word)字典，同时还有对词计数的<code>word2count</code>后续用来对数量很少的词进行替换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">SOS_token = <span class="number">0</span></div><div class="line">EOS_token = <span class="number">1</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Lang</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></div><div class="line">        self.name = name</div><div class="line">        self.word2index = &#123;&#125;</div><div class="line">        self.word2count = &#123;&#125;</div><div class="line">        self.index2word = &#123;<span class="number">0</span>: <span class="string">"SOS"</span>, <span class="number">1</span>: <span class="string">"EOS"</span>&#125;</div><div class="line">        self.n_words = <span class="number">2</span>  <span class="comment"># Count SOS and EOS</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addSentence</span><span class="params">(self, sentence)</span>:</span></div><div class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sentence.split(<span class="string">' '</span>):</div><div class="line">            self.addWord(word)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addWord</span><span class="params">(self, word)</span>:</span></div><div class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> self.word2index:</div><div class="line">            self.word2index[word] = self.n_words</div><div class="line">            self.word2count[word] = <span class="number">1</span></div><div class="line">            self.index2word[self.n_words] = word</div><div class="line">            self.n_words += <span class="number">1</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self.word2count[word] += <span class="number">1</span></div></pre></td></tr></table></figure>
<p>这些文件都是Unicode编码，为了简单我们将Unicode字符转换为ASCII，将字符统一为小写，剔除标点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Turn a Unicode string to plain ASCII, thanks to</span></div><div class="line"><span class="comment"># http://stackoverflow.com/a/518232/2809427</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">unicodeToAscii</span><span class="params">(s)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(</div><div class="line">        c <span class="keyword">for</span> c <span class="keyword">in</span> unicodedata.normalize(<span class="string">'NFD'</span>, s)</div><div class="line">        <span class="keyword">if</span> unicodedata.category(c) != <span class="string">'Mn'</span></div><div class="line">    )</div><div class="line"></div><div class="line"><span class="comment"># Lowercase, trim, and remove non-letter characters</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalizeString</span><span class="params">(s)</span>:</span></div><div class="line">    s = unicodeToAscii(s.lower().strip())</div><div class="line">    s = re.sub(<span class="string">r"([.!?])"</span>, <span class="string">r" \1"</span>, s)</div><div class="line">    s = re.sub(<span class="string">r"[^a-zA-Z.!?]+"</span>, <span class="string">r" "</span>, s)</div><div class="line">    <span class="keyword">return</span> s</div></pre></td></tr></table></figure>
<p>为了读取数据文件我们将文件拆解为行，将行信息拆解为对。这些文件都是英语-&gt;其它语言，因此为了我们能从其它语言-&gt;英语我添加了一个<code>reverse</code>标志来转换翻译语言之间的方向。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">readLangs</span><span class="params">(lang1, lang2, reverse=False)</span>:</span></div><div class="line">    print(<span class="string">"Reading lines..."</span>)</div><div class="line"></div><div class="line">    <span class="comment"># Read the file and split into lines</span></div><div class="line">    lines = open(<span class="string">'data/%s-%s.txt'</span> % (lang1, lang2), encoding=<span class="string">'utf-8'</span>).\</div><div class="line">        read().strip().split(<span class="string">'\n'</span>)</div><div class="line"></div><div class="line">    <span class="comment"># Split every line into pairs and normalize</span></div><div class="line">    pairs = [[normalizeString(s) <span class="keyword">for</span> s <span class="keyword">in</span> l.split(<span class="string">'\t'</span>)] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</div><div class="line"></div><div class="line">    <span class="comment"># Reverse pairs, make Lang instances</span></div><div class="line">    <span class="keyword">if</span> reverse:</div><div class="line">        pairs = [list(reversed(p)) <span class="keyword">for</span> p <span class="keyword">in</span> pairs]</div><div class="line">        input_lang = Lang(lang2)</div><div class="line">        output_lang = Lang(lang1)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        input_lang = Lang(lang1)</div><div class="line">        output_lang = Lang(lang2)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> input_lang, output_lang, pairs</div></pre></td></tr></table></figure>
<p>在数据文件中有非常多的各种长度的句子然而我们希望训练可以更简单更快的进行，所以我们将对数据进行裁剪只保留简短和简单的句子。这里限制最大句子长度是10个单词(包括了结束符号)然后我们将过滤出那些将翻译为”I am”或者“He is”等样式的句子(撇号在先前已被替换)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">MAX_LENGTH = <span class="number">10</span></div><div class="line"></div><div class="line">eng_prefixes = (</div><div class="line">    <span class="string">"i am "</span>, <span class="string">"i m "</span>,</div><div class="line">    <span class="string">"he is"</span>, <span class="string">"he s "</span>,</div><div class="line">    <span class="string">"she is"</span>, <span class="string">"she s"</span>,</div><div class="line">    <span class="string">"you are"</span>, <span class="string">"you re "</span>,</div><div class="line">    <span class="string">"we are"</span>, <span class="string">"we re "</span>,</div><div class="line">    <span class="string">"they are"</span>, <span class="string">"they re "</span></div><div class="line">)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">filterPair</span><span class="params">(p)</span>:</span></div><div class="line">    <span class="keyword">return</span> len(p[<span class="number">0</span>].split(<span class="string">' '</span>)) &lt; MAX_LENGTH <span class="keyword">and</span> \</div><div class="line">        len(p[<span class="number">1</span>].split(<span class="string">' '</span>)) &lt; MAX_LENGTH <span class="keyword">and</span> \</div><div class="line">        p[<span class="number">1</span>].startswith(eng_prefixes)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">filterPairs</span><span class="params">(pairs)</span>:</span></div><div class="line">    <span class="keyword">return</span> [pair <span class="keyword">for</span> pair <span class="keyword">in</span> pairs <span class="keyword">if</span> filterPair(pair)]</div></pre></td></tr></table></figure>
<p>整个准备数据的过程如下：</p>
<ul>
<li>读取文件将文件信息按行拆分，再将行信息拆分层句子对</li>
<li>格式化文档，通过句子长度和内容过滤文档</li>
<li>从句子对中制作词汇表</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepareData</span><span class="params">(lang1, lang2, reverse=False)</span>:</span></div><div class="line">    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)</div><div class="line">    print(<span class="string">"Read %s sentence pairs"</span> % len(pairs))</div><div class="line">    pairs = filterPairs(pairs)</div><div class="line">    print(<span class="string">"Trimmed to %s sentence pairs"</span> % len(pairs))</div><div class="line">    print(<span class="string">"Counting words..."</span>)</div><div class="line">    <span class="keyword">for</span> pair <span class="keyword">in</span> pairs:</div><div class="line">        input_lang.addSentence(pair[<span class="number">0</span>])</div><div class="line">        output_lang.addSentence(pair[<span class="number">1</span>])</div><div class="line">    print(<span class="string">"Counted words:"</span>)</div><div class="line">    print(input_lang.name, input_lang.n_words)</div><div class="line">    print(output_lang.name, output_lang.n_words)</div><div class="line">    <span class="keyword">return</span> input_lang, output_lang, pairs</div><div class="line"></div><div class="line"></div><div class="line">input_lang, output_lang, pairs = prepareData(<span class="string">'eng'</span>, <span class="string">'fra'</span>, <span class="keyword">True</span>)</div><div class="line">print(random.choice(pairs))</div></pre></td></tr></table></figure>
<p><strong>Out</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">Reading lines...</div><div class="line">Read 135842 sentence pairs</div><div class="line">Trimmed to 10853 sentence pairs</div><div class="line">Counting words...</div><div class="line">Counted words:</div><div class="line">fra 4489</div><div class="line">eng 2925</div><div class="line">[&apos;nous sommes faits l un pour l autre .&apos;, &apos;we re meant for each other .&apos;]</div></pre></td></tr></table></figure></p>
<h2 id="The-Seq2Seq-Model-序列到序列模型"><a href="#The-Seq2Seq-Model-序列到序列模型" class="headerlink" title="The Seq2Seq Model(序列到序列模型)"></a>The Seq2Seq Model(序列到序列模型)</h2><p>循环神经网络，或者叫RNN，是一种操作序列和使用自身的输出作为后续步骤输入的神经网络。</p>
<p>一个<a href="http://arxiv.org/abs/1409.3215" target="_blank" rel="external">序列到序列网络</a>，或者seq2seq神经网络，或者<a href="https://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="external">编码解码神经网络</a>，是一个包含了两个RNN分别作为编码器和解码器的模型。编码器读取输入序列输出单独的向量，然后解码器读取此向量产生输出序列。</p>
<p><img src="/image/seq2seq/seq2seq.png"></p>
<p>不同于单个用来进行序列预测的RNN，每个输入对应了一个输出，seq2seq模型将我们从序列长度中释放出来，这对于两种语言之间的翻译非常理想。</p>
<p>考虑这样的句子”Je ne suis pas le chat noir”-&gt;”I am not the black cat”。大部分输入序列的词可以直接翻译到输出序列，但有些细微的区别，e.g.“chat(猫) noir(黑)”和“black (黑)cat(猫)”。因为“ne/pas”结构在输入序列中也会多一个词出来。这个会给根据输入序列来直接产生一个正确的翻译结果造成困难。</p>
<p>使用seq2seq模型编码器创建了一个单独的向量，在理想情况下，编码输入序列的“含义”到一个单独的向量-一个在N为句子空间中的一个点。</p>
<h3 id="The-Encoder-编码器"><a href="#The-Encoder-编码器" class="headerlink" title="The Encoder(编码器)"></a>The Encoder(编码器)</h3><p>seq2seq网络的编码器是一个RNN它对输入句子的每个单词都输出一些值。每个输入编码器的单词输出一个向量和一个隐藏状态，然后下一个输入的单词将使用这个隐藏状态。</p>
<p><img src="/image/seq2seq/encoder-network.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderRNN</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, n_layers=<span class="number">1</span>)</span>:</span></div><div class="line">        super(EncoderRNN, self).__init__()</div><div class="line">        self.n_layers = n_layers</div><div class="line">        self.hidden_size = hidden_size</div><div class="line"></div><div class="line">        self.embedding = nn.Embedding(input_size, hidden_size)</div><div class="line">        self.gru = nn.GRU(hidden_size, hidden_size)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden)</span>:</span></div><div class="line">        embedded = self.embedding(input).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>)</div><div class="line">        output = embedded</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_layers):</div><div class="line">            output, hidden = self.gru(output, hidden)</div><div class="line">        <span class="keyword">return</span> output, hidden</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></div><div class="line">        result = Variable(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size))</div><div class="line">        <span class="keyword">if</span> use_cuda:</div><div class="line">            <span class="keyword">return</span> result.cuda()</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> result</div></pre></td></tr></table></figure>
<h3 id="The-Decoder-解码器"><a href="#The-Decoder-解码器" class="headerlink" title="The Decoder(解码器)"></a>The Decoder(解码器)</h3><p>解码器是另外一个RNN它接收编码器输出的向量然后输出单词的序列来进行翻译。</p>
<h4 id="Simple-Decoder-简单解码器"><a href="#Simple-Decoder-简单解码器" class="headerlink" title="Simple Decoder(简单解码器)"></a>Simple Decoder(简单解码器)</h4><p>在最简单的seq2seq解码器中我们仅仅使用编码器的最后一层输出。最后一层输出有时也叫做<em>上下文向量</em>它是整个输入序列的编码。这个上下文向量用来初始化解码器隐藏状态。</p>
<p>在每一个解码步骤，解码器得到的是输入字符和隐藏状态。初始化的输入字符是字符串开始标志<code>&lt;SOS&gt;</code>，第一个隐藏状态是上下文向量(编码器的最后一个隐藏状态)。</p>
<p><img src="/image/seq2seq/decoder-network.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderRNN</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_size, output_size, n_layers=<span class="number">1</span>)</span>:</span></div><div class="line">        super(DecoderRNN, self).__init__()</div><div class="line">        self.n_layers = n_layers</div><div class="line">        self.hidden_size = hidden_size</div><div class="line"></div><div class="line">        self.embedding = nn.Embedding(output_size, hidden_size)</div><div class="line">        self.gru = nn.GRU(hidden_size, hidden_size)</div><div class="line">        self.out = nn.Linear(hidden_size, output_size)</div><div class="line">        self.softmax = nn.LogSoftmax()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden)</span>:</span></div><div class="line">        output = self.embedding(input).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>)</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_layers):</div><div class="line">            output = F.relu(output)</div><div class="line">            output, hidden = self.gru(output, hidden)</div><div class="line">        output = self.softmax(self.out(output[<span class="number">0</span>]))</div><div class="line">        <span class="keyword">return</span> output, hidden</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></div><div class="line">        result = Variable(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size))</div><div class="line">        <span class="keyword">if</span> use_cuda:</div><div class="line">            <span class="keyword">return</span> result.cuda()</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> result</div></pre></td></tr></table></figure>
<p>我鼓励你训练和观察这个模型的结果，但是为了节约空间我们将直接介绍重要的注意力机制。</p>
<h4 id="Attention-Decoder-注意力解码器"><a href="#Attention-Decoder-注意力解码器" class="headerlink" title="Attention Decoder(注意力解码器)"></a>Attention Decoder(注意力解码器)</h4><p>如果只有上下文向量联通解码和编码器，那么这单个向量承担了编码的整改序列的重担。</p>
<p>注意力模型允许解码器网络在每次输出的时候“集中”在编码输出的不同部分。首先我们计算<em>注意力权值</em>集合。它们将和编码器输出向量相乘去创建一个权值组合。结果(在代码中称作<code>attn_applied</code>)应该包括输入序列具体部分的信息，这样帮助解码器选择正确单词输出。</p>
<p><img src="/image/seq2seq/1152PYf.png"></p>
<p>通过一个前馈层<code>attn</code>计算注意力权值，使用到解码器输入和隐藏层状态作为输入。因为这里的训练集中的句子长度各异，为了实际创建和训练这个层我们不得不选择一个最大的句子长度(输入长度，编码输出)来应用。最大长度的句子将使用所有的权值，短些的句子只使用前面一些。</p>
<p><img src="/image/seq2seq/attention-decoder-network.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttnDecoderRNN</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_size, output_size, n_layers=<span class="number">1</span>, dropout_p=<span class="number">0.1</span>, max_length=MAX_LENGTH)</span>:</span></div><div class="line">        super(AttnDecoderRNN, self).__init__()</div><div class="line">        self.hidden_size = hidden_size</div><div class="line">        self.output_size = output_size</div><div class="line">        self.n_layers = n_layers</div><div class="line">        self.dropout_p = dropout_p</div><div class="line">        self.max_length = max_length</div><div class="line"></div><div class="line">        self.embedding = nn.Embedding(self.output_size, self.hidden_size)</div><div class="line">        self.attn = nn.Linear(self.hidden_size * <span class="number">2</span>, self.max_length)</div><div class="line">        self.attn_combine = nn.Linear(self.hidden_size * <span class="number">2</span>, self.hidden_size)</div><div class="line">        self.dropout = nn.Dropout(self.dropout_p)</div><div class="line">        self.gru = nn.GRU(self.hidden_size, self.hidden_size)</div><div class="line">        self.out = nn.Linear(self.hidden_size, self.output_size)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden, encoder_output, encoder_outputs)</span>:</span></div><div class="line">        embedded = self.embedding(input).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>)</div><div class="line">        embedded = self.dropout(embedded)</div><div class="line"></div><div class="line">        attn_weights = F.softmax(</div><div class="line">            self.attn(torch.cat((embedded[<span class="number">0</span>], hidden[<span class="number">0</span>]), <span class="number">1</span>)))</div><div class="line">        attn_applied = torch.bmm(attn_weights.unsqueeze(<span class="number">0</span>),</div><div class="line">                                 encoder_outputs.unsqueeze(<span class="number">0</span>))</div><div class="line"></div><div class="line">        output = torch.cat((embedded[<span class="number">0</span>], attn_applied[<span class="number">0</span>]), <span class="number">1</span>)</div><div class="line">        output = self.attn_combine(output).unsqueeze(<span class="number">0</span>)</div><div class="line"></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_layers):</div><div class="line">            output = F.relu(output)</div><div class="line">            output, hidden = self.gru(output, hidden)</div><div class="line"></div><div class="line">        output = F.log_softmax(self.out(output[<span class="number">0</span>]))</div><div class="line">        <span class="keyword">return</span> output, hidden, attn_weights</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></div><div class="line">        result = Variable(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size))</div><div class="line">        <span class="keyword">if</span> use_cuda:</div><div class="line">            <span class="keyword">return</span> result.cuda()</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> result</div></pre></td></tr></table></figure>
<p><strong>Note</strong></p>
<blockquote>
<p>这里还有其它形式的注意力机制可以通过相对位置实现以此避免长度的限制。遇到关于“local attention”在<a href="https://arxiv.org/abs/1508.04025" target="_blank" rel="external">Effective Approaches to Attention-based Neural Machine Translation</a>。</p>
</blockquote>
<h2 id="Training-训练"><a href="#Training-训练" class="headerlink" title="Training(训练)"></a>Training(训练)</h2><h3 id="Preparing-Training-Data-训练数据准备"><a href="#Preparing-Training-Data-训练数据准备" class="headerlink" title="Preparing Training Data(训练数据准备)"></a>Preparing Training Data(训练数据准备)</h3><p>为了训练，每个语言对我们需要输入tensor(输入序列词的索引)和目标tensor(目标序列词的索引)。当创建这些向量我们会添加EOS字符在每个序列最后。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">indexesFromSentence</span><span class="params">(lang, sentence)</span>:</span></div><div class="line">    <span class="keyword">return</span> [lang.word2index[word] <span class="keyword">for</span> word <span class="keyword">in</span> sentence.split(<span class="string">' '</span>)]</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">variableFromSentence</span><span class="params">(lang, sentence)</span>:</span></div><div class="line">    indexes = indexesFromSentence(lang, sentence)</div><div class="line">    indexes.append(EOS_token)</div><div class="line">    result = Variable(torch.LongTensor(indexes).view(<span class="number">-1</span>, <span class="number">1</span>))</div><div class="line">    <span class="keyword">if</span> use_cuda:</div><div class="line">        <span class="keyword">return</span> result.cuda()</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> result</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">variablesFromPair</span><span class="params">(pair)</span>:</span></div><div class="line">    input_variable = variableFromSentence(input_lang, pair[<span class="number">0</span>])</div><div class="line">    target_variable = variableFromSentence(output_lang, pair[<span class="number">1</span>])</div><div class="line">    <span class="keyword">return</span> (input_variable, target_variable)</div></pre></td></tr></table></figure>
<h3 id="Training-the-Model-训练模型"><a href="#Training-the-Model-训练模型" class="headerlink" title="Training the Model(训练模型)"></a>Training the Model(训练模型)</h3><p>为了训练我们往编码器中输入数据，然后跟踪每个输入和最新的隐藏状态。然后解码器提供<code>&lt;SOS&gt;</code>标志作为第一个输入，然后解码器的上一个隐藏状态作为这次的隐藏状态。</p>
<p>“Teacher forcing”是一个使用真实目标输出作为每次下一个输入的概念，而不是使用解码器预测的输出作为下一个输入。使用teacher forcing因为它可以更快收敛，但是<a href="http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf" target="_blank" rel="external">当训练网络利用它时，它的表现并不稳定</a>。</p>
<p>你可以观察teacher-forced神经网络的输出序列它们读起来更有连续清晰的语法而不是和正确翻译相差很远混乱的翻译-按照直觉来，它学会了输出语法然后在老师告诉它一些单词后能“挑选”有含义的词，但是并不能正确学习从头创造一个句子。</p>
<p>因为PyTorch的自动梯度计算提供给了我们自由的选择，我们可以随机的选择使用teacher foring或者不使用它。打开<code>teacher_forcing_ratio</code>来更多的使用它。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div></pre></td><td class="code"><pre><div class="line">teacher_forcing_ratio = <span class="number">0.5</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH)</span>:</span></div><div class="line">    encoder_hidden = encoder.initHidden()</div><div class="line"></div><div class="line">    encoder_optimizer.zero_grad()</div><div class="line">    decoder_optimizer.zero_grad()</div><div class="line"></div><div class="line">    input_length = input_variable.size()[<span class="number">0</span>]</div><div class="line">    target_length = target_variable.size()[<span class="number">0</span>]</div><div class="line"></div><div class="line">    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))</div><div class="line">    encoder_outputs = encoder_outputs.cuda() <span class="keyword">if</span> use_cuda <span class="keyword">else</span> encoder_outputs</div><div class="line"></div><div class="line">    loss = <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> ei <span class="keyword">in</span> range(input_length):</div><div class="line">        encoder_output, encoder_hidden = encoder(</div><div class="line">            input_variable[ei], encoder_hidden)</div><div class="line">        encoder_outputs[ei] = encoder_output[<span class="number">0</span>][<span class="number">0</span>]</div><div class="line"></div><div class="line">    decoder_input = Variable(torch.LongTensor([[SOS_token]]))</div><div class="line">    decoder_input = decoder_input.cuda() <span class="keyword">if</span> use_cuda <span class="keyword">else</span> decoder_input</div><div class="line"></div><div class="line">    decoder_hidden = encoder_hidden</div><div class="line"></div><div class="line">    use_teacher_forcing = <span class="keyword">True</span> <span class="keyword">if</span> random.random() &lt; teacher_forcing_ratio <span class="keyword">else</span> <span class="keyword">False</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> use_teacher_forcing:</div><div class="line">        <span class="comment"># Teacher forcing: Feed the target as the next input</span></div><div class="line">        <span class="keyword">for</span> di <span class="keyword">in</span> range(target_length):</div><div class="line">            decoder_output, decoder_hidden, decoder_attention = decoder(</div><div class="line">                decoder_input, decoder_hidden, encoder_output, encoder_outputs)</div><div class="line">            loss += criterion(decoder_output[<span class="number">0</span>], target_variable[di])</div><div class="line">            decoder_input = target_variable[di]  <span class="comment"># Teacher forcing</span></div><div class="line"></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># Without teacher forcing: use its own predictions as the next input</span></div><div class="line">        <span class="keyword">for</span> di <span class="keyword">in</span> range(target_length):</div><div class="line">            decoder_output, decoder_hidden, decoder_attention = decoder(</div><div class="line">                decoder_input, decoder_hidden, encoder_output, encoder_outputs)</div><div class="line">            topv, topi = decoder_output.data.topk(<span class="number">1</span>)</div><div class="line">            ni = topi[<span class="number">0</span>][<span class="number">0</span>]</div><div class="line"></div><div class="line">            decoder_input = Variable(torch.LongTensor([[ni]]))</div><div class="line">            decoder_input = decoder_input.cuda() <span class="keyword">if</span> use_cuda <span class="keyword">else</span> decoder_input</div><div class="line"></div><div class="line">            loss += criterion(decoder_output[<span class="number">0</span>], target_variable[di])</div><div class="line">            <span class="keyword">if</span> ni == EOS_token:</div><div class="line">                <span class="keyword">break</span></div><div class="line"></div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    encoder_optimizer.step()</div><div class="line">    decoder_optimizer.step()</div><div class="line"></div><div class="line">    <span class="keyword">return</span> loss.data[<span class="number">0</span>] / target_length</div></pre></td></tr></table></figure>
<p>这是一个用来帮助打印时间的函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">import</span> math</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">asMinutes</span><span class="params">(s)</span>:</span></div><div class="line">    m = math.floor(s / <span class="number">60</span>)</div><div class="line">    s -= m * <span class="number">60</span></div><div class="line">    <span class="keyword">return</span> <span class="string">'%dm %ds'</span> % (m, s)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeSince</span><span class="params">(since, percent)</span>:</span></div><div class="line">    now = time.time()</div><div class="line">    s = now - since</div><div class="line">    es = s / (percent)</div><div class="line">    rs = es - s</div><div class="line">    <span class="keyword">return</span> <span class="string">'%s (- %s)'</span> % (asMinutes(s), asMinutes(rs))</div></pre></td></tr></table></figure>
<p>整个训练过程如下：</p>
<ul>
<li>打开计时器</li>
<li>初始化优化器和代价函数</li>
<li>创建训练集</li>
<li>创建一个空代价数组用来绘图</li>
</ul>
<p>然后我们调用<code>train</code>多次然后偶尔打印进程(% 循环次数，到目前为止的时间，估计时间)和平均损失。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainEpochs</span><span class="params">(encoder, decoder, n_epochs, print_every=<span class="number">1000</span>, plot_every=<span class="number">100</span>, learning_rate=<span class="number">0.01</span>)</span>:</span></div><div class="line">    start = time.time()</div><div class="line">    plot_losses = []</div><div class="line">    print_loss_total = <span class="number">0</span>  <span class="comment"># Reset every print_every</span></div><div class="line">    plot_loss_total = <span class="number">0</span>  <span class="comment"># Reset every plot_every</span></div><div class="line"></div><div class="line">    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)</div><div class="line">    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)</div><div class="line">    training_pairs = [variablesFromPair(random.choice(pairs))</div><div class="line">                      <span class="keyword">for</span> i <span class="keyword">in</span> range(n_epochs)]</div><div class="line">    criterion = nn.NLLLoss()</div><div class="line"></div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, n_epochs + <span class="number">1</span>):</div><div class="line">        training_pair = training_pairs[epoch - <span class="number">1</span>]</div><div class="line">        input_variable = training_pair[<span class="number">0</span>]</div><div class="line">        target_variable = training_pair[<span class="number">1</span>]</div><div class="line"></div><div class="line">        loss = train(input_variable, target_variable, encoder,</div><div class="line">                     decoder, encoder_optimizer, decoder_optimizer, criterion)</div><div class="line">        print_loss_total += loss</div><div class="line">        plot_loss_total += loss</div><div class="line"></div><div class="line">        <span class="keyword">if</span> epoch % print_every == <span class="number">0</span>:</div><div class="line">            print_loss_avg = print_loss_total / print_every</div><div class="line">            print_loss_total = <span class="number">0</span></div><div class="line">            print(<span class="string">'%s (%d %d%%) %.4f'</span> % (timeSince(start, epoch / n_epochs),</div><div class="line">                                         epoch, epoch / n_epochs * <span class="number">100</span>, print_loss_avg))</div><div class="line"></div><div class="line">        <span class="keyword">if</span> epoch % plot_every == <span class="number">0</span>:</div><div class="line">            plot_loss_avg = plot_loss_total / plot_every</div><div class="line">            plot_losses.append(plot_loss_avg)</div><div class="line">            plot_loss_total = <span class="number">0</span></div><div class="line"></div><div class="line">    showPlot(plot_losses)</div></pre></td></tr></table></figure></p>
<h3 id="Plotting-results-绘制结果"><a href="#Plotting-results-绘制结果" class="headerlink" title="Plotting results(绘制结果)"></a>Plotting results(绘制结果)</h3><p>使用matplotlib来绘制，在训练的时候使用<code>plot_losses</code>损失数组保存损失。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> matplotlib.ticker <span class="keyword">as</span> ticker</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">showPlot</span><span class="params">(points)</span>:</span></div><div class="line">    plt.figure()</div><div class="line">    fig, ax = plt.subplots()</div><div class="line">    <span class="comment"># this locator puts ticks at regular intervals</span></div><div class="line">    loc = ticker.MultipleLocator(base=<span class="number">0.2</span>)</div><div class="line">    ax.yaxis.set_major_locator(loc)</div><div class="line">    plt.plot(points)</div></pre></td></tr></table></figure></p>
<h2 id="Evaluation-评价"><a href="#Evaluation-评价" class="headerlink" title="Evaluation(评价)"></a>Evaluation(评价)</h2><p>评价和训练大部分是一样的，但评价没有目标因此仅仅提供解码器每步预测的词汇给它自己作为下次输入。每次预测的单词我们将它添加到输出字符串，然后当预测到EOS字符的时候停止。我们也存储解码器的注意力输出作为后续的展示使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(encoder, decoder, sentence, max_length=MAX_LENGTH)</span>:</span></div><div class="line">    input_variable = variableFromSentence(input_lang, sentence)</div><div class="line">    input_length = input_variable.size()[<span class="number">0</span>]</div><div class="line">    encoder_hidden = encoder.initHidden()</div><div class="line"></div><div class="line">    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))</div><div class="line">    encoder_outputs = encoder_outputs.cuda() <span class="keyword">if</span> use_cuda <span class="keyword">else</span> encoder_outputs</div><div class="line"></div><div class="line">    <span class="keyword">for</span> ei <span class="keyword">in</span> range(input_length):</div><div class="line">        encoder_output, encoder_hidden = encoder(input_variable[ei],</div><div class="line">                                                 encoder_hidden)</div><div class="line">        encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[<span class="number">0</span>][<span class="number">0</span>]</div><div class="line"></div><div class="line">    decoder_input = Variable(torch.LongTensor([[SOS_token]]))  <span class="comment"># SOS</span></div><div class="line">    decoder_input = decoder_input.cuda() <span class="keyword">if</span> use_cuda <span class="keyword">else</span> decoder_input</div><div class="line"></div><div class="line">    decoder_hidden = encoder_hidden</div><div class="line"></div><div class="line">    decoded_words = []</div><div class="line">    decoder_attentions = torch.zeros(max_length, max_length)</div><div class="line"></div><div class="line">    <span class="keyword">for</span> di <span class="keyword">in</span> range(max_length):</div><div class="line">        decoder_output, decoder_hidden, decoder_attention = decoder(</div><div class="line">            decoder_input, decoder_hidden, encoder_output, encoder_outputs)</div><div class="line">        decoder_attentions[di] = decoder_attention.data</div><div class="line">        topv, topi = decoder_output.data.topk(<span class="number">1</span>)</div><div class="line">        ni = topi[<span class="number">0</span>][<span class="number">0</span>]</div><div class="line">        <span class="keyword">if</span> ni == EOS_token:</div><div class="line">            decoded_words.append(<span class="string">'&lt;EOS&gt;'</span>)</div><div class="line">            <span class="keyword">break</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            decoded_words.append(output_lang.index2word[ni])</div><div class="line"></div><div class="line">        decoder_input = Variable(torch.LongTensor([[ni]]))</div><div class="line">        decoder_input = decoder_input.cuda() <span class="keyword">if</span> use_cuda <span class="keyword">else</span> decoder_input</div><div class="line"></div><div class="line">    <span class="keyword">return</span> decoded_words, decoder_attentions[:di + <span class="number">1</span>]</div></pre></td></tr></table></figure>
<p>我们能从训练集中随机选择句子来进行评估然后打印输入，目标，和输出以便进行主观的模型效果判断。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluateRandomly</span><span class="params">(encoder, decoder, n=<span class="number">10</span>)</span>:</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</div><div class="line">        pair = random.choice(pairs)</div><div class="line">        print(<span class="string">'&gt;'</span>, pair[<span class="number">0</span>])</div><div class="line">        print(<span class="string">'='</span>, pair[<span class="number">1</span>])</div><div class="line">        output_words, attentions = evaluate(encoder, decoder, pair[<span class="number">0</span>])</div><div class="line">        output_sentence = <span class="string">' '</span>.join(output_words)</div><div class="line">        print(<span class="string">'&lt;'</span>, output_sentence)</div><div class="line">        print(<span class="string">''</span>)</div></pre></td></tr></table></figure>
<h2 id="Training-and-Evaluating-训练和评估"><a href="#Training-and-Evaluating-训练和评估" class="headerlink" title="Training and Evaluating(训练和评估)"></a>Training and Evaluating(训练和评估)</h2><p>通过这些有用的函数(这看起来像是额外的工作，但是它更简单的运行–这里翻译有点问题)我们可以实际初始化神经网络并开始训练。</p>
<p>留意输入的句子是进行深度的过滤。对于这些小的数据集我们可以对应使用拥有256个隐藏节点和一个单独的GRU层的小神经网络。在MacBook上使用CPU差不多40分钟后可以获得一个可信赖的结果<br><strong>Note</strong></p>
<blockquote>
<p>如果你运行这个notebook你可以进行训练，断点内核，评估，和继续训练。注释编码器和解码器的初始化部分然后再次运行<code>trainEpochs</code>(感觉不对呢)。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">hidden_size = <span class="number">256</span></div><div class="line">encoder1 = EncoderRNN(input_lang.n_words, hidden_size)</div><div class="line">attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words,</div><div class="line">                               <span class="number">1</span>, dropout_p=<span class="number">0.1</span>)</div><div class="line"></div><div class="line"><span class="keyword">if</span> use_cuda:</div><div class="line">    encoder1 = encoder1.cuda()</div><div class="line">    attn_decoder1 = attn_decoder1.cuda()</div><div class="line"></div><div class="line">trainEpochs(encoder1, attn_decoder1, <span class="number">75000</span>, print_every=<span class="number">5000</span>)</div></pre></td></tr></table></figure>
<p><img src="/image/seq2seq/lossPic.png"></p>
<p><strong>Out</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">1m 49s (- 25m 35s) (5000 6%) 2.9102</div><div class="line">3m 33s (- 23m 9s) (10000 13%) 2.3276</div><div class="line">5m 18s (- 21m 14s) (15000 20%) 2.0305</div><div class="line">7m 2s (- 19m 23s) (20000 26%) 1.7443</div><div class="line">8m 46s (- 17m 33s) (25000 33%) 1.5357</div><div class="line">10m 29s (- 15m 44s) (30000 40%) 1.3806</div><div class="line">12m 13s (- 13m 58s) (35000 46%) 1.2429</div><div class="line">13m 57s (- 12m 13s) (40000 53%) 1.1654</div><div class="line">15m 41s (- 10m 27s) (45000 60%) 1.0286</div><div class="line">17m 25s (- 8m 42s) (50000 66%) 0.9044</div><div class="line">19m 9s (- 6m 57s) (55000 73%) 0.8432</div><div class="line">20m 53s (- 5m 13s) (60000 80%) 0.7452</div><div class="line">22m 36s (- 3m 28s) (65000 86%) 0.7195</div><div class="line">24m 20s (- 1m 44s) (70000 93%) 0.6386</div><div class="line">26m 3s (- 0m 0s) (75000 100%) 0.5978</div></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">evaluateRandomly(encoder1, attn_decoder1)</div></pre></td></tr></table></figure>
<p><strong>Out</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&gt; vous etes celle la .</div><div class="line">= you are the one .</div><div class="line">&lt; you are the one . &lt;EOS&gt;</div><div class="line"></div><div class="line">&gt; nous sommes encore maries .</div><div class="line">= we re still married .</div><div class="line">&lt; we re still married . &lt;EOS&gt;</div><div class="line"></div><div class="line">&gt; tu vas mourir .</div><div class="line">= you re going to die .</div><div class="line">&lt; you re going to die . &lt;EOS&gt;</div><div class="line"></div><div class="line">&gt; il est non fumeur .</div><div class="line">= he s a nonsmoker .</div><div class="line">&lt; he s a nonsmoker . &lt;EOS&gt;</div></pre></td></tr></table></figure></p>
<h3 id="Visualizing-Attention-可视化注意力"><a href="#Visualizing-Attention-可视化注意力" class="headerlink" title="Visualizing Attention(可视化注意力)"></a>Visualizing Attention(可视化注意力)</h3><p>注意力机制一个有用的特性是它的高度可解释性输出。因为它使用权值指出特定输入序列的编码输出，我们可以图像化寻找神经网络在每个时间步集中注意力在哪里。</p>
<p>你可以简单的运行<code>plt.matshow(attentions)</code>去观察按照矩阵形式输出的注意力，列表示输入步骤行表示输出步骤：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">output_words, attentions = evaluate(</div><div class="line">    encoder1, attn_decoder1, <span class="string">"je suis trop froid ."</span>)</div><div class="line">plt.matshow(attentions.numpy())</div></pre></td></tr></table></figure></p>
<p><img src="/image/seq2seq/attentionpic.png"></p>
<p>为了更好的观察我们将添加额外的坐标和标签：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">showAttention</span><span class="params">(input_sentence, output_words, attentions)</span>:</span></div><div class="line">    <span class="comment"># Set up figure with colorbar</span></div><div class="line">    fig = plt.figure()</div><div class="line">    ax = fig.add_subplot(<span class="number">111</span>)</div><div class="line">    cax = ax.matshow(attentions.numpy(), cmap=<span class="string">'bone'</span>)</div><div class="line">    fig.colorbar(cax)</div><div class="line"></div><div class="line">    <span class="comment"># Set up axes</span></div><div class="line">    ax.set_xticklabels([<span class="string">''</span>] + input_sentence.split(<span class="string">' '</span>) +</div><div class="line">                       [<span class="string">'&lt;EOS&gt;'</span>], rotation=<span class="number">90</span>)</div><div class="line">    ax.set_yticklabels([<span class="string">''</span>] + output_words)</div><div class="line"></div><div class="line">    <span class="comment"># Show label at every tick</span></div><div class="line">    ax.xaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</div><div class="line">    ax.yaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</div><div class="line"></div><div class="line">    plt.show()</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluateAndShowAttention</span><span class="params">(input_sentence)</span>:</span></div><div class="line">    output_words, attentions = evaluate(</div><div class="line">        encoder1, attn_decoder1, input_sentence)</div><div class="line">    print(<span class="string">'input ='</span>, input_sentence)</div><div class="line">    print(<span class="string">'output ='</span>, <span class="string">' '</span>.join(output_words))</div><div class="line">    showAttention(input_sentence, output_words, attentions)</div><div class="line"></div><div class="line"></div><div class="line">evaluateAndShowAttention(<span class="string">"elle a cinq ans de moins que moi ."</span>)</div><div class="line"></div><div class="line">evaluateAndShowAttention(<span class="string">"elle est trop petit ."</span>)</div><div class="line"></div><div class="line">evaluateAndShowAttention(<span class="string">"je ne crains pas de mourir ."</span>)</div><div class="line"></div><div class="line">evaluateAndShowAttention(<span class="string">"c est un jeune directeur plein de talent ."</span>)</div></pre></td></tr></table></figure>
<p><img src="/image/seq2seq/attention4pic.png"></p>
<p><strong>Out</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">input = elle a cinq ans de moins que moi .</div><div class="line">output = she s five years younger than me . &lt;EOS&gt;</div><div class="line">input = elle est trop petit .</div><div class="line">output = she is too late . &lt;EOS&gt;</div><div class="line">input = je ne crains pas de mourir .</div><div class="line">output = i m not scared to die . &lt;EOS&gt;</div><div class="line">input = c est un jeune directeur plein de talent .</div><div class="line">output = he s a talented young guy . &lt;EOS&gt;</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
            <category> translate </category>
            
        </categories>
        
        
        <tags>
            
            <tag> rnn </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Generating Names with a Character-Level RNN(翻译)]]></title>
      <url>/2017/05/23/GeneratName/</url>
      <content type="html"><![CDATA[<p>原文链接：<a href="http://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html" target="_blank" rel="external">Generating Names with a Character-Level RNN</a><br>作者：<a href="https://github.com/spro/practical-pytorch" target="_blank" rel="external">Sean Robertson</a><br>在上一篇<a href="http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html" target="_blank" rel="external">指导</a>我们使用RNN来进行名字所属语言的分类。这次我们换一个方向来根据语言生成名字。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">&gt; python sample.py Russian RUS</div><div class="line">Rovakov</div><div class="line">Uantov</div><div class="line">Shavakov</div><div class="line"></div><div class="line">&gt; python sample.py German GER</div><div class="line">Gerren</div><div class="line">Ereng</div><div class="line">Rosher</div><div class="line"></div><div class="line">&gt; python sample.py Spanish SPA</div><div class="line">Salla</div><div class="line">Parer</div><div class="line">Allan</div><div class="line"></div><div class="line">&gt; python sample.py Chinese CHI</div><div class="line">Chan</div><div class="line">Hang</div><div class="line">Iun</div></pre></td></tr></table></figure></p>
<p>我们任然手动完成一个包含几个线性层的小的RNN。最大的不同是我们并不在读入整个名字后预测其所属语言，而是我们输入分类然后每个时间步输出一个字母。循环预测规定语言的下一个字符(也可以是单词或者其它高级结构)经常也称为“语言模型”。</p>
<h2 id="Recommended-Reading-推荐阅读"><a href="#Recommended-Reading-推荐阅读" class="headerlink" title="Recommended Reading(推荐阅读)"></a>Recommended Reading(推荐阅读)</h2><p>我假设你目前至少安装来PyTorch，了解Python，并且理解Tensor：  </p>
<ul>
<li><a href="http://pytorch.org" target="_blank" rel="external">http://pytorch.org</a>从这里获取安装指导</li>
<li><a href="http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="external">Deep Learning with PyTorch: A 60 Minute Blitz</a>一般从这里开始PyToch</li>
<li><a href="http://pytorch.org/tutorials/beginner/pytorch_with_examples.html" target="_blank" rel="external">Learning PyTorch with Examples</a>获取更有广度和深度的概述</li>
<li><a href="http://pytorch.org/tutorials/beginner/former_torchies_tutorial.html" target="_blank" rel="external">PyTorch for former Torch users</a>如果你是Lua Torch用户</li>
</ul>
<p>下面对你了解RNNs和它们如何工作很有用：</p>
<ul>
<li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">The Unreasonable Effectiveness of Recurrent Neural Networks</a>展示了一些实际生活中的例子</li>
<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM Networks</a>是重点关于LSTMs的但也提供了RNNs的一般信息</li>
</ul>
<p>我也建议你阅读前一个指导，<a href="http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html" target="_blank" rel="external">Classifying Names with a Character-Level RNN</a></p>
<h2 id="Preparing-the-Data-数据准备"><a href="#Preparing-the-Data-数据准备" class="headerlink" title="Preparing the Data(数据准备)"></a>Preparing the Data(数据准备)</h2><p><strong>Note</strong></p>
<blockquote>
<p>从<a href="https://download.pytorch.org/tutorial/data.zip" target="_blank" rel="external">这里</a>下载数据然后解压到当前文件夹</p>
</blockquote>
<p>了解更多这个过程的细节可以看上一篇指导。简而言之，这里有一系列以<code>data/names/[Language].txt</code>命名的纯文本文件每行都是一个名字。我们将每行拆分进数组，从Unicode转换为ASCII，最后生成一个字典<code>{language: [names ...]}</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</div><div class="line"><span class="keyword">from</span> io <span class="keyword">import</span> open</div><div class="line"><span class="keyword">import</span> glob</div><div class="line"><span class="keyword">import</span> unicodedata</div><div class="line"><span class="keyword">import</span> string</div><div class="line"></div><div class="line">all_letters = string.ascii_letters + <span class="string">" .,;'-"</span></div><div class="line">n_letters = len(all_letters) + <span class="number">1</span> <span class="comment"># Plus EOS marker</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">findFiles</span><span class="params">(path)</span>:</span> <span class="keyword">return</span> glob.glob(path)</div><div class="line"></div><div class="line"><span class="comment"># Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">unicodeToAscii</span><span class="params">(s)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(</div><div class="line">        c <span class="keyword">for</span> c <span class="keyword">in</span> unicodedata.normalize(<span class="string">'NFD'</span>, s)</div><div class="line">        <span class="keyword">if</span> unicodedata.category(c) != <span class="string">'Mn'</span></div><div class="line">        <span class="keyword">and</span> c <span class="keyword">in</span> all_letters</div><div class="line">    )</div><div class="line"></div><div class="line"><span class="comment"># Read a file and split into lines</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">readLines</span><span class="params">(filename)</span>:</span></div><div class="line">    lines = open(filename, encoding=<span class="string">'utf-8'</span>).read().strip().split(<span class="string">'\n'</span>)</div><div class="line">    <span class="keyword">return</span> [unicodeToAscii(line) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</div><div class="line"></div><div class="line"><span class="comment"># Build the category_lines dictionary, a list of lines per category</span></div><div class="line">category_lines = &#123;&#125;</div><div class="line">all_categories = []</div><div class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> findFiles(<span class="string">'data/names/*.txt'</span>):</div><div class="line">    category = filename.split(<span class="string">'/'</span>)[<span class="number">-1</span>].split(<span class="string">'.'</span>)[<span class="number">0</span>]</div><div class="line">    all_categories.append(category)</div><div class="line">    lines = readLines(filename)</div><div class="line">    category_lines[category] = lines</div><div class="line"></div><div class="line">n_categories = len(all_categories)</div><div class="line"></div><div class="line">print(<span class="string">'# categories:'</span>, n_categories, all_categories)</div><div class="line">print(unicodeToAscii(<span class="string">"O'Néàl"</span>))</div></pre></td></tr></table></figure>
<p>Out:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># categories: 18 [&apos;Arabic&apos;, &apos;Chinese&apos;, &apos;Czech&apos;, &apos;Dutch&apos;, &apos;English&apos;, &apos;French&apos;, &apos;German&apos;, &apos;Greek&apos;, &apos;Irish&apos;, &apos;Italian&apos;, &apos;Japanese&apos;, &apos;Korean&apos;, &apos;Polish&apos;, &apos;Portuguese&apos;, &apos;Russian&apos;, &apos;Scottish&apos;, &apos;Spanish&apos;, &apos;Vietnamese&apos;]</div><div class="line">O&apos;Neal</div></pre></td></tr></table></figure>
<h2 id="Creating-the-Network-创建神经网络"><a href="#Creating-the-Network-创建神经网络" class="headerlink" title="Creating the Network(创建神经网络)"></a>Creating the Network(创建神经网络)</h2><p>这个神经网络是<a href="http://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html#Creating-the-Network" target="_blank" rel="external">上一篇指导</a>的神经网络的扩展多了分类Tensor作为参数，它们和其它参数存在联系。分类Tensor和字母输入一样是one-hot向量。</p>
<p>我们可以理解输出是下一个字母是什么的概率。当进行采样时，最可能输出字母被作为下一个输入字母使用。</p>
<p>我添加了第二个线性层<code>o2o</code>(在组合了隐藏状态和输出)让这个神经网络更好的工作。这里还有个dropout层，用来根据概率(这里是0.1)随机生成0通常用来稀疏输入数据避免过拟合。这里我们使用它在神经网络来增加一些噪音然后增加采样。</p>
<p><img src="/image/GenNameRnn/genName.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, output_size)</span>:</span></div><div class="line">        super(RNN, self).__init__()</div><div class="line">        self.hidden_size = hidden_size</div><div class="line"></div><div class="line">        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)</div><div class="line">        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)</div><div class="line">        self.o2o = nn.Linear(hidden_size + output_size, output_size)</div><div class="line">        self.dropout = nn.Dropout(<span class="number">0.1</span>)</div><div class="line">        self.softmax = nn.LogSoftmax()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, category, input, hidden)</span>:</span></div><div class="line">        input_combined = torch.cat((category, input, hidden), <span class="number">1</span>)</div><div class="line">        hidden = self.i2h(input_combined)</div><div class="line">        output = self.i2o(input_combined)</div><div class="line">        output_combined = torch.cat((hidden, output), <span class="number">1</span>)</div><div class="line">        output = self.o2o(output_combined)</div><div class="line">        output = self.dropout(output)</div><div class="line">        output = self.softmax(output)</div><div class="line">        <span class="keyword">return</span> output, hidden</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> Variable(torch.zeros(<span class="number">1</span>, self.hidden_size))</div></pre></td></tr></table></figure>
<h2 id="Training-训练"><a href="#Training-训练" class="headerlink" title="Training(训练)"></a>Training(训练)</h2><h3 id="Preparing-for-Training-训练准备"><a href="#Preparing-for-Training-训练准备" class="headerlink" title="Preparing for Training(训练准备)"></a>Preparing for Training(训练准备)</h3><p>首先，一个帮助函数用来获得随机的组合(category,line):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> random</div><div class="line"></div><div class="line"><span class="comment"># Random item from a list</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomChoice</span><span class="params">(l)</span>:</span></div><div class="line">    <span class="keyword">return</span> l[random.randint(<span class="number">0</span>, len(l) - <span class="number">1</span>)]</div><div class="line"></div><div class="line"><span class="comment"># Get a random category and random line from that category</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomTrainingPair</span><span class="params">()</span>:</span></div><div class="line">    category = randomChoice(all_categories)</div><div class="line">    line = randomChoice(category_lines[category])</div><div class="line">    <span class="keyword">return</span> category, line</div></pre></td></tr></table></figure>
<p>对于每个时间步(指每个训练单词中的字母)输入神经网络的是<code>(category, current letter, hidden state)</code>输出是<code>(next letter, next hidden state)</code>。因此对于每个训练集，我们需要分类，输入字母集，和输出集/目标集。</p>
<p>因此在每个时间步骤我们从当前字母去预测下一个字母，字母组合是一组从每行名字得到的连续字母-e.g.对于<code>ABCD&lt;EOS&gt;</code>我们会创建(“A”, “B”),(“B”,”C”),(“C”,”D”),(“D”,”EOS”)。</p>
<p><img src="/image/GenNameRnn/letter.png"></p>
<p>分类tensor是<a href="https://en.wikipedia.org/wiki/One-hot" target="_blank" rel="external">one-hot tensor</a>大小是<code>&lt;1 x n_categories&gt;</code>。当训练的时候在每个时间步骤我们将它输入到神经网络-这个是一个设计选择，它可能作为初始化的隐藏状态活着一些其它策略的一部分。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># One-hot vector for category</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">categoryTensor</span><span class="params">(category)</span>:</span></div><div class="line">    li = all_categories.index(category)</div><div class="line">    tensor = torch.zeros(<span class="number">1</span>, n_categories)</div><div class="line">    tensor[<span class="number">0</span>][li] = <span class="number">1</span></div><div class="line">    <span class="keyword">return</span> tensor</div><div class="line"></div><div class="line"><span class="comment"># One-hot matrix of first to last letters (not including EOS) for input</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">inputTensor</span><span class="params">(line)</span>:</span></div><div class="line">    tensor = torch.zeros(len(line), <span class="number">1</span>, n_letters)</div><div class="line">    <span class="keyword">for</span> li <span class="keyword">in</span> range(len(line)):</div><div class="line">        letter = line[li]</div><div class="line">        tensor[li][<span class="number">0</span>][all_letters.find(letter)] = <span class="number">1</span></div><div class="line">    <span class="keyword">return</span> tensor</div><div class="line"></div><div class="line"><span class="comment"># LongTensor of second letter to end (EOS) for target</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">targetTensor</span><span class="params">(line)</span>:</span></div><div class="line">    letter_indexes = [all_letters.find(line[li]) <span class="keyword">for</span> li <span class="keyword">in</span> range(<span class="number">1</span>, len(line))]</div><div class="line">    letter_indexes.append(n_letters - <span class="number">1</span>) <span class="comment"># EOS</span></div><div class="line">    <span class="keyword">return</span> torch.LongTensor(letter_indexes)</div></pre></td></tr></table></figure>
<p>为了方便在训练中我们将使用<code>randomTrainingSet</code>函数获取(category,line)训练组合然后将其转换为要求(category,input,target)的tensor。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Make category, input, and target tensors from a random category, line pair</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomTrainingSet</span><span class="params">()</span>:</span></div><div class="line">    category, line = randomTrainingPair()</div><div class="line">    category_tensor = Variable(categoryTensor(category))</div><div class="line">    input_line_tensor = Variable(inputTensor(line))</div><div class="line">    target_line_tensor = Variable(targetTensor(line))</div><div class="line">    <span class="keyword">return</span> category_tensor, input_line_tensor, target_line_tensor</div></pre></td></tr></table></figure></p>
<h3 id="Training-the-Network-训练神经网络"><a href="#Training-the-Network-训练神经网络" class="headerlink" title="Training the Network(训练神经网络)"></a>Training the Network(训练神经网络)</h3><p>不同于分类我们只需使用最后一个输出，这里我们需要预测每一时间步，因此我们需要计算每个时间步的损失。</p>
<p>神奇的是自动梯度计算允许你简单的求和每个步骤的损失然后调用反向传播。但是不要问我为什么初始化loss为0。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">criterion = nn.NLLLoss()</div><div class="line"></div><div class="line">learning_rate = <span class="number">0.0005</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(category_tensor, input_line_tensor, target_line_tensor)</span>:</span></div><div class="line">    hidden = rnn.initHidden()</div><div class="line"></div><div class="line">    rnn.zero_grad()</div><div class="line"></div><div class="line">    loss = <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(input_line_tensor.size()[<span class="number">0</span>]):</div><div class="line">        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)</div><div class="line">        loss += criterion(output, target_line_tensor[i])</div><div class="line"></div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> rnn.parameters():</div><div class="line">        p.data.add_(-learning_rate, p.grad.data)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> output, loss.data[<span class="number">0</span>] / input_line_tensor.size()[<span class="number">0</span>]</div></pre></td></tr></table></figure>
<p>为了保持跟踪我训练了多长时间我添加了一个<code>timeSince(timestamp)</code>函数它会返回一个可读的字符串。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">import</span> math</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeSince</span><span class="params">(since)</span>:</span></div><div class="line">    now = time.time()</div><div class="line">    s = now - since</div><div class="line">    m = math.floor(s / <span class="number">60</span>)</div><div class="line">    s -= m * <span class="number">60</span></div><div class="line">    <span class="keyword">return</span> <span class="string">'%dm %ds'</span> % (m, s)</div></pre></td></tr></table></figure></p>
<p>训练和前面一样-调用训练后等待一段时间，打印当前时间和损失在每个<code>print_every</code>步，保存每<code>plot_every</code>步的平均损失在<code>all_losses</code>打印。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">rnn = RNN(n_letters, <span class="number">128</span>, n_letters)</div><div class="line"></div><div class="line">n_epochs = <span class="number">100000</span></div><div class="line">print_every = <span class="number">5000</span></div><div class="line">plot_every = <span class="number">500</span></div><div class="line">all_losses = []</div><div class="line">total_loss = <span class="number">0</span> <span class="comment"># Reset every plot_every epochs</span></div><div class="line"></div><div class="line">start = time.time()</div><div class="line"></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, n_epochs + <span class="number">1</span>):</div><div class="line">    output, loss = train(*randomTrainingSet())</div><div class="line">    total_loss += loss</div><div class="line"></div><div class="line">    <span class="keyword">if</span> epoch % print_every == <span class="number">0</span>:</div><div class="line">        print(<span class="string">'%s (%d %d%%) %.4f'</span> % (timeSince(start), epoch, epoch / n_epochs * <span class="number">100</span>, loss))</div><div class="line"></div><div class="line">    <span class="keyword">if</span> epoch % plot_every == <span class="number">0</span>:</div><div class="line">        all_losses.append(total_loss / plot_every)</div><div class="line">        total_loss = <span class="number">0</span></div></pre></td></tr></table></figure></p>
<p>Out:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">0m 12s (5000 5%) 2.3159</div><div class="line">0m 24s (10000 10%) 2.6862</div><div class="line">0m 36s (15000 15%) 1.8658</div><div class="line">0m 49s (20000 20%) 2.5288</div><div class="line">1m 1s (25000 25%) 2.2218</div><div class="line">1m 14s (30000 30%) 3.0179</div><div class="line">1m 26s (35000 35%) 2.2104</div><div class="line">1m 39s (40000 40%) 2.2386</div><div class="line">1m 51s (45000 45%) 2.0167</div><div class="line">2m 3s (50000 50%) 2.3285</div><div class="line">2m 16s (55000 55%) 1.8585</div><div class="line">2m 28s (60000 60%) 2.2254</div><div class="line">2m 40s (65000 65%) 2.6131</div><div class="line">2m 53s (70000 70%) 2.1845</div><div class="line">3m 5s (75000 75%) 1.9502</div><div class="line">3m 17s (80000 80%) 1.7935</div><div class="line">3m 30s (85000 85%) 1.9056</div><div class="line">3m 42s (90000 90%) 2.4642</div><div class="line">3m 54s (95000 95%) 1.8065</div><div class="line">4m 7s (100000 100%) 2.0977</div></pre></td></tr></table></figure></p>
<h3 id="Plotting-the-Losses-绘制损失"><a href="#Plotting-the-Losses-绘制损失" class="headerlink" title="Plotting the Losses(绘制损失)"></a>Plotting the Losses(绘制损失)</h3><p>从神经网络学习中的all_losses绘制历史损失变化。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> matplotlib.ticker <span class="keyword">as</span> ticker</div><div class="line"></div><div class="line">plt.figure()</div><div class="line">plt.plot(all_losses)</div></pre></td></tr></table></figure></p>
<p><img src="/image/GenNameRnn/loss.png"></p>
<h2 id="Sampling-the-Network-神经网络抽样"><a href="#Sampling-the-Network-神经网络抽样" class="headerlink" title="Sampling the Network(神经网络抽样)"></a>Sampling the Network(神经网络抽样)</h2><p>为了抽样我们提供给神经网络一个字母然后要求下一个，再将它作为下一个输入传递给神经网络，如此循环直到EOS标志出现。</p>
<ul>
<li>创建一个tensor以便输入分类，开始单词，和空的隐藏状态</li>
<li>创建一个开始字母的<code>output_name</code></li>
<li>直到最大的输出长度<ul>
<li>返回输出字母给神经网络</li>
<li>获取下一个最大可能的输出，和下一个隐藏层状态</li>
<li>如果字母是EOS就停止循环</li>
<li>如果是正常字母就添加到<code>output_name</code>然后继续</li>
</ul>
</li>
<li>返回最终的名字</li>
</ul>
<p><strong>Note</strong></p>
<blockquote>
<p>有些策略不是提供一个开始字母而是在训练时包括了一个”开始”标志，然后神经网络自行选择开始的字母</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">max_length = <span class="number">20</span></div><div class="line"></div><div class="line"><span class="comment"># Sample from a category and starting letter</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(category, start_letter=<span class="string">'A'</span>)</span>:</span></div><div class="line">    category_tensor = Variable(categoryTensor(category))</div><div class="line">    input = Variable(inputTensor(start_letter))</div><div class="line">    hidden = rnn.initHidden()</div><div class="line"></div><div class="line">    output_name = start_letter</div><div class="line"></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_length):</div><div class="line">        output, hidden = rnn(category_tensor, input[<span class="number">0</span>], hidden)</div><div class="line">        topv, topi = output.data.topk(<span class="number">1</span>)</div><div class="line">        topi = topi[<span class="number">0</span>][<span class="number">0</span>]</div><div class="line">        <span class="keyword">if</span> topi == n_letters - <span class="number">1</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            letter = all_letters[topi]</div><div class="line">            output_name += letter</div><div class="line">        input = Variable(inputTensor(letter))</div><div class="line"></div><div class="line">    <span class="keyword">return</span> output_name</div><div class="line"></div><div class="line"><span class="comment"># Get multiple samples from one category and multiple starting letters</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">samples</span><span class="params">(category, start_letters=<span class="string">'ABC'</span>)</span>:</span></div><div class="line">    <span class="keyword">for</span> start_letter <span class="keyword">in</span> start_letters:</div><div class="line">        print(sample(category, start_letter))</div><div class="line"></div><div class="line">samples(<span class="string">'Russian'</span>, <span class="string">'RUS'</span>)</div><div class="line"></div><div class="line">samples(<span class="string">'German'</span>, <span class="string">'GER'</span>)</div><div class="line"></div><div class="line">samples(<span class="string">'Spanish'</span>, <span class="string">'SPA'</span>)</div><div class="line"></div><div class="line">samples(<span class="string">'Chinese'</span>, <span class="string">'CHI'</span>)</div></pre></td></tr></table></figure>
<p>Out:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">Rovallov</div><div class="line">Uarinov</div><div class="line">Shavanov</div><div class="line">Garter</div><div class="line">Eren</div><div class="line">Romall</div><div class="line">Sallan</div><div class="line">Perran</div><div class="line">Allan</div><div class="line">Chang</div><div class="line">Ha</div><div class="line">Iua</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
            <category> translate </category>
            
        </categories>
        
        
        <tags>
            
            <tag> rnn </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Classifying Names with a Character-Level RNN(翻译)]]></title>
      <url>/2017/05/22/classifyName/</url>
      <content type="html"><![CDATA[<p>原文链接：<a href="http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html" target="_blank" rel="external">Classifying Names with a Character-Level RNN</a><br>作者：<a href="https://github.com/spro/practical-pytorch" target="_blank" rel="external">Sean Robertson</a></p>
<p>我们将构建和训练一个基础的字符级别的RNN网络去进行词汇分类。一个字符级别的RNN将词汇作为字符序列读入-在每步输出预测值和“隐藏状态”，将上一步的隐藏状态传递给下一步。我们将最后的预测作为输出，i.e.就是词汇所属的分类。<br><a id="more"></a><br>具体来说，我们会从18种语言中训练几千个姓，然后通过一个姓的拼写来预测它属于哪种语言：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">$ python predict.py Hinton</div><div class="line">(<span class="number">-0.47</span>) Scottish</div><div class="line">(<span class="number">-1.52</span>) English</div><div class="line">(<span class="number">-3.57</span>) Irish</div><div class="line"></div><div class="line">$ python predict.py Schmidhuber</div><div class="line">(<span class="number">-0.19</span>) German</div><div class="line">(<span class="number">-2.48</span>) Czech</div><div class="line">(<span class="number">-2.68</span>) Dutch</div></pre></td></tr></table></figure></p>
<h2 id="Recommended-Reading-推荐阅读"><a href="#Recommended-Reading-推荐阅读" class="headerlink" title="Recommended Reading(推荐阅读)"></a>Recommended Reading(推荐阅读)</h2><p>我假设你目前至少安装来PyTorch，了解Python，并且理解Tensor：  </p>
<ul>
<li><a href="http://pytorch.org" target="_blank" rel="external">http://pytorch.org</a>从这里获取安装指导</li>
<li><a href="http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="external">Deep Learning with PyTorch: A 60 Minute Blitz</a>一般从这里开始PyToch</li>
<li><a href="http://pytorch.org/tutorials/beginner/pytorch_with_examples.html" target="_blank" rel="external">Learning PyTorch with Examples</a>获取更有广度和深度的概述</li>
<li><a href="http://pytorch.org/tutorials/beginner/former_torchies_tutorial.html" target="_blank" rel="external">PyTorch for former Torch users</a>如果你是Lua Torch用户</li>
</ul>
<p>下面对你了解RNNs和它们如何工作很有用：</p>
<ul>
<li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">The Unreasonable Effectiveness of Recurrent Neural Networks</a>展示了一些实际生活中的例子</li>
<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM Networks</a>是重点关于LSTMs的但也提供了RNNs的一般信息</li>
</ul>
<h2 id="Preparing-the-Data-数据准备"><a href="#Preparing-the-Data-数据准备" class="headerlink" title="Preparing the Data(数据准备)"></a>Preparing the Data(数据准备)</h2><p><strong>Note</strong></p>
<blockquote>
<p>从<a href="https://download.pytorch.org/tutorial/data.zip" target="_blank" rel="external">这里</a>下载数据并解压到当前文件</p>
</blockquote>
<p>包含在<code>data/names</code>目录下的是18个以“[Language].txt”格式命名的文件。每个文件包含了一些名字，一个名字一行，大部分是罗马字母(但是我们任然需要将编码格式从Unicode转换为ASCII)。</p>
<p>我们最终得到的是一个以语言名称为键以名字列表为值的字典，<code>{language:[names ...]}</code>。通常的变量“category”和“line”(在我们的例子中对应语言和名字)是用来后续扩展。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</div><div class="line"><span class="keyword">from</span> io <span class="keyword">import</span> open</div><div class="line"><span class="keyword">import</span> glob</div><div class="line"><span class="comment">#glob是python自己带的一个文件操作相关模块，用它可以查找符合自己目的的文件，只会</span></div><div class="line"><span class="comment">#当前查询目录下匹配到的文件路径，不关注子文件目录</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">findFiles</span><span class="params">(path)</span>:</span> <span class="keyword">return</span> glob.glob(path)</div><div class="line"></div><div class="line">print(findFiles(<span class="string">'data/names/*.txt'</span>))</div><div class="line"></div><div class="line"><span class="keyword">import</span> unicodedata</div><div class="line"><span class="keyword">import</span> string</div><div class="line"></div><div class="line">all_letters = string.ascii_letters + <span class="string">" .,;'"</span></div><div class="line">n_letters = len(all_letters)</div><div class="line"></div><div class="line"><span class="comment"># Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">unicodeToAscii</span><span class="params">(s)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(</div><div class="line">        c <span class="keyword">for</span> c <span class="keyword">in</span> unicodedata.normalize(<span class="string">'NFD'</span>, s)</div><div class="line">        <span class="keyword">if</span> unicodedata.category(c) != <span class="string">'Mn'</span></div><div class="line">        <span class="keyword">and</span> c <span class="keyword">in</span> all_letters</div><div class="line">    )</div><div class="line"></div><div class="line">print(unicodeToAscii(<span class="string">'Ślusàrski'</span>))</div><div class="line"></div><div class="line"><span class="comment"># Build the category_lines dictionary, a list of names per language</span></div><div class="line">category_lines = &#123;&#125;</div><div class="line">all_categories = []</div><div class="line"></div><div class="line"><span class="comment"># Read a file and split into lines</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">readLines</span><span class="params">(filename)</span>:</span></div><div class="line">    lines = open(filename, encoding=<span class="string">'utf-8'</span>).read().strip().split(<span class="string">'\n'</span>)</div><div class="line">    <span class="keyword">return</span> [unicodeToAscii(line) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</div><div class="line"><span class="comment"># 也可以使用os.path.basename('data/names/test.txt')得到test.txt</span></div><div class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> findFiles(<span class="string">'data/names/*.txt'</span>):</div><div class="line">    category = filename.split(<span class="string">'/'</span>)[<span class="number">-1</span>].split(<span class="string">'.'</span>)[<span class="number">0</span>]</div><div class="line">    all_categories.append(category)</div><div class="line">    lines = readLines(filename)</div><div class="line">    category_lines[category] = lines</div><div class="line"></div><div class="line">n_categories = len(all_categories)</div></pre></td></tr></table></figure>
<p>Out:</p>
<blockquote>
<p>[‘data/names/Arabic.txt’, ‘data/names/Chinese.txt’, ‘data/names/Czech.txt’, ‘data/names/Dutch.txt’, ‘data/names/English.txt’, ‘data/names/French.txt’, ‘data/names/German.txt’, ‘data/names/Greek.txt’, ‘data/names/Irish.txt’, ‘data/names/Italian.txt’, ‘data/names/Japanese.txt’, ‘data/names/Korean.txt’, ‘data/names/Polish.txt’, ‘data/names/Portuguese.txt’, ‘data/names/Russian.txt’, ‘data/names/Scottish.txt’, ‘data/names/Spanish.txt’, ‘data/names/Vietnamese.txt’]<br>Slusarski</p>
</blockquote>
<p>现在我们有<code>category_lines</code>，一个从每个分类(语言)到行的列表(名字)映射的字典。我们也记下了<code>all_categories</code>(语言列表)和<code>n_categories</code>后续使用。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">print(category_lines[<span class="string">'Italian'</span>][:<span class="number">5</span>])</div></pre></td></tr></table></figure></p>
<p>Out:</p>
<blockquote>
<p>[‘Abandonato’, ‘Abatangelo’, ‘Abatantuono’, ‘Abate’, ‘Abategiovanni’]</p>
</blockquote>
<h3 id="Turning-Names-into-Tensors-转换名字为Tensor"><a href="#Turning-Names-into-Tensors-转换名字为Tensor" class="headerlink" title="Turning Names into Tensors(转换名字为Tensor)"></a>Turning Names into Tensors(转换名字为Tensor)</h3><p>现在我们已经有了所有组织好的名字，我们需要将它们转换为Tensor以便利用。</p>
<p>为了表示字母，我们使用<code>&lt;1 x n_letters&gt;</code>大小的“one-hot vector”。一个one-hot vector除当前字母的位置填为1外，使用0来填充其它位置，e.g.<code>&quot;b&quot;=&lt;0 1 0 0 0 ...&gt;</code>。</p>
<p>为了表示一个词语我们把一串这些字母整合进一个2维矩阵<code>&lt;line_length x 1 x n_letter&gt;</code>。<br>这个多出来的一维是因为PyTorch假设任何东西是在一个批次中，我们仅使用一个大小为1的批次在这里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"></div><div class="line"><span class="comment"># Find letter index from all_letters, e.g. "a" = 0</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">letterToIndex</span><span class="params">(letter)</span>:</span></div><div class="line">    <span class="keyword">return</span> all_letters.find(letter)</div><div class="line"></div><div class="line"><span class="comment"># Just for demonstration, turn a letter into a &lt;1 x n_letters&gt; Tensor</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">letterToTensor</span><span class="params">(letter)</span>:</span></div><div class="line">    tensor = torch.zeros(<span class="number">1</span>, n_letters)</div><div class="line">    tensor[<span class="number">0</span>][letterToIndex(letter)] = <span class="number">1</span></div><div class="line">    <span class="keyword">return</span> tensor</div><div class="line"></div><div class="line"><span class="comment"># Turn a line into a &lt;line_length x 1 x n_letters&gt;,</span></div><div class="line"><span class="comment"># or an array of one-hot letter vectors</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lineToTensor</span><span class="params">(line)</span>:</span></div><div class="line">    tensor = torch.zeros(len(line), <span class="number">1</span>, n_letters)</div><div class="line">    <span class="keyword">for</span> li, letter <span class="keyword">in</span> enumerate(line):</div><div class="line">        tensor[li][<span class="number">0</span>][letterToIndex(letter)] = <span class="number">1</span></div><div class="line">    <span class="keyword">return</span> tensor</div><div class="line"></div><div class="line">print(letterToTensor(<span class="string">'J'</span>))</div><div class="line"></div><div class="line">print(lineToTensor(<span class="string">'Jones'</span>).size())</div></pre></td></tr></table></figure>
<p>Out:</p>
<blockquote>
<p>Columns 0 to 12<br>    0     0     0     0     0     0     0     0     0     0     0     0     0</p>
<p>Columns 13 to 25<br>    0     0     0     0     0     0     0     0     0     0     0     0     0</p>
<p>Columns 26 to 38<br>    0     0     0     0     0     0     0     0     0     1     0     0     0</p>
<p>Columns 39 to 51<br>    0     0     0     0     0     0     0     0     0     0     0     0     0</p>
<p>Columns 52 to 56<br>    0     0     0     0     0<br>[torch.FloatTensor of size 1x57]</p>
<p>torch.Size([5, 1, 57])</p>
</blockquote>
<h2 id="Creating-the-Network-创建神经网络"><a href="#Creating-the-Network-创建神经网络" class="headerlink" title="Creating the Network(创建神经网络)"></a>Creating the Network(创建神经网络)</h2><p>在自动梯度计算前，使用Torch创建一个循环神经网络需要按照时间步骤克隆每层的参数。每个层拥有隐藏状态和梯度现在整个的都在计算图中。这意味你可以很“纯净”的方式实现一个RNN，就像普通的前向层网络。</p>
<p>这个RNN模型(大部分拷贝自<a href="https://github.com/pytorch/tutorials/blob/master/Introduction%20to%20PyTorch%20for%20former%20Torchies.ipynb" target="_blank" rel="external">the PyTorch for Torch users tutorial</a>)是一个用来操作输入和隐藏层状态的两层线性层，和处理输出的LogSoftmax层组成。</p>
<p><img src="/image/NameRnn/rnnModel.png"><br>这里说明logsoftmax的公式是$f_i(x)=log(\frac{e^{x_i}}{\sum_{j}e^{x_j}})$<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, output_size)</span>:</span></div><div class="line">        super(RNN, self).__init__()</div><div class="line"></div><div class="line">        self.hidden_size = hidden_size</div><div class="line"></div><div class="line">        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)</div><div class="line">        self.i2o = nn.Linear(input_size + hidden_size, output_size)</div><div class="line">        <span class="comment"># 带有log处理的softmax</span></div><div class="line">        self.softmax = nn.LogSoftmax()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden)</span>:</span></div><div class="line">        <span class="comment"># 按照dim=1列进行拼接</span></div><div class="line">        combined = torch.cat((input, hidden), <span class="number">1</span>)</div><div class="line">        hidden = self.i2h(combined)</div><div class="line">        output = self.i2o(combined)</div><div class="line">        output = self.softmax(output)</div><div class="line">        <span class="keyword">return</span> output, hidden</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> Variable(torch.zeros(<span class="number">1</span>, self.hidden_size))</div><div class="line"></div><div class="line">n_hidden = <span class="number">128</span></div><div class="line">rnn = RNN(n_letters, n_hidden, n_categories)</div></pre></td></tr></table></figure></p>
<p>为了运行这样的网络一步我们需要通过输入数据(在我们的例子中是表示当前字母的Tensor)和上一个隐层状态(我们先使用0来初始化隐层状态)。我们将获得输出(分类语言的概率)和下一个隐层状态(保存以便下一步使用)</p>
<p>需要留意PyTorch模型是在Variable上操作而不是直接操作Tensor。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">input = Variable(letterToTensor(<span class="string">'A'</span>))</div><div class="line">hidden = Variable(torch.zeros(<span class="number">1</span>, n_hidden))</div><div class="line"></div><div class="line">output, next_hidden = rnn(input, hidden)</div></pre></td></tr></table></figure></p>
<p>从效率考虑我们不想每步都创建一个Tensor，因此我们使用<code>lineToTensor</code>而不是<code>letterToTensor</code>然后使用切片操作。这个通过预计算Tensor的批次来进一步优化。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">input = Variable(lineToTensor(<span class="string">'Albert'</span>))</div><div class="line">hidden = Variable(torch.zeros(<span class="number">1</span>, n_hidden))</div><div class="line"></div><div class="line">output, next_hidden = rnn(input[<span class="number">0</span>], hidden)</div><div class="line">print(output)</div></pre></td></tr></table></figure></p>
<p>Out:</p>
<blockquote>
<p>Variable containing:</p>
<p>Columns 0 to 9<br>-2.9407 -3.0152 -2.7955 -2.8986 -2.9942 -2.8066 -2.7735 -2.8927 -2.8127 -2.9285</p>
<p>Columns 10 to 17<br>-2.9342 -2.9568 -2.9258 -2.8479 -2.9587 -2.8280 -2.8510 -2.9090<br>[torch.FloatTensor of size 1x18]</p>
</blockquote>
<p>正如你看到的输出是一个<code>&lt;1 x n_categories&gt;</code>Tensor，它的每一个小项目都是名字所属语言分类的似然估计(越大越可能)</p>
<h2 id="Training-训练"><a href="#Training-训练" class="headerlink" title="Training(训练)"></a>Training(训练)</h2><h3 id="Preparing-for-Training-训练准备"><a href="#Preparing-for-Training-训练准备" class="headerlink" title="Preparing for Training(训练准备)"></a>Preparing for Training(训练准备)</h3><p>在进行训练前我们应该完成一些有用的函数。第一个是用来解析输出结果的函数，我们知道它们是属于某个分类的似然估计。我们使用<code>Tensor.topk</code>去获取最大值的索引。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">categoryFromOutput</span><span class="params">(output)</span>:</span></div><div class="line">    <span class="comment"># topk(1)返回每一行数据最大的一个值的取值和在每行的位置信息。</span></div><div class="line">    top_n, top_i = output.data.topk(<span class="number">1</span>) <span class="comment"># Tensor out of Variable with .data</span></div><div class="line">    category_i = top_i[<span class="number">0</span>][<span class="number">0</span>]</div><div class="line">    <span class="keyword">return</span> all_categories[category_i], category_i</div><div class="line"></div><div class="line">print(categoryFromOutput(output))</div></pre></td></tr></table></figure></p>
<p>Out：</p>
<blockquote>
<p>(‘German’, 6)</p>
</blockquote>
<p>我们也需要一个快速获取训练样例的方法(名字加所属分类)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> random</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomChoice</span><span class="params">(l)</span>:</span></div><div class="line">    <span class="keyword">return</span> l[random.randint(<span class="number">0</span>, len(l) - <span class="number">1</span>)]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomTrainingPair</span><span class="params">()</span>:</span></div><div class="line">    category = randomChoice(all_categories)</div><div class="line">    line = randomChoice(category_lines[category])</div><div class="line">    category_tensor = Variable(torch.LongTensor([all_categories.index(category)]))</div><div class="line">    line_tensor = Variable(lineToTensor(line))</div><div class="line">    <span class="keyword">return</span> category, line, category_tensor, line_tensor</div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">    category, line, category_tensor, line_tensor = randomTrainingPair()</div><div class="line">    print(<span class="string">'category ='</span>, category, <span class="string">'/ line ='</span>, line)</div></pre></td></tr></table></figure></p>
<p>Out：</p>
<blockquote>
<p>category = Dutch / line = Oomen<br>category = Arabic / line = Kassis<br>category = Irish / line = Mclain<br>category = Czech / line = Kerner<br>category = Chinese / line = She<br>category = Scottish / line = Anderson<br>category = Polish / line = Sokolof<br>category = Irish / line = Daly<br>category = Vietnamese / line = An<br>category = Russian / line = Tsagareli</p>
</blockquote>
<h3 id="Training-the-Network-训练神经网络"><a href="#Training-the-Network-训练神经网络" class="headerlink" title="Training the Network(训练神经网络)"></a>Training the Network(训练神经网络)</h3><p>现在所有要做的通过展示大量的训练例子给神经网络进行训练，让它进行预测，然后告诉它正确与否。</p>
<p>使用<code>nn.NLLLoss</code>作为损失函数便足够了，因为RNNs的最后一层事<code>nn.LogSoftmax</code>它加上<code>nn.NLLLoss</code>便是交叉熵损失函数的表达式。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">criterion = nn.NLLLoss()</div></pre></td></tr></table></figure></p>
<p>每个训练中的循环会完成：</p>
<ul>
<li>创建输入和目标tensor</li>
<li>使用0初始化隐藏状态</li>
<li>读取每个字母<ul>
<li>并为下轮保持隐藏状态</li>
</ul>
</li>
<li>计算输出层输出结果</li>
<li>进行反向传播</li>
<li>返回输出结果和误差</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">learning_rate = <span class="number">0.005</span> <span class="comment"># If you set this too high, it might explode. If too low, it might not learn</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(category_tensor, line_tensor)</span>:</span></div><div class="line">    hidden = rnn.initHidden()</div><div class="line"></div><div class="line">    rnn.zero_grad()</div><div class="line"></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(line_tensor.size()[<span class="number">0</span>]):</div><div class="line">        output, hidden = rnn(line_tensor[i], hidden)</div><div class="line"></div><div class="line">    loss = criterion(output, category_tensor)</div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    <span class="comment"># Add parameters' gradients to their values, multiplied by learning rate</span></div><div class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> rnn.parameters():</div><div class="line">        p.data.add_(-learning_rate, p.grad.data)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> output, loss.data[<span class="number">0</span>]</div></pre></td></tr></table></figure>
<p>现在我们需要训练大量的训练样例。因为<code>train</code>函数返回了输出和损失我们可以打印预测值和保存损失以便绘图。因为这里有1000s训练例子我们按照<code>print_every</code>一次时间步骤打印，然后取loss的平均值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">import</span> math</div><div class="line"></div><div class="line">n_epochs = <span class="number">100000</span></div><div class="line">print_every = <span class="number">5000</span></div><div class="line">plot_every = <span class="number">1000</span></div><div class="line"></div><div class="line">rnn = RNN(n_letters, n_hidden, n_categories)</div><div class="line"></div><div class="line"><span class="comment"># Keep track of losses for plotting</span></div><div class="line">current_loss = <span class="number">0</span></div><div class="line">all_losses = []</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeSince</span><span class="params">(since)</span>:</span></div><div class="line">    now = time.time()</div><div class="line">    s = now - since</div><div class="line">    m = math.floor(s / <span class="number">60</span>)</div><div class="line">    s -= m * <span class="number">60</span></div><div class="line">    <span class="keyword">return</span> <span class="string">'%dm %ds'</span> % (m, s)</div><div class="line"></div><div class="line">start = time.time()</div><div class="line"></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, n_epochs + <span class="number">1</span>):</div><div class="line">    category, line, category_tensor, line_tensor = randomTrainingPair()</div><div class="line">    output, loss = train(category_tensor, line_tensor)</div><div class="line">    current_loss += loss</div><div class="line"></div><div class="line">    <span class="comment"># Print epoch number, loss, name and guess</span></div><div class="line">    <span class="keyword">if</span> epoch % print_every == <span class="number">0</span>:</div><div class="line">        guess, guess_i = categoryFromOutput(output)</div><div class="line">        correct = <span class="string">'✓'</span> <span class="keyword">if</span> guess == category <span class="keyword">else</span> <span class="string">'✗ (%s)'</span> % category</div><div class="line">        print(<span class="string">'%d %d%% (%s) %.4f %s / %s %s'</span> % (epoch, epoch / n_epochs * <span class="number">100</span>, timeSince(start), loss, line, guess, correct))</div><div class="line"></div><div class="line">    <span class="comment"># Add current loss avg to list of losses</span></div><div class="line">    <span class="keyword">if</span> epoch % plot_every == <span class="number">0</span>:</div><div class="line">        all_losses.append(current_loss / plot_every)</div><div class="line">        current_loss = <span class="number">0</span></div></pre></td></tr></table></figure></p>
<p>Out:</p>
<blockquote>
<p>5000 5% (0m 5s) 2.8818 Aodh / Vietnamese ✗ (Irish)<br>10000 10% (0m 10s) 2.5445 Radford / Japanese ✗ (English)<br>15000 15% (0m 15s) 2.6274 Viteri / Italian ✗ (Spanish)<br>20000 20% (0m 20s) 3.4456 Molloy / Scottish ✗ (Irish)<br>25000 25% (0m 25s) 0.3957 Demakis / Greek ✓<br>30000 30% (0m 30s) 3.5076 Jez / Chinese ✗ (Polish)<br>35000 35% (0m 35s) 2.1709 Docherty / English ✗ (Scottish)<br>40000 40% (0m 40s) 0.9810 Paszek / Polish ✓<br>45000 45% (0m 45s) 0.7359 Ryu / Korean ✓<br>50000 50% (0m 50s) 2.6953 Delaney / Spanish ✗ (Irish)<br>55000 55% (0m 55s) 1.4272 Lieu / Vietnamese ✓<br>60000 60% (1m 0s) 1.8603 Holzer / Polish ✗ (German)<br>65000 65% (1m 5s) 2.6793 Costa / Czech ✗ (Portuguese)<br>70000 70% (1m 10s) 0.6097 Dalach / Irish ✓<br>75000 75% (1m 15s) 0.1429 Kowalczyk / Polish ✓<br>80000 80% (1m 20s) 0.8101 Cha / Korean ✓<br>85000 85% (1m 25s) 0.2391 Travert / French ✓<br>90000 90% (1m 30s) 1.8437 Nuremberg / Dutch ✗ (German)<br>95000 95% (1m 35s) 1.2834 Faucher / French ✓<br>100000 100% (1m 40s) 0.5508 Urogataya / Japanese ✓</p>
</blockquote>
<h3 id="Plotting-the-Results-绘制结果"><a href="#Plotting-the-Results-绘制结果" class="headerlink" title="Plotting the Results(绘制结果)"></a>Plotting the Results(绘制结果)</h3><p>根据<code>all_losses</code>绘制历史损失展示深度网络学习。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> matplotlib.ticker <span class="keyword">as</span> ticker</div><div class="line"></div><div class="line">plt.figure()</div><div class="line">plt.plot(all_losses)</div></pre></td></tr></table></figure></p>
<p><img src="/image/NameRnn/loss.png"></p>
<h2 id="Evaluating-the-Results（验证结果）"><a href="#Evaluating-the-Results（验证结果）" class="headerlink" title="Evaluating the Results（验证结果）"></a>Evaluating the Results（验证结果）</h2><p>为了观察神经网络在不同分类熵的表现，我们会创建一个混和的矩阵，标示出实际语言(行)和模型预测语言(列)。为了计算混合矩阵一系列的样本将在神经网络中运行通过<code>evaluate()</code>，它和<code>train()</code>除去backprop一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Keep track of correct guesses in a confusion matrix</span></div><div class="line">confusion = torch.zeros(n_categories, n_categories)</div><div class="line">n_confusion = <span class="number">10000</span></div><div class="line"></div><div class="line"><span class="comment"># Just return an output given a line</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(line_tensor)</span>:</span></div><div class="line">    hidden = rnn.initHidden()</div><div class="line"></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(line_tensor.size()[<span class="number">0</span>]):</div><div class="line">        output, hidden = rnn(line_tensor[i], hidden)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> output</div><div class="line"></div><div class="line"><span class="comment"># Go through a bunch of examples and record which are correctly guessed</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_confusion):</div><div class="line">    category, line, category_tensor, line_tensor = randomTrainingPair()</div><div class="line">    output = evaluate(line_tensor)</div><div class="line">    guess, guess_i = categoryFromOutput(output)</div><div class="line">    category_i = all_categories.index(category)</div><div class="line">    confusion[category_i][guess_i] += <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># Normalize by dividing every row by its sum</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_categories):</div><div class="line">    confusion[i] = confusion[i] / confusion[i].sum()</div><div class="line"></div><div class="line"><span class="comment"># Set up plot</span></div><div class="line">fig = plt.figure()</div><div class="line">ax = fig.add_subplot(<span class="number">111</span>)</div><div class="line">cax = ax.matshow(confusion.numpy())</div><div class="line">fig.colorbar(cax)</div><div class="line"></div><div class="line"><span class="comment"># Set up axes</span></div><div class="line">ax.set_xticklabels([<span class="string">''</span>] + all_categories, rotation=<span class="number">90</span>)</div><div class="line">ax.set_yticklabels([<span class="string">''</span>] + all_categories)</div><div class="line"></div><div class="line"><span class="comment"># Force label at every tick</span></div><div class="line">ax.xaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</div><div class="line">ax.yaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</div><div class="line"></div><div class="line"><span class="comment"># sphinx_gallery_thumbnail_number = 2</span></div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<image src="/image/NameRnn/group.png">

<p>你从非对角线上挑选一些亮点出来，它们表示语言预测不对，e.g.中文对应韩文，西班牙语对应意大利语。看起来希腊语预测比较好，英语预测的不好(可能英语和其它语言重叠笔记多)</p>
<h3 id="Running-on-User-Input-运行用户输入"><a href="#Running-on-User-Input-运行用户输入" class="headerlink" title="Running on User Input(运行用户输入)"></a>Running on User Input(运行用户输入)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(input_line, n_predictions=<span class="number">3</span>)</span>:</span></div><div class="line">    print(<span class="string">'\n&gt; %s'</span> % input_line)</div><div class="line">    output = evaluate(Variable(lineToTensor(input_line)))</div><div class="line"></div><div class="line">    <span class="comment"># Get top N categories</span></div><div class="line">    topv, topi = output.data.topk(n_predictions, <span class="number">1</span>, <span class="keyword">True</span>)</div><div class="line">    predictions = []</div><div class="line"></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n_predictions):</div><div class="line">        value = topv[<span class="number">0</span>][i]</div><div class="line">        category_index = topi[<span class="number">0</span>][i]</div><div class="line">        print(<span class="string">'(%.2f) %s'</span> % (value, all_categories[category_index]))</div><div class="line">        predictions.append([value, all_categories[category_index]])</div><div class="line"></div><div class="line">predict(<span class="string">'Dovesky'</span>)</div><div class="line">predict(<span class="string">'Jackson'</span>)</div><div class="line">predict(<span class="string">'Satoshi'</span>)</div></pre></td></tr></table></figure>
<p>Out:</p>
<blockquote>
<p>Dovesky<br>(-0.46) Russian<br>(-1.51) Czech<br>(-2.63) English</p>
<p>Jackson<br>(-0.83) Scottish<br>(-1.10) English<br>(-2.34) Russian</p>
<p>Satoshi<br>(-0.96) Italian<br>(-1.61) Arabic<br>(-1.90) Japanese</p>
</blockquote>
<p>最终的版本的脚本在<a href="">Practical PyTorch repo</a>拆分成了下面的几个文件</p>
<ul>
<li><code>data.py</code>(加载文件)</li>
<li><code>model.py</code>(定义RNN)</li>
<li><code>train.py</code>(进行训练)</li>
<li><code>predict.py</code>(运行<code>predict()</code>使用命令行)</li>
<li><code>server.py</code>（在JSON API使用bottel.py运行预测服务）</li>
</ul>
<p>运行<code>train.py</code>去训练和保存神经网络。<br>运行<code>predict.py</code>带名字可以看预测结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ python predict.py Hazaki</div><div class="line">(<span class="number">-0.42</span>) Japanese</div><div class="line">(<span class="number">-1.39</span>) Polish</div><div class="line">(<span class="number">-3.51</span>) Czech</div></pre></td></tr></table></figure>
<p>运行<code>server.py</code>然后访问<a href="http://localhost:5533/Yourname" target="_blank" rel="external">http://localhost:5533/Yourname</a>获取JSON输出的预测结果。</p>
</image>]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
            <category> translate </category>
            
        </categories>
        
        
        <tags>
            
            <tag> rnn </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Transfer Learning tutorial(翻译)]]></title>
      <url>/2017/05/21/TransLearning/</url>
      <content type="html"><![CDATA[<p>原文链接：<a href="http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html" target="_blank" rel="external">Transfer Learning tutorial</a><br>作者：<a href="https://chsasank.github.io" target="_blank" rel="external">Sasank Chilamkurthy</a><br>在这篇文章中，你将会学到如何通过迁移学习训练你的神经网络。你可以在<a href="http://cs231n.github.io/transfer-learning/" target="_blank" rel="external">cs231n notes</a>上了解更过关于迁移学习的内容。<br>引用这个笔记：</p>
<blockquote>
<p>在实践中，很少人会从零开始训练一个卷积神经网络(使用随机初始化)，因为拥有一个相当数量的训练数据是不常见的。作为替代，通常做法是对一个卷积神经网络在大数据上(e.g. ImageNet，包括了120万图片和1000分类)做预训练，然后使用这个卷积神经网络作为权值初始化使用或者任务关注的固定特征提取器。</p>
</blockquote>
<a id="more"></a>
<p>迁移学习包含了下面两个重点：   </p>
<ul>
<li><strong>卷积网络微调</strong>：不是随机的去初始化神经网络，我们使用那些就像在imagenet的1000个数据集上预训练的神经网络去初始化。剩下的训练过程和正常训练过程一致。   </li>
<li><strong>卷积神经网络作为固定特征提取器</strong>：这里，我们将除最后一个全连接层外的其它层权值参数都固定。将最后的全连接层替换为一个随机初始化权值后的层，也就只有这个层被训练。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># License: BSD</span></div><div class="line"><span class="comment"># Author: Sasank Chilamkurthy</span></div><div class="line"><span class="comment"># from __future__是在当前版本到处一些未来会定型的特性出来使用比如变成函数的print</span></div><div class="line"><span class="comment"># print_function 和 真实世界的除法 division 不会进行向下取整，如果需要向下取整</span></div><div class="line"><span class="comment"># 就使用//</span></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function, division</div><div class="line"></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> torchvision</div><div class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, models, transforms</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">import</span> copy</div><div class="line"><span class="keyword">import</span> os</div><div class="line"></div><div class="line">plt.ion()   <span class="comment"># interactive mode</span></div></pre></td></tr></table></figure>
<h2 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h2><p>我们会使用torchvision和torch.utils.data包来加载数据。<br>我们现在的任务是训练一个蚂蚁和蜜蜂的分类模型。我们对于每个蚂蚁和蜜蜂的分类各有120个训练图片。对应每个分类有75个验证图片。如果是从头开始训练的话，这点数据显得很少。因此我们使用迁移学习，这样可以达到一个好的效果。</p>
<p>这是imagenet的一非常小的数据集。<br><strong>Note</strong></p>
<blockquote>
<p>通过<a href="https://download.pytorch.org/tutorial/hymenoptera_data.zip" target="_blank" rel="external">这里</a>来下载数据并解压到当前文件。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Data augmentation and normalization for training</span></div><div class="line"><span class="comment"># Just normalization for validation</span></div><div class="line">data_transforms = &#123;</div><div class="line">    <span class="string">'train'</span>: transforms.Compose([</div><div class="line">        transforms.RandomSizedCrop(<span class="number">224</span>),</div><div class="line">        transforms.RandomHorizontalFlip(),</div><div class="line">        transforms.ToTensor(),</div><div class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</div><div class="line">    ]),</div><div class="line">    <span class="string">'val'</span>: transforms.Compose([</div><div class="line">        transforms.Scale(<span class="number">256</span>),</div><div class="line">        transforms.CenterCrop(<span class="number">224</span>),</div><div class="line">        transforms.ToTensor(),</div><div class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</div><div class="line">    ]),</div><div class="line">&#125;</div><div class="line"></div><div class="line">data_dir = <span class="string">'hymenoptera_data'</span></div><div class="line">dsets = &#123;x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])</div><div class="line">         <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</div><div class="line">dset_loaders = &#123;x: torch.utils.data.DataLoader(dsets[x], batch_size=<span class="number">4</span>,</div><div class="line">                                               shuffle=<span class="keyword">True</span>, num_workers=<span class="number">4</span>)</div><div class="line">                <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</div><div class="line">dset_sizes = &#123;x: len(dsets[x]) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</div><div class="line">dset_classes = dsets[<span class="string">'train'</span>].classes</div><div class="line"></div><div class="line">use_gpu = torch.cuda.is_available()</div></pre></td></tr></table></figure>
<h3 id="可视化一些图片"><a href="#可视化一些图片" class="headerlink" title="可视化一些图片"></a>可视化一些图片</h3><p>让我们可视化一些训练数据以便理解数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(inp, title=None)</span>:</span></div><div class="line">    <span class="string">"""Imshow for Tensor."""</span></div><div class="line">    inp = inp.numpy().transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</div><div class="line">    mean = np.array([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>])</div><div class="line">    std = np.array([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</div><div class="line">    inp = std * inp + mean</div><div class="line">    plt.imshow(inp)</div><div class="line">    <span class="keyword">if</span> title <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        plt.title(title)</div><div class="line">    plt.pause(<span class="number">0.001</span>)  <span class="comment"># pause a bit so that plots are updated</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># Get a batch of training data</span></div><div class="line">inputs, classes = next(iter(dset_loaders[<span class="string">'train'</span>]))</div><div class="line"></div><div class="line"><span class="comment"># Make a grid from batch</span></div><div class="line">out = torchvision.utils.make_grid(inputs)</div><div class="line"></div><div class="line">imshow(out, title=[dset_classes[x] <span class="keyword">for</span> x <span class="keyword">in</span> classes])</div></pre></td></tr></table></figure></p>
<p><img src="/image/transLearn/translenVisimage.png"></p>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>现在，让我们写一个通用的函数去训练模型。这里会做出说明：</p>
<ul>
<li>安排学习率</li>
<li>保存(深拷贝)最好的模型<br>接下来，参数<code>lr_scheduler(optimizer, epoch)</code>是一个用来修改optimizer的函数以便可以根据计划改变学习率。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model, criterion, optimizer, lr_scheduler, num_epochs=<span class="number">25</span>)</span>:</span></div><div class="line">    since = time.time()</div><div class="line"></div><div class="line">    best_model = model</div><div class="line">    best_acc = <span class="number">0.0</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</div><div class="line">        print(<span class="string">'Epoch &#123;&#125;/&#123;&#125;'</span>.format(epoch, num_epochs - <span class="number">1</span>))</div><div class="line">        print(<span class="string">'-'</span> * <span class="number">10</span>)</div><div class="line"></div><div class="line">        <span class="comment"># Each epoch has a training and validation phase</span></div><div class="line">        <span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]:</div><div class="line">            <span class="keyword">if</span> phase == <span class="string">'train'</span>:</div><div class="line">                optimizer = lr_scheduler(optimizer, epoch)</div><div class="line">                model.train(<span class="keyword">True</span>)  <span class="comment"># Set model to training mode</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                model.train(<span class="keyword">False</span>)  <span class="comment"># Set model to evaluate mode</span></div><div class="line"></div><div class="line">            running_loss = <span class="number">0.0</span></div><div class="line">            running_corrects = <span class="number">0</span></div><div class="line"></div><div class="line">            <span class="comment"># Iterate over data.</span></div><div class="line">            <span class="keyword">for</span> data <span class="keyword">in</span> dset_loaders[phase]:</div><div class="line">                <span class="comment"># get the inputs</span></div><div class="line">                inputs, labels = data</div><div class="line"></div><div class="line">                <span class="comment"># wrap them in Variable</span></div><div class="line">                <span class="keyword">if</span> use_gpu:</div><div class="line">                    inputs, labels = Variable(inputs.cuda()), \</div><div class="line">                        Variable(labels.cuda())</div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    inputs, labels = Variable(inputs), Variable(labels)</div><div class="line"></div><div class="line">                <span class="comment"># zero the parameter gradients</span></div><div class="line">                optimizer.zero_grad()</div><div class="line"></div><div class="line">                <span class="comment"># forward</span></div><div class="line">                outputs = model(inputs)</div><div class="line">                _, preds = torch.max(outputs.data, <span class="number">1</span>)</div><div class="line">                loss = criterion(outputs, labels)</div><div class="line"></div><div class="line">                <span class="comment"># backward + optimize only if in training phase</span></div><div class="line">                <span class="keyword">if</span> phase == <span class="string">'train'</span>:</div><div class="line">                    loss.backward()</div><div class="line">                    optimizer.step()</div><div class="line"></div><div class="line">                <span class="comment"># statistics</span></div><div class="line">                running_loss += loss.data[<span class="number">0</span>]</div><div class="line">                running_corrects += torch.sum(preds == labels.data)</div><div class="line"></div><div class="line">            epoch_loss = running_loss / dset_sizes[phase]</div><div class="line">            epoch_acc = running_corrects / dset_sizes[phase]</div><div class="line"></div><div class="line">            print(<span class="string">'&#123;&#125; Loss: &#123;:.4f&#125; Acc: &#123;:.4f&#125;'</span>.format(</div><div class="line">                phase, epoch_loss, epoch_acc))</div><div class="line"></div><div class="line">            <span class="comment"># deep copy the model</span></div><div class="line">            <span class="keyword">if</span> phase == <span class="string">'val'</span> <span class="keyword">and</span> epoch_acc &gt; best_acc:</div><div class="line">                best_acc = epoch_acc</div><div class="line">                best_model = copy.deepcopy(model)</div><div class="line"></div><div class="line">        print()</div><div class="line"></div><div class="line">    time_elapsed = time.time() - since</div><div class="line">    print(<span class="string">'Training complete in &#123;:.0f&#125;m &#123;:.0f&#125;s'</span>.format(</div><div class="line">        time_elapsed // <span class="number">60</span>, time_elapsed % <span class="number">60</span>))</div><div class="line">    print(<span class="string">'Best val Acc: &#123;:4f&#125;'</span>.format(best_acc))</div><div class="line">    <span class="keyword">return</span> best_model</div></pre></td></tr></table></figure>
<h3 id="学习率计划"><a href="#学习率计划" class="headerlink" title="学习率计划"></a>学习率计划</h3><p>让我们创建一个学习率计划，在每个外层循环步骤我们会呈几何的减小学习率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">exp_lr_scheduler</span><span class="params">(optimizer, epoch, init_lr=<span class="number">0.001</span>, lr_decay_epoch=<span class="number">7</span>)</span>:</span></div><div class="line">    <span class="string">"""Decay learning rate by a factor of 0.1 every lr_decay_epoch epochs."""</span></div><div class="line">    lr = init_lr * (<span class="number">0.1</span>**(epoch // lr_decay_epoch))</div><div class="line"></div><div class="line">    <span class="keyword">if</span> epoch % lr_decay_epoch == <span class="number">0</span>:</div><div class="line">        print(<span class="string">'LR is set to &#123;&#125;'</span>.format(lr))</div><div class="line"></div><div class="line">    <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</div><div class="line">        param_group[<span class="string">'lr'</span>] = lr</div><div class="line"></div><div class="line">    <span class="keyword">return</span> optimizer</div></pre></td></tr></table></figure>
<h3 id="可视化模型预测"><a href="#可视化模型预测" class="headerlink" title="可视化模型预测"></a>可视化模型预测</h3><p>一个通用的函数用来展示一些图片预测。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_model</span><span class="params">(model, num_images=<span class="number">6</span>)</span>:</span></div><div class="line">    images_so_far = <span class="number">0</span></div><div class="line">    fig = plt.figure()</div><div class="line"></div><div class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(dset_loaders[<span class="string">'val'</span>]):</div><div class="line">        inputs, labels = data</div><div class="line">        <span class="keyword">if</span> use_gpu:</div><div class="line">            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            inputs, labels = Variable(inputs), Variable(labels)</div><div class="line"></div><div class="line">        outputs = model(inputs)</div><div class="line">        _, preds = torch.max(outputs.data, <span class="number">1</span>)</div><div class="line"></div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(inputs.size()[<span class="number">0</span>]):</div><div class="line">            images_so_far += <span class="number">1</span></div><div class="line">            ax = plt.subplot(num_images//<span class="number">2</span>, <span class="number">2</span>, images_so_far)</div><div class="line">            ax.axis(<span class="string">'off'</span>)</div><div class="line">            ax.set_title(<span class="string">'predicted: &#123;&#125;'</span>.format(dset_classes[labels.data[j]]))</div><div class="line">            imshow(inputs.cpu().data[j])</div><div class="line"></div><div class="line">            <span class="keyword">if</span> images_so_far == num_images:</div><div class="line">                <span class="keyword">return</span></div></pre></td></tr></table></figure></p>
<h2 id="卷积网络微调"><a href="#卷积网络微调" class="headerlink" title="卷积网络微调"></a>卷积网络微调</h2><p>加载预训练的模型然后重置最后一层全连接层。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">model_ft = models.resnet18(pretrained=<span class="keyword">True</span>)</div><div class="line">num_ftrs = model_ft.fc.in_features</div><div class="line">model_ft.fc = nn.Linear(num_ftrs, <span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">if</span> use_gpu:</div><div class="line">    model_ft = model_ft.cuda()</div><div class="line"></div><div class="line">criterion = nn.CrossEntropyLoss()</div><div class="line"></div><div class="line"><span class="comment"># Observe that all parameters are being optimized</span></div><div class="line">optimizer_ft = optim.SGD(model_ft.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</div></pre></td></tr></table></figure></p>
<h3 id="训练和评判"><a href="#训练和评判" class="headerlink" title="训练和评判"></a>训练和评判</h3><p>在CPU上会花费15-25分钟。在GPU上的花费少于1分钟。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25</div></pre></td></tr></table></figure></p>
<p>OUT：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div></pre></td><td class="code"><pre><div class="line">Epoch 0/24</div><div class="line">----------</div><div class="line">LR is set to 0.001</div><div class="line">train Loss: 0.1565 Acc: 0.6844</div><div class="line">val Loss: 0.1242 Acc: 0.7908</div><div class="line"></div><div class="line">Epoch 1/24</div><div class="line">----------</div><div class="line">train Loss: 0.1269 Acc: 0.7582</div><div class="line">val Loss: 0.0473 Acc: 0.9150</div><div class="line"></div><div class="line">Epoch 2/24</div><div class="line">----------</div><div class="line">train Loss: 0.1062 Acc: 0.8156</div><div class="line">val Loss: 0.0714 Acc: 0.8562</div><div class="line"></div><div class="line">Epoch 3/24</div><div class="line">----------</div><div class="line">train Loss: 0.0993 Acc: 0.8156</div><div class="line">val Loss: 0.0566 Acc: 0.9085</div><div class="line"></div><div class="line">Epoch 4/24</div><div class="line">----------</div><div class="line">train Loss: 0.0962 Acc: 0.8730</div><div class="line">val Loss: 0.2831 Acc: 0.6863</div><div class="line"></div><div class="line">Epoch 5/24</div><div class="line">----------</div><div class="line">train Loss: 0.1778 Acc: 0.7787</div><div class="line">val Loss: 0.4696 Acc: 0.6667</div><div class="line"></div><div class="line">Epoch 6/24</div><div class="line">----------</div><div class="line">train Loss: 0.1765 Acc: 0.8238</div><div class="line">val Loss: 0.0994 Acc: 0.8758</div><div class="line"></div><div class="line">Epoch 7/24</div><div class="line">----------</div><div class="line">LR is set to 0.0001</div><div class="line">train Loss: 0.1600 Acc: 0.8156</div><div class="line">val Loss: 0.0850 Acc: 0.8824</div><div class="line"></div><div class="line">Epoch 8/24</div><div class="line">----------</div><div class="line">train Loss: 0.0876 Acc: 0.8730</div><div class="line">val Loss: 0.0841 Acc: 0.8889</div><div class="line"></div><div class="line">Epoch 9/24</div><div class="line">----------</div><div class="line">train Loss: 0.1105 Acc: 0.8279</div><div class="line">val Loss: 0.0777 Acc: 0.8954</div><div class="line"></div><div class="line">Epoch 10/24</div><div class="line">----------</div><div class="line">train Loss: 0.0976 Acc: 0.8607</div><div class="line">val Loss: 0.0719 Acc: 0.9020</div><div class="line"></div><div class="line">Epoch 11/24</div><div class="line">----------</div><div class="line">train Loss: 0.0644 Acc: 0.9057</div><div class="line">val Loss: 0.0637 Acc: 0.9085</div><div class="line"></div><div class="line">Epoch 12/24</div><div class="line">----------</div><div class="line">train Loss: 0.1003 Acc: 0.8279</div><div class="line">val Loss: 0.0678 Acc: 0.9085</div><div class="line"></div><div class="line">Epoch 13/24</div><div class="line">----------</div><div class="line">train Loss: 0.0755 Acc: 0.8770</div><div class="line">val Loss: 0.0576 Acc: 0.9150</div><div class="line"></div><div class="line">Epoch 14/24</div><div class="line">----------</div><div class="line">LR is set to 1.0000000000000003e-05</div><div class="line">train Loss: 0.0642 Acc: 0.9139</div><div class="line">val Loss: 0.0608 Acc: 0.9150</div><div class="line"></div><div class="line">Epoch 15/24</div><div class="line">----------</div><div class="line">train Loss: 0.0870 Acc: 0.8484</div><div class="line">val Loss: 0.0618 Acc: 0.9085</div><div class="line"></div><div class="line">Epoch 16/24</div><div class="line">----------</div><div class="line">train Loss: 0.0704 Acc: 0.8975</div><div class="line">val Loss: 0.0629 Acc: 0.9085</div><div class="line"></div><div class="line">Epoch 17/24</div><div class="line">----------</div><div class="line">train Loss: 0.0714 Acc: 0.8730</div><div class="line">val Loss: 0.0627 Acc: 0.9150</div><div class="line"></div><div class="line">Epoch 18/24</div><div class="line">----------</div><div class="line">train Loss: 0.0739 Acc: 0.8730</div><div class="line">val Loss: 0.0637 Acc: 0.9020</div><div class="line"></div><div class="line">Epoch 19/24</div><div class="line">----------</div><div class="line">train Loss: 0.0865 Acc: 0.8320</div><div class="line">val Loss: 0.0701 Acc: 0.8954</div><div class="line"></div><div class="line">Epoch 20/24</div><div class="line">----------</div><div class="line">train Loss: 0.0868 Acc: 0.8566</div><div class="line">val Loss: 0.0654 Acc: 0.9216</div><div class="line"></div><div class="line">Epoch 21/24</div><div class="line">----------</div><div class="line">LR is set to 1.0000000000000002e-06</div><div class="line">train Loss: 0.0868 Acc: 0.8525</div><div class="line">val Loss: 0.0619 Acc: 0.9216</div><div class="line"></div><div class="line">Epoch 22/24</div><div class="line">----------</div><div class="line">train Loss: 0.0867 Acc: 0.8730</div><div class="line">val Loss: 0.0742 Acc: 0.8824</div><div class="line"></div><div class="line">Epoch 23/24</div><div class="line">----------</div><div class="line">train Loss: 0.0860 Acc: 0.8566</div><div class="line">val Loss: 0.0603 Acc: 0.9150</div><div class="line"></div><div class="line">Epoch 24/24</div><div class="line">----------</div><div class="line">train Loss: 0.1004 Acc: 0.8402</div><div class="line">val Loss: 0.0652 Acc: 0.9085</div><div class="line"></div><div class="line">Training complete in 1m 8s</div><div class="line">Best val Acc: 0.921569</div></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">visualize_model(model_ft)</div></pre></td></tr></table></figure>
<p><img src="/image/transLearn/translenPre.png"></p>
<h2 id="卷积神经网络作为固定特征提取器"><a href="#卷积神经网络作为固定特征提取器" class="headerlink" title="卷积神经网络作为固定特征提取器"></a>卷积神经网络作为固定特征提取器</h2><p>这里，我们需要固定除开最后一个全连接层以外的其它层。我们需要设置<code>requires_grad == False</code>去固定参数这样在进行<code>backward()</code>时他们不会被计算更新。</p>
<p>你可以阅读这里更详细的<a href="http://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-backward" target="_blank" rel="external">文档</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">model_conv = torchvision.models.resnet18(pretrained=<span class="keyword">True</span>)</div><div class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model_conv.parameters():</div><div class="line">    param.requires_grad = <span class="keyword">False</span></div><div class="line"></div><div class="line"><span class="comment"># Parameters of newly constructed modules have requires_grad=True by default</span></div><div class="line">num_ftrs = model_conv.fc.in_features</div><div class="line">model_conv.fc = nn.Linear(num_ftrs, <span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">if</span> use_gpu:</div><div class="line">    model_conv = model_conv.cuda()</div><div class="line"></div><div class="line">criterion = nn.CrossEntropyLoss()</div><div class="line"></div><div class="line"><span class="comment"># Observe that only parameters of final layer are being optimized as</span></div><div class="line"><span class="comment"># opoosed to before.</span></div><div class="line">optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</div></pre></td></tr></table></figure></p>
<h3 id="训练和评判-1"><a href="#训练和评判-1" class="headerlink" title="训练和评判"></a>训练和评判</h3><p>在CPU上会花费上前一个例子一半的时间。这里神经网络的大部分梯度都不需要计算。当然前向还是需要计算。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">model_conv = train_model(model_conv, criterion, optimizer_conv,</div><div class="line">                         exp_lr_scheduler, num_epochs=<span class="number">25</span>)</div></pre></td></tr></table></figure></p>
<p>OUT：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div></pre></td><td class="code"><pre><div class="line">Epoch 0/24</div><div class="line">----------</div><div class="line">LR is set to 0.001</div><div class="line">train Loss: 0.1731 Acc: 0.6025</div><div class="line">val Loss: 0.0868 Acc: 0.8627</div><div class="line"></div><div class="line">Epoch 1/24</div><div class="line">----------</div><div class="line">train Loss: 0.1219 Acc: 0.7336</div><div class="line">val Loss: 0.0470 Acc: 0.9542</div><div class="line"></div><div class="line">Epoch 2/24</div><div class="line">----------</div><div class="line">train Loss: 0.1205 Acc: 0.8033</div><div class="line">val Loss: 0.0505 Acc: 0.9412</div><div class="line"></div><div class="line">Epoch 3/24</div><div class="line">----------</div><div class="line">train Loss: 0.0967 Acc: 0.8156</div><div class="line">val Loss: 0.0736 Acc: 0.8954</div><div class="line"></div><div class="line">Epoch 4/24</div><div class="line">----------</div><div class="line">train Loss: 0.1452 Acc: 0.7664</div><div class="line">val Loss: 0.0447 Acc: 0.9346</div><div class="line"></div><div class="line">Epoch 5/24</div><div class="line">----------</div><div class="line">train Loss: 0.1170 Acc: 0.7992</div><div class="line">val Loss: 0.0791 Acc: 0.8758</div><div class="line"></div><div class="line">Epoch 6/24</div><div class="line">----------</div><div class="line">train Loss: 0.1260 Acc: 0.7787</div><div class="line">val Loss: 0.0551 Acc: 0.9346</div><div class="line"></div><div class="line">Epoch 7/24</div><div class="line">----------</div><div class="line">LR is set to 0.0001</div><div class="line">train Loss: 0.0580 Acc: 0.9016</div><div class="line">val Loss: 0.0473 Acc: 0.9412</div><div class="line"></div><div class="line">Epoch 8/24</div><div class="line">----------</div><div class="line">train Loss: 0.0950 Acc: 0.8115</div><div class="line">val Loss: 0.0579 Acc: 0.9216</div><div class="line"></div><div class="line">Epoch 9/24</div><div class="line">----------</div><div class="line">train Loss: 0.0868 Acc: 0.8689</div><div class="line">val Loss: 0.0500 Acc: 0.9477</div><div class="line"></div><div class="line">Epoch 10/24</div><div class="line">----------</div><div class="line">train Loss: 0.1027 Acc: 0.8033</div><div class="line">val Loss: 0.0476 Acc: 0.9412</div><div class="line"></div><div class="line">Epoch 11/24</div><div class="line">----------</div><div class="line">train Loss: 0.0911 Acc: 0.8443</div><div class="line">val Loss: 0.0500 Acc: 0.9477</div><div class="line"></div><div class="line">Epoch 12/24</div><div class="line">----------</div><div class="line">train Loss: 0.0968 Acc: 0.8361</div><div class="line">val Loss: 0.0520 Acc: 0.9412</div><div class="line"></div><div class="line">Epoch 13/24</div><div class="line">----------</div><div class="line">train Loss: 0.0930 Acc: 0.8361</div><div class="line">val Loss: 0.0501 Acc: 0.9412</div><div class="line"></div><div class="line">Epoch 14/24</div><div class="line">----------</div><div class="line">LR is set to 1.0000000000000003e-05</div><div class="line">train Loss: 0.1072 Acc: 0.7992</div><div class="line">val Loss: 0.0631 Acc: 0.9150</div><div class="line"></div><div class="line">Epoch 15/24</div><div class="line">----------</div><div class="line">train Loss: 0.1021 Acc: 0.8197</div><div class="line">val Loss: 0.0458 Acc: 0.9477</div><div class="line"></div><div class="line">Epoch 16/24</div><div class="line">----------</div><div class="line">train Loss: 0.1014 Acc: 0.8279</div><div class="line">val Loss: 0.0467 Acc: 0.9477</div><div class="line"></div><div class="line">Epoch 17/24</div><div class="line">----------</div><div class="line">train Loss: 0.0645 Acc: 0.8934</div><div class="line">val Loss: 0.0517 Acc: 0.9477</div><div class="line"></div><div class="line">Epoch 18/24</div><div class="line">----------</div><div class="line">train Loss: 0.0704 Acc: 0.8811</div><div class="line">val Loss: 0.0511 Acc: 0.9346</div><div class="line"></div><div class="line">Epoch 19/24</div><div class="line">----------</div><div class="line">train Loss: 0.0821 Acc: 0.8648</div><div class="line">val Loss: 0.0469 Acc: 0.9412</div><div class="line"></div><div class="line">Epoch 20/24</div><div class="line">----------</div><div class="line">train Loss: 0.0838 Acc: 0.8730</div><div class="line">val Loss: 0.0431 Acc: 0.9477</div><div class="line"></div><div class="line">Epoch 21/24</div><div class="line">----------</div><div class="line">LR is set to 1.0000000000000002e-06</div><div class="line">train Loss: 0.0964 Acc: 0.8443</div><div class="line">val Loss: 0.0433 Acc: 0.9477</div><div class="line"></div><div class="line">Epoch 22/24</div><div class="line">----------</div><div class="line">train Loss: 0.0823 Acc: 0.8525</div><div class="line">val Loss: 0.0462 Acc: 0.9477</div><div class="line"></div><div class="line">Epoch 23/24</div><div class="line">----------</div><div class="line">train Loss: 0.0769 Acc: 0.8525</div><div class="line">val Loss: 0.0452 Acc: 0.9542</div><div class="line"></div><div class="line">Epoch 24/24</div><div class="line">----------</div><div class="line">train Loss: 0.0953 Acc: 0.8320</div><div class="line">val Loss: 0.0594 Acc: 0.9281</div><div class="line"></div><div class="line">Training complete in 0m 35s</div><div class="line">Best val Acc: 0.954248</div></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">visualize_model(model_conv)</div><div class="line"></div><div class="line">plt.ioff()</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><image src="/image/transLearn/translenFeat.png"><br>整个训练脚本花费时间:(1分 48.038秒)</image></p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
            <category> translate </category>
            
        </categories>
        
        
        <tags>
            
            <tag> transfer learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Learning PyTorch with Example(翻译)]]></title>
      <url>/2017/05/20/pytorch1/</url>
      <content type="html"><![CDATA[<p>原文链接：<a href="http://pytorch.org/tutorials/beginner/pytorch_with_examples.html" target="_blank" rel="external">Learning PyTorch with Examples</a><br>作者：<a href="https://github.com/jcjohnson/pytorch-examples" target="_blank" rel="external">Justin Johnos</a></p>
<p>这份学习指导通过下面的例子介绍<a href="https://github.com/pytorch/pytorch" target="_blank" rel="external">PyTorch</a>的一些基本概念。<br>PyTorch提供了以下两个核心的特性：   </p>
<ul>
<li>一个和numpy相似的多维Tensor(张量)，但可在GPU上执行</li>
<li>对构建的神经网络进行自动求导<a id="more"></a>
</li>
</ul>
<p>我们讲解的例子为激活函数是ReLU的全连接神经网络。这个神经网络将只包含一个隐藏层，使用欧式距离作为输出值和实际值度量，我们将使用梯度下降去最小化欧式距离代价函数。</p>
<h2 id="Tensor-张量"><a href="#Tensor-张量" class="headerlink" title="Tensor(张量)"></a>Tensor(张量)</h2><h3 id="热身-numpy"><a href="#热身-numpy" class="headerlink" title="热身:numpy"></a>热身:numpy</h3><p>在正式介绍PyTorch前，我们先通过numpy来实现这个神经网络。<br>Numpy提供了一个n维数组对象，并提供了许多函数来操作这个数组。Numpy是一个通用科学运算框架。它本身对计算图，深度学习和梯度是一无所知。然而我们可以轻松的通过随机数据去训练一个使用numpy实现了前向和后向传播的神经网络。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random input and output data</span></div><div class="line">x = np.random.randn(N, D_in)</div><div class="line">y = np.random.randn(N, D_out)</div><div class="line"></div><div class="line"><span class="comment"># Randomly initialize weights</span></div><div class="line">w1 = np.random.randn(D_in, H)</div><div class="line">w2 = np.random.randn(H, D_out)</div><div class="line"></div><div class="line">learning_rate = <span class="number">1e-6</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: compute predicted y</span></div><div class="line">    h = x.dot(w1)</div><div class="line">    h_relu = np.maximum(h, <span class="number">0</span>)</div><div class="line">    y_pred = h_relu.dot(w2)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss</span></div><div class="line">    loss = np.square(y_pred - y).sum()</div><div class="line">    print(t, loss)</div><div class="line"></div><div class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></div><div class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</div><div class="line">    grad_w2 = h_relu.T.dot(grad_y_pred)</div><div class="line">    grad_h_relu = grad_y_pred.dot(w2.T)</div><div class="line">    grad_h = grad_h_relu.copy()</div><div class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></div><div class="line">    grad_w1 = x.T.dot(grad_h)</div><div class="line"></div><div class="line">    <span class="comment"># Update weights</span></div><div class="line">    w1 -= learning_rate * grad_w1</div><div class="line">    w2 -= learning_rate * grad_w2</div></pre></td></tr></table></figure></p>
<h3 id="PyTorch-Tensor"><a href="#PyTorch-Tensor" class="headerlink" title="PyTorch:Tensor"></a>PyTorch:Tensor</h3><p>Numpy是一个伟大的框架，但是它并不能利用GPUs来对数学计算加速。对于现代的深度神经网络，GPUs通常可以提供50倍甚至更多的提速，因此很可惜numpy不能满足现代深度学习的需求。</p>
<p>接下来我们介绍一个最重要的PyTorch概念:<strong>Tensor(张量)</strong>。在概念上PyTorch的Tensor和numpy的array相似：Tensor是一个n维数组，PyTorch提供了很多函数去操作它。和numpy的array一样，PyTorch Tensor同样对深度学习或者计算图或者梯度等概念一无所知；它同样是一个通用的科学运算工具。</p>
<p>然而不似numpy，PyTorch Tensor 能利用GPUs去加速他们的数学运算。为了可以在GPU上运行PyTorch Tensor，你只需要将它简单的转换为一个新的数据类型。</p>
<p>下面我们使用Pytorch Tensor去训练一个两层的神经网络。像上面的numpy的例子我们需要手动实现神经网络的前向和后向传播。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"></div><div class="line"></div><div class="line">dtype = torch.FloatTensor</div><div class="line"><span class="comment"># 取消下面的注释，程序运行在GPU上</span></div><div class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random input and output data</span></div><div class="line">x = torch.randn(N, D_in).type(dtype)</div><div class="line">y = torch.randn(N, D_out).type(dtype)</div><div class="line"></div><div class="line"><span class="comment"># Randomly initialize weights</span></div><div class="line">w1 = torch.randn(D_in, H).type(dtype)</div><div class="line">w2 = torch.randn(H, D_out).type(dtype)</div><div class="line"></div><div class="line">learning_rate = <span class="number">1e-6</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: compute predicted y</span></div><div class="line">    h = x.mm(w1)</div><div class="line">    h_relu = h.clamp(min=<span class="number">0</span>)</div><div class="line">    y_pred = h_relu.mm(w2)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss</span></div><div class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</div><div class="line">    print(t, loss)</div><div class="line"></div><div class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></div><div class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</div><div class="line">    grad_w2 = h_relu.t().mm(grad_y_pred)</div><div class="line">    grad_h_relu = grad_y_pred.mm(w2.t())</div><div class="line">    grad_h = grad_h_relu.clone()</div><div class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></div><div class="line">    grad_w1 = x.t().mm(grad_h)</div><div class="line"></div><div class="line">    <span class="comment"># Update weights using gradient descent</span></div><div class="line">    w1 -= learning_rate * grad_w1</div><div class="line">    w2 -= learning_rate * grad_w2</div></pre></td></tr></table></figure>
<h2 id="Autograd-自动求导"><a href="#Autograd-自动求导" class="headerlink" title="Autograd(自动求导)"></a>Autograd(自动求导)</h2><h3 id="PyTorch-Variables-and-autograd"><a href="#PyTorch-Variables-and-autograd" class="headerlink" title="PyTorch:Variables and autograd"></a>PyTorch:Variables and autograd</h3><p>在上面的例子中，我们不得不实现神经网络的前向和后向传播。手动实现后向传播对于一个简单的两层神经网络并不是一个困难的事情，但对于一个大型复杂的神经网络将变的非常困难。</p>
<p>幸运的是，我们可以使用<a href="https://en.wikipedia.org/wiki/Automatic_differentiation" target="_blank" rel="external">自动求导</a>来自动的进行神经网络的反向传播过程。在PyTorch中的<strong>autograd</strong>包提供了此功能。当使用自动求导时，神经网络的前向传播将会定义<strong>计算图</strong>；在计算图中的节点就是Tensor，然后连线就是根据输入Tensor产生输出Tensor的函数。反向传播过程通过这个计算图可以很容易计算梯度。</p>
<p>这听起来似乎很复杂，但在练习中使用起来却很简单。我们将PyTorch的Tensor包含在<strong>Variable</strong>对象中；Variable在计算图中实际表示一个节点。如果x是一个Variable那么x.data是Tensor，x.grad是另外一个保存了x的标量梯度值的Variable。</p>
<p>PyTorch Variable拥有和PyTorch Tensor相同的API：(大部分)任何你在Tensor上可以做的操作同样可以在Variable生效；不同点在于，使用Variable可以定义计算图，允许你自动的计算梯度。</p>
<p>下面是我们使用PyTorch Variable和自动梯度计算实现的两层神经网络：现在我们不再需要手动实现神经网络的反向传播。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line">dtype = torch.FloatTensor</div><div class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold input and outputs, and wrap them in Variables.</span></div><div class="line"><span class="comment"># Setting requires_grad=False indicates that we do not need to compute gradients</span></div><div class="line"><span class="comment"># with respect to these Variables during the backward pass.</span></div><div class="line">x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=<span class="keyword">False</span>)</div><div class="line">y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors for weights, and wrap them in Variables.</span></div><div class="line"><span class="comment"># Setting requires_grad=True indicates that we want to compute gradients with</span></div><div class="line"><span class="comment"># respect to these Variables during the backward pass.</span></div><div class="line"><span class="comment"># 需要计算梯度的Variable需要将requires_grad置为True</span></div><div class="line">w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=<span class="keyword">True</span>)</div><div class="line">w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">learning_rate = <span class="number">1e-6</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: compute predicted y using operations on Variables; these</span></div><div class="line">    <span class="comment"># are exactly the same operations we used to compute the forward pass using</span></div><div class="line">    <span class="comment"># Tensors, but we do not need to keep references to intermediate values since</span></div><div class="line">    <span class="comment"># we are not implementing the backward pass by hand.</span></div><div class="line">    <span class="comment"># mm表示矩阵乘法</span></div><div class="line">    y_pred = x.mm(w1).clamp(min=<span class="number">0</span>).mm(w2)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss using operations on Variables.</span></div><div class="line">    <span class="comment"># Now loss is a Variable of shape (1,) and loss.data is a Tensor of shape</span></div><div class="line">    <span class="comment"># (1,); loss.data[0] is a scalar value holding the loss.</span></div><div class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Use autograd to compute the backward pass. This call will compute the</span></div><div class="line">    <span class="comment"># gradient of loss with respect to all Variables with requires_grad=True.</span></div><div class="line">    <span class="comment"># After this call w1.grad and w2.grad will be Variables holding the gradient</span></div><div class="line">    <span class="comment"># of the loss with respect to w1 and w2 respectively.</span></div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    <span class="comment"># Update weights using gradient descent; w1.data and w2.data are Tensors,</span></div><div class="line">    <span class="comment"># w1.grad and w2.grad are Variables and w1.grad.data and w2.grad.data are</span></div><div class="line">    <span class="comment"># Tensors.</span></div><div class="line">    w1.data -= learning_rate * w1.grad.data</div><div class="line">    w2.data -= learning_rate * w2.grad.data</div><div class="line"></div><div class="line">    <span class="comment"># Manually zero the gradients after updating weights</span></div><div class="line">    w1.grad.data.zero_()</div><div class="line">    w2.grad.data.zero_()</div></pre></td></tr></table></figure>
<h3 id="PyTorch-定义一个新的自动梯度计算函数"><a href="#PyTorch-定义一个新的自动梯度计算函数" class="headerlink" title="PyTorch:定义一个新的自动梯度计算函数"></a>PyTorch:定义一个新的自动梯度计算函数</h3><p>掩盖在自动计算梯度下，实际都包含着两个关于Tensor的基本函数操作。一是<strong>前向</strong>函数通过输入的Tensor计算输出Tensor，一是<strong>反向</strong>函数接受输出Tensor的标量梯度，然后根据这些值计算输入Tensor的梯度。</p>
<p>在PyTorch中我们可以轻易的定义我们自己的自动计算梯度操作，通过重定义<strong>torch.autograd.Fuction</strong>然后实现<strong>forward</strong>和<strong>backward</strong>函数。然后我们可以构造一个实例，然后像函数一样去调用它，通过Variable保存输入数据。</p>
<p>在下面的例子中我们自定义了一个ReLu的自动梯度计算函数，使用它来实现我们的两层神经网络：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyReLU</span><span class="params">(torch.autograd.Function)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    We can implement our own custom autograd Functions by subclassing</div><div class="line">    torch.autograd.Function and implementing the forward and backward passes</div><div class="line">    which operate on Tensors.</div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        In the forward pass we receive a Tensor containing the input and return a</div><div class="line">        Tensor containing the output. You can cache arbitrary Tensors for use in the</div><div class="line">        backward pass using the save_for_backward method.</div><div class="line">        """</div><div class="line">        self.save_for_backward(input)</div><div class="line">        <span class="keyword">return</span> input.clamp(min=<span class="number">0</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad_output)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        In the backward pass we receive a Tensor containing the gradient of the loss</div><div class="line">        with respect to the output, and we need to compute the gradient of the loss</div><div class="line">        with respect to the input.</div><div class="line">        """</div><div class="line">        <span class="comment"># 这里说明下ReLU在进行反向传播的时候对于是0的部分是没有梯度的。</span></div><div class="line">        input, = self.saved_tensors</div><div class="line">        grad_input = grad_output.clone()</div><div class="line">        grad_input[input &lt; <span class="number">0</span>] = <span class="number">0</span></div><div class="line">        <span class="keyword">return</span> grad_input</div><div class="line"></div><div class="line"></div><div class="line">dtype = torch.FloatTensor</div><div class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold input and outputs, and wrap them in Variables.</span></div><div class="line">x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=<span class="keyword">False</span>)</div><div class="line">y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors for weights, and wrap them in Variables.</span></div><div class="line">w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=<span class="keyword">True</span>)</div><div class="line">w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">learning_rate = <span class="number">1e-6</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Construct an instance of our MyReLU class to use in our network</span></div><div class="line">    relu = MyReLU()</div><div class="line"></div><div class="line">    <span class="comment"># Forward pass: compute predicted y using operations on Variables; we compute</span></div><div class="line">    <span class="comment"># ReLU using our custom autograd operation.</span></div><div class="line">    y_pred = relu(x.mm(w1)).mm(w2)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss</span></div><div class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Use autograd to compute the backward pass.</span></div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    <span class="comment"># Update weights using gradient descent</span></div><div class="line">    w1.data -= learning_rate * w1.grad.data</div><div class="line">    w2.data -= learning_rate * w2.grad.data</div><div class="line"></div><div class="line">    <span class="comment"># Manually zero the gradients after updating weights</span></div><div class="line">    w1.grad.data.zero_()</div><div class="line">    w2.grad.data.zero_()</div></pre></td></tr></table></figure>
<h3 id="TensorFlow-Static-Graphs"><a href="#TensorFlow-Static-Graphs" class="headerlink" title="TensorFlow:Static Graphs"></a>TensorFlow:Static Graphs</h3><p>PyTorch的autograd看起来和TensorFlow很相似：在这两个框架我们都定义计算图，然后使用自动微分技术计算梯度。两者计算图最大的区别是TenorFlow的计算图是<strong>静态</strong>的PyTorch是<strong>动图</strong>的计算图。</p>
<p>在TensorFlow中，我们定义好计算图后可以一边又一边的执行计算图，输入不同的输入数据给它。在PyTorch中，每个前向操作定义一个新的计算图。</p>
<p>静态图优势在于你可以先期进行图的优化；举个例讲一个框架可以决定有效的融合一些图操作，或者提出跨机器或者GPUs的分布式计算图策略。如果你一次次的重复使用一个计算图，那么这个潜在的前期图优化代价，将分散到每次的重复上(这段翻译的有点问题)。</p>
<p>一方面静态和动态的计算图结构差别还在控制流上。在一些模型中我们希望对每个数据点做梯度计算;比如在循环神经网络中一个数据点我们会按照时间步骤来展开；这些展开可以作为循环来计算。静态图需要将这个循环结构变成计算图的一部分；因此TensorFlow提供来一个<strong>tf.scan</strong>操作来将循环结构嵌入到计算图中。在这个情况下使用动态图处理上会更简单：因为我们是在运行时创建计算图，我们使用一般必要的流控方式来计算不同输入数据。</p>
<p>同上面的PyTorch自动梯度计算例子相比，下面是我们使用TensorFlow来实现一个简单的两层网络：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># First we set up the computational graph:</span></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create placeholders for the input and target data; these will be filled</span></div><div class="line"><span class="comment"># with real data when we execute the graph.</span></div><div class="line">x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, D_in))</div><div class="line">y = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, D_out))</div><div class="line"></div><div class="line"><span class="comment"># Create Variables for the weights and initialize them with random data.</span></div><div class="line"><span class="comment"># A TensorFlow Variable persists its value across executions of the graph.</span></div><div class="line">w1 = tf.Variable(tf.random_normal((D_in, H)))</div><div class="line">w2 = tf.Variable(tf.random_normal((H, D_out)))</div><div class="line"></div><div class="line"><span class="comment"># Forward pass: Compute the predicted y using operations on TensorFlow Tensors.</span></div><div class="line"><span class="comment"># Note that this code does not actually perform any numeric operations; it</span></div><div class="line"><span class="comment"># merely sets up the computational graph that we will later execute.</span></div><div class="line">h = tf.matmul(x, w1)</div><div class="line">h_relu = tf.maximum(h, tf.zeros(<span class="number">1</span>))</div><div class="line">y_pred = tf.matmul(h_relu, w2)</div><div class="line"></div><div class="line"><span class="comment"># Compute loss using operations on TensorFlow Tensors</span></div><div class="line">loss = tf.reduce_sum((y - y_pred) ** <span class="number">2.0</span>)</div><div class="line"></div><div class="line"><span class="comment"># Compute gradient of the loss with respect to w1 and w2.</span></div><div class="line">grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])</div><div class="line"></div><div class="line"><span class="comment"># Update the weights using gradient descent. To actually update the weights</span></div><div class="line"><span class="comment"># we need to evaluate new_w1 and new_w2 when executing the graph. Note that</span></div><div class="line"><span class="comment"># in TensorFlow the the act of updating the value of the weights is part of</span></div><div class="line"><span class="comment"># the computational graph; in PyTorch this happens outside the computational</span></div><div class="line"><span class="comment"># graph.</span></div><div class="line">learning_rate = <span class="number">1e-6</span></div><div class="line">new_w1 = w1.assign(w1 - learning_rate * grad_w1)</div><div class="line">new_w2 = w2.assign(w2 - learning_rate * grad_w2)</div><div class="line"></div><div class="line"><span class="comment"># Now we have built our computational graph, so we enter a TensorFlow session to</span></div><div class="line"><span class="comment"># actually execute the graph.</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    <span class="comment"># Run the graph once to initialize the Variables w1 and w2.</span></div><div class="line">    sess.run(tf.global_variables_initializer())</div><div class="line"></div><div class="line">    <span class="comment"># Create numpy arrays holding the actual data for the inputs x and targets</span></div><div class="line">    <span class="comment"># y</span></div><div class="line">    x_value = np.random.randn(N, D_in)</div><div class="line">    y_value = np.random.randn(N, D_out)</div><div class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">        <span class="comment"># Execute the graph many times. Each time it executes we want to bind</span></div><div class="line">        <span class="comment"># x_value to x and y_value to y, specified with the feed_dict argument.</span></div><div class="line">        <span class="comment"># Each time we execute the graph we want to compute the values for loss,</span></div><div class="line">        <span class="comment"># new_w1, and new_w2; the values of these Tensors are returned as numpy</span></div><div class="line">        <span class="comment"># arrays.</span></div><div class="line">        loss_value, _, _ = sess.run([loss, new_w1, new_w2],</div><div class="line">                                    feed_dict=&#123;x: x_value, y: y_value&#125;)</div><div class="line">        print(loss_value)</div></pre></td></tr></table></figure></p>
<h2 id="nn-module-神经网络模块"><a href="#nn-module-神经网络模块" class="headerlink" title="nn module(神经网络模块)"></a>nn module(神经网络模块)</h2><h3 id="PyTorch-nn"><a href="#PyTorch-nn" class="headerlink" title="PyTorch:nn"></a>PyTorch:nn</h3><p>计算图和自动梯度计算是非常有用的范式去定义复杂的操作和自动的执行导数计算；然而对于大型神经网络来说这样未加处理的自动梯度计算就显的太低层了。</p>
<p>当构建一个神经网络时，我们经常考虑在<strong>layer(层)</strong>中安排计算，这些层中有些包含了需要在学习过程中需要进行优化的<strong>leanable parameters(可学习参数)</strong>。</p>
<p>在TensorFlow中，提供了像<strong>Keras</strong>,<strong>TensorFlow-Slim</strong>,和<strong>TFLearn</strong>包他们对原始计算图方式进行了高层抽象，对于神经网络的构建非常有用。</p>
<p>在PyTorch，nn包为了相同的目的诞生。nn包定义了一个<strong>Modules</strong>集，它门大致等于神经网络的层。一个Module接受输入的Variable然后计算输出Variable，但也可以保存中间状态就像Variable包含可学习参数。nn包也定义了在训练神经网络时常用的损失函数集。</p>
<p>在这个例子中我们使用nn包去实现我们的两层神经网络：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables.</span></div><div class="line">x = Variable(torch.randn(N, D_in))</div><div class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Use the nn package to define our model as a sequence of layers. nn.Sequential</span></div><div class="line"><span class="comment"># is a Module which contains other Modules, and applies them in sequence to</span></div><div class="line"><span class="comment"># produce its output. Each Linear Module computes output from input using a</span></div><div class="line"><span class="comment"># linear function, and holds internal Variables for its weight and bias.</span></div><div class="line">model = torch.nn.Sequential(</div><div class="line">    torch.nn.Linear(D_in, H),</div><div class="line">    torch.nn.ReLU(),</div><div class="line">    torch.nn.Linear(H, D_out),</div><div class="line">)</div><div class="line"></div><div class="line"><span class="comment"># The nn package also contains definitions of popular loss functions; in this</span></div><div class="line"><span class="comment"># case we will use Mean Squared Error (MSE) as our loss function.</span></div><div class="line">loss_fn = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</div><div class="line"></div><div class="line">learning_rate = <span class="number">1e-4</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: compute predicted y by passing x to the model. Module objects</span></div><div class="line">    <span class="comment"># override the __call__ operator so you can call them like functions. When</span></div><div class="line">    <span class="comment"># doing so you pass a Variable of input data to the Module and it produces</span></div><div class="line">    <span class="comment"># a Variable of output data.</span></div><div class="line">    y_pred = model(x)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss. We pass Variables containing the predicted and true</span></div><div class="line">    <span class="comment"># values of y, and the loss function returns a Variable containing the</span></div><div class="line">    <span class="comment"># loss.</span></div><div class="line">    loss = loss_fn(y_pred, y)</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Zero the gradients before running the backward pass.</span></div><div class="line">    model.zero_grad()</div><div class="line"></div><div class="line">    <span class="comment"># Backward pass: compute gradient of the loss with respect to all the learnable</span></div><div class="line">    <span class="comment"># parameters of the model. Internally, the parameters of each Module are stored</span></div><div class="line">    <span class="comment"># in Variables with requires_grad=True, so this call will compute gradients for</span></div><div class="line">    <span class="comment"># all learnable parameters in the model.</span></div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    <span class="comment"># Update the weights using gradient descent. Each parameter is a Variable, so</span></div><div class="line">    <span class="comment"># we can access its data and gradients like we did before.</span></div><div class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</div><div class="line">        param.data -= learning_rate * param.grad.data</div></pre></td></tr></table></figure></p>
<h3 id="PyTorch-optim"><a href="#PyTorch-optim" class="headerlink" title="PyTorch:optim"></a>PyTorch:optim</h3><p>到目前为止我们通过手动的转换Vraiable保存的可学习参数<strong>.data</strong>来更新模型权重。这个并不是一个大的负担对于这个简单的随机梯度下降算法，但是在练习中我们通常使用更复杂的优化器比如AdaGrad，RMSProp，Adam等。</p>
<p>在PyTorch中optim包抽象了优化算法的想法并提供了通常使用的优化算法。</p>
<p>下面的例子中我们像先前一样使用nn包去定义我们的模型，但是我们使用optim包的Adam算法去优化我们的模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables.</span></div><div class="line">x = Variable(torch.randn(N, D_in))</div><div class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Use the nn package to define our model and loss function.</span></div><div class="line">model = torch.nn.Sequential(</div><div class="line">    torch.nn.Linear(D_in, H),</div><div class="line">    torch.nn.ReLU(),</div><div class="line">    torch.nn.Linear(H, D_out),</div><div class="line">)</div><div class="line">loss_fn = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Use the optim package to define an Optimizer that will update the weights of</span></div><div class="line"><span class="comment"># the model for us. Here we will use Adam; the optim package contains many other</span></div><div class="line"><span class="comment"># optimization algoriths. The first argument to the Adam constructor tells the</span></div><div class="line"><span class="comment"># optimizer which Variables it should update.</span></div><div class="line">learning_rate = <span class="number">1e-4</span></div><div class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: compute predicted y by passing x to the model.</span></div><div class="line">    y_pred = model(x)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss.</span></div><div class="line">    loss = loss_fn(y_pred, y)</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Before the backward pass, use the optimizer object to zero all of the</span></div><div class="line">    <span class="comment"># gradients for the variables it will update (which are the learnable weights</span></div><div class="line">    <span class="comment"># of the model)</span></div><div class="line">    optimizer.zero_grad()</div><div class="line"></div><div class="line">    <span class="comment"># Backward pass: compute gradient of the loss with respect to model</span></div><div class="line">    <span class="comment"># parameters</span></div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    <span class="comment"># Calling the step function on an Optimizer makes an update to its</span></div><div class="line">    <span class="comment"># parameters</span></div><div class="line">    optimizer.step()</div></pre></td></tr></table></figure>
<h3 id="PyTorch-Custom-nn-Modules"><a href="#PyTorch-Custom-nn-Modules" class="headerlink" title="PyTorch:Custom nn Modules"></a>PyTorch:Custom nn Modules</h3><p>有些时候你可能像实现一个比一系列已经存在的Modules更复杂的模型；基于此你可以通过继承nn.Module然后定义forward来接受输入的Variable，使用其它模块或者别的自动梯度计算方式来生成Variable。</p>
<p>在这个例子中我们实现了我们两层的神经网络作为一个定制的Module子类：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayerNet</span><span class="params">(torch.nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        In the constructor we instantiate two nn.Linear modules and assign them as</div><div class="line">        member variables.</div><div class="line">        """</div><div class="line">        super(TwoLayerNet, self).__init__()</div><div class="line">        self.linear1 = torch.nn.Linear(D_in, H)</div><div class="line">        self.linear2 = torch.nn.Linear(H, D_out)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        In the forward function we accept a Variable of input data and we must return</div><div class="line">        a Variable of output data. We can use Modules defined in the constructor as</div><div class="line">        well as arbitrary operators on Variables.</div><div class="line">        """</div><div class="line">        h_relu = self.linear1(x).clamp(min=<span class="number">0</span>)</div><div class="line">        y_pred = self.linear2(h_relu)</div><div class="line">        <span class="keyword">return</span> y_pred</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables</span></div><div class="line">x = Variable(torch.randn(N, D_in))</div><div class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Construct our model by instantiating the class defined above</span></div><div class="line">model = TwoLayerNet(D_in, H, D_out)</div><div class="line"></div><div class="line"><span class="comment"># Construct our loss function and an Optimizer. The call to model.parameters()</span></div><div class="line"><span class="comment"># in the SGD constructor will contain the learnable parameters of the two</span></div><div class="line"><span class="comment"># nn.Linear modules which are members of the model.</span></div><div class="line">criterion = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</div><div class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-4</span>)</div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: Compute predicted y by passing x to the model</span></div><div class="line">    y_pred = model(x)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss</span></div><div class="line">    loss = criterion(y_pred, y)</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Zero gradients, perform a backward pass, and update the weights.</span></div><div class="line">    optimizer.zero_grad()</div><div class="line">    loss.backward()</div><div class="line">    optimizer.step()</div></pre></td></tr></table></figure></p>
<h3 id="PyTorch-Control-Flow-Weight-Sharing"><a href="#PyTorch-Control-Flow-Weight-Sharing" class="headerlink" title="PyTorch:Control Flow + Weight Sharing"></a>PyTorch:Control Flow + Weight Sharing</h3><p>作为一个动态图和共享权重的例子，我们实现了一个非常奇怪的模型：一个ReLU作为激活函数的全连接层神经网络，它的每个前向传播选择0到4之间的一个随机数作为需要使用的中间层数，重复使用相同的权重多次去计算隐藏层(这段翻译的不好没有怎么理解)。</p>
<p>对这样的模型我们使用普通的Python流控去实现循环，然后在定义前向传播时在最深处的层通过简单的重复使用相同模块多次实现权重共享。</p>
<p>我们能轻易的实现这个模型通过继承Module。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DynamicNet</span><span class="params">(torch.nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        In the constructor we construct three nn.Linear instances that we will use</div><div class="line">        in the forward pass.</div><div class="line">        """</div><div class="line">        super(DynamicNet, self).__init__()</div><div class="line">        self.input_linear = torch.nn.Linear(D_in, H)</div><div class="line">        self.middle_linear = torch.nn.Linear(H, H)</div><div class="line">        self.output_linear = torch.nn.Linear(H, D_out)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3</div><div class="line">        and reuse the middle_linear Module that many times to compute hidden layer</div><div class="line">        representations.</div><div class="line"></div><div class="line">        Since each forward pass builds a dynamic computation graph, we can use normal</div><div class="line">        Python control-flow operators like loops or conditional statements when</div><div class="line">        defining the forward pass of the model.</div><div class="line"></div><div class="line">        Here we also see that it is perfectly safe to reuse the same Module many</div><div class="line">        times when defining a computational graph. This is a big improvement from Lua</div><div class="line">        Torch, where each Module could be used only once.</div><div class="line">        """</div><div class="line">        h_relu = self.input_linear(x).clamp(min=<span class="number">0</span>)</div><div class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(random.randint(<span class="number">0</span>, <span class="number">3</span>)):</div><div class="line">            h_relu = self.middle_linear(h_relu).clamp(min=<span class="number">0</span>)</div><div class="line">        y_pred = self.output_linear(h_relu)</div><div class="line">        <span class="keyword">return</span> y_pred</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables</span></div><div class="line">x = Variable(torch.randn(N, D_in))</div><div class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Construct our model by instantiating the class defined above</span></div><div class="line">model = DynamicNet(D_in, H, D_out)</div><div class="line"></div><div class="line"><span class="comment"># Construct our loss function and an Optimizer. Training this strange model with</span></div><div class="line"><span class="comment"># vanilla stochastic gradient descent is tough, so we use momentum</span></div><div class="line">criterion = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</div><div class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-4</span>, momentum=<span class="number">0.9</span>)</div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: Compute predicted y by passing x to the model</span></div><div class="line">    y_pred = model(x)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss</span></div><div class="line">    loss = criterion(y_pred, y)</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Zero gradients, perform a backward pass, and update the weights.</span></div><div class="line">    optimizer.zero_grad()</div><div class="line">    loss.backward()</div><div class="line">    optimizer.step()</div></pre></td></tr></table></figure>]]></content>
      
        <categories>
            
            <category> frame </category>
            
            <category> translate </category>
            
        </categories>
        
        
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[pytorch学习笔记]]></title>
      <url>/2017/05/18/pytorch/</url>
      <content type="html"><![CDATA[<h2 id="pytorch基本操作"><a href="#pytorch基本操作" class="headerlink" title="pytorch基本操作"></a>pytorch基本操作</h2><p>在pytorch中会使用到Tensor张量，它可以用来表示多维数组并且如果存在GPU的话可以进行利用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch   </div><div class="line">x = torch.Tensor(<span class="number">5</span>, <span class="number">3</span>) <span class="comment">#构造一个未初始化的矩阵，注意是未初始化的矩阵.    </span></div><div class="line"><span class="comment">#矩阵加法</span></div><div class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>) <span class="comment">#构造一个使用随机数初始化的矩阵，注意返回的这个矩阵是Tensor</span></div><div class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</div><div class="line"><span class="comment">#以下两种方式都可以</span></div><div class="line">x + y</div><div class="line">torch.add(x, y)</div><div class="line"><span class="comment">#如果需要输出tensor</span></div><div class="line">result = torch.Tensor(<span class="number">5</span>, <span class="number">3</span>)</div><div class="line">torch.add(x, y, out=result)</div><div class="line"><span class="comment">#这种方法就会直接将y和x相加后的值更新到y中</span></div><div class="line"><span class="comment">#其中带有“_”的运算上的操作都带有相似的含义</span></div><div class="line">y.add_(x)</div></pre></td></tr></table></figure>
<a id="more"></a>
<p><em>注意：如果是维数相同的矩阵相加那么就是矩阵对应元素相加，如果是矩阵和标量相加那么就是矩阵每个元素都和这个标量相加。</em></p>
<table>
<thead>
<tr>
<th style="text-align:left">操作</th>
<th style="text-align:left">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">x+y</td>
<td style="text-align:left">相加，需要定义另外的变量接受返回值</td>
</tr>
<tr>
<td style="text-align:left">torch_add(x, y)</td>
<td style="text-align:left">含义同上</td>
</tr>
<tr>
<td style="text-align:left">x.add_(y)</td>
<td style="text-align:left">将y加到x上并更新x的值</td>
</tr>
<tr>
<td style="text-align:left">torch.add(x, y, out=result)</td>
<td style="text-align:left">相加后的值更新到自定义的Tensor变量result</td>
</tr>
</tbody>
</table>
<h3 id="Tensor与array转换"><a href="#Tensor与array转换" class="headerlink" title="Tensor与array转换"></a>Tensor与array转换</h3><p>在Torch中进行tensor和numpy的array的转换非常方便。转换后两者会共享同一块存储空间，意思是<strong>一个发生改变另外一个也会改变</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="comment">#生成一个tensor矩阵a</span></div><div class="line">a = torch.ones(<span class="number">5</span>)</div><div class="line"><span class="comment"># 利用Tensor对象a的numpy()方法，完成tensor到numpy的转换</span></div><div class="line">b = a.numpy()</div><div class="line"><span class="comment">#生成一个numpy矩阵c</span></div><div class="line">c = np.ones(<span class="number">5</span>)</div><div class="line"><span class="comment"># 完成从numpy array到tensor的转换</span></div><div class="line">d = torch.from_numpy(c)</div></pre></td></tr></table></figure>
<p><em>注意：使用x.cuda()可以将tensor移动到GPU上进行运算，这个是说得通过这样才能保障这个部分在GPU上进行运算吗？</em></p>
<h2 id="Autograd-自动求导"><a href="#Autograd-自动求导" class="headerlink" title="Autograd(自动求导)"></a>Autograd(自动求导)</h2><p>在pytorch中提供了一个自动求导的功能，这个功能依赖于torch中的autograd.Variable这个类。一般通过一个tensor和requires_grad的表示是否需要计算梯度的标志来实例化。Variable这个的内部简单看包括了data对应传入的tensor，grad对应计算得到的梯度，creator对应这个Variable的创建操作，如果是开发人员自定义的话这个Creator的值为None。</p>
<p><img src="/image/Variable.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line">x = Variable(torch.ones(<span class="number">2</span>, <span class="number">2</span>), requires_grad=<span class="keyword">True</span>)</div><div class="line">print(x.creator)</div><div class="line">&gt;<span class="keyword">None</span></div><div class="line">y = x + <span class="number">2</span></div><div class="line">print(y.creator)</div><div class="line"><span class="comment"># 在经过一个x+2这个操作得到的Variable变量y，y的creator属性表示它是通过一个加操作得到。</span></div><div class="line">&gt;&lt;torch.autograd._functions.basic_ops.AddConstant object at <span class="number">0x104ba7908</span>&gt;</div></pre></td></tr></table></figure>
<p>通过Variable中的data属性和creator属性就可以构建出一个计算图出来。从一个creator为None的地方输入，根据creator来衔接最后得到输出结果。以下绘制了一个简略图以说明<br><img src="/image/autograd.png"><br>在定义了Variable并围绕它进行一系列的操作后得到z，然后通过z.backward()调用次方法进行反向传播自动得到x的梯度，通过x.grad属性就可以获取到。这个注意到creator不为None的对象在反向传播过程中不会返回梯度。<br><em>注意：这里通过z=Mean(Y)得到是一个标量，也就是一些列的计算最后的输出只有一个那么进行backward()时就默认从这里开始，但是如果最后得到的结果有多个元素组成，那么就需要指定一个和输出结果tensor匹配的grad_output参数backward(grad_output)</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line">A = torch.ones(<span class="number">4</span>)</div><div class="line">x = Variable(A, requires_grad=<span class="keyword">True</span>)</div><div class="line">y = x + <span class="number">2</span></div><div class="line">gradients = torch.FloatTensor([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>])</div><div class="line">y.backward(gradients)</div></pre></td></tr></table></figure></p>
<p>另外这个grad_output参数还有个作用举个<a href="http://quabr.com/43451125/pytorch-what-are-the-gradient-arguments" target="_blank" rel="external">例子</a>如果输入的数据x，然后得到了一个中间结果y=[y1, y2, y3]，中间结果被使用来计算最终输出结果z那么：<br>$\frac{\partial z}{\partial x}=\frac{\partial z}{\partial y1}\frac{\partial y1}{\partial x}+\frac{\partial z}{\partial y2}\frac{\partial y2}{\partial x}+\frac{\partial z}{\partial y3}\frac{\partial y3}{\partial x}$这样在grad_output=[$\frac{\partial z}{\partial y1}$, $\frac{\partial z}{\partial y2}$, $\frac{\partial z}{\partial y3}$]再y.backward(grad_output)就可以得到最终的$\frac{\partial z}{\partial x}$了。</p>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>接下来是通过pytorch来实现一个简易卷积神经网络来对minist手写数字进行分类。对于卷积神经网络可以通过<a href="http://www.memorycrash.cn/2017/05/10/cnnNote/" target="_blank" rel="external">前面的文章</a>进行了解。<br><img src="/image/pytorchCnn.png"><br>对于一个卷积神经网络一般有以下模块组成：</p>
<ul>
<li>输入数据集合</li>
<li>若干卷积层+激活层+池化层</li>
<li>全连接层</li>
<li>softmax输出层</li>
</ul>
<p>对于一个卷积神经网络的训练一般是这样：</p>
<ol>
<li>forward前向传播，最后通过交叉熵代价函数得到代价也就是预测值和期望值之间的差距</li>
<li>backward反向传播，将误差信息往会传递获得神经元间连接的权值对应的偏导数</li>
<li>基于SGD(随机梯度下降),根据这些得到的偏导数刷新这些权值(weight=weight-learning_rate*gradient)，返回1如此循环以期望降低代价</li>
</ol>
<p>接下来定义一个卷积神经网络<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(Net, self).__init__() <span class="comment"># 调用Net的父类的__init__()函数进行初始化</span></div><div class="line">        <span class="comment"># 虽然我们使用的是SGD但是在实际中的SGD每次输入的并不是单个样本，而是一个小批次的样本</span></div><div class="line">        <span class="comment"># 1 图片输入通道，6个输出通道(这个6个可以理解为一个本卷积层有6个核)，5x5的感受野</span></div><div class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</div><div class="line">        <span class="comment"># 第2类的卷积层承接上一层的信息所以有6个输入通道，同时有16个核，5x5的感受野</span></div><div class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</div><div class="line">        <span class="comment"># 全连接层接受卷积层的数据y=Wx+b</span></div><div class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</div><div class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</div><div class="line">        <span class="comment"># 最后的输出是10个节点因为要识别0到9的数字</span></div><div class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="comment"># 使用最大池化的池化层，每次池化的窗口是2x2。这里先通过卷积层进行卷积计算后通过</span></div><div class="line">        <span class="comment"># relu来作为激活层进行激活后通过最大池化层池化(采样)</span></div><div class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</div><div class="line">        <span class="comment"># 如果池化的窗口是一个方阵可以只指定一个数字</span></div><div class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</div><div class="line">        <span class="comment"># torch 的 view 类似 numpy 的reshape用来调整形状，指定某个位置为-1表示将这个位置</span></div><div class="line">        <span class="comment"># 应该填写的内容交给系统自行计算。这里是为了方便输入数据到全连接层将调整后列数设置为</span></div><div class="line">        <span class="comment"># 16*5*5的大小，行数由系统计算得到</span></div><div class="line">        x = x.view(<span class="number">-1</span>, self.num_flat_features(x))</div><div class="line">        <span class="comment"># 使用relu作为激活函数的全连接层</span></div><div class="line">        x = F.relu(self.fc1(x))</div><div class="line">        x = F.relu(self.fc2(x))</div><div class="line">        x = self.fc3(x)</div><div class="line">        <span class="keyword">return</span> x</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="comment"># x[TensorSamples,nChannels,Height,Width]</span></div><div class="line">        <span class="comment"># x.size()[1:]得到x[nChannels,Height,Width]</span></div><div class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># 获取除开批次后的其它维度</span></div><div class="line">        num_features = <span class="number">1</span></div><div class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</div><div class="line">            num_features *= s</div><div class="line">        <span class="keyword">return</span> num_features</div><div class="line">        </div><div class="line">net = Net()</div><div class="line">print(net)</div></pre></td></tr></table></figure></p>
<h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h3><p>这里考虑用来计算模型预测值和期望值之间度量的代价函数，nn.MSELoss是一个均方误差代价函数$E=\frac{1}{N}\sum_{i}(\hat{y_i}-y_i)^2$还有需要注意的是我们需要训练的参数可以通过net.parameters()获得，得到的，然后转换成list后可以按照定义的顺序来获取到训练参数。下面的链式顺序表示从输入到获得loss的过程。</p>
<blockquote>
<p>input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d<br>     -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear<br>     -&gt; MSELoss<br>     -&gt; loss  </p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"><span class="comment"># 以下代码只是举例如何使用代价函数</span></div><div class="line"><span class="comment"># 手动构造一个测试数据</span></div><div class="line">input = Variable(torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>))</div><div class="line"><span class="comment"># 使用数据初始化一个神经网络</span></div><div class="line">output = net(input)</div><div class="line"><span class="comment"># 手动初始化一个目标值</span></div><div class="line">target = Variable(torch.range(<span class="number">1</span>, <span class="number">10</span>))  </div><div class="line"><span class="comment"># 定义一个均方误差代价函数</span></div><div class="line">criterion = nn.MSELoss()</div><div class="line"><span class="comment"># 获得代价值</span></div><div class="line">loss = criterion(output, target)</div><div class="line">loss.backward()</div></pre></td></tr></table></figure>
<h3 id="更新权值"><a href="#更新权值" class="headerlink" title="更新权值"></a>更新权值</h3><p>这里更新权值的方式是调用已经封装好的函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch, optim <span class="keyword">as</span> optim</div><div class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</div><div class="line"></div><div class="line"> <span class="comment">#选择我们进行优化的方式SGD随机梯度下降，传入需要处理的参数已经学习率</span></div><div class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</div><div class="line"></div><div class="line"> <span class="comment">#构建一个训练的循环</span></div><div class="line">optimizer.zero_grad()   <span class="comment"># 将梯度先值0，因为还没有开始进行训练</span></div><div class="line">output = net(input)</div><div class="line">loss = criterion(output, target)</div><div class="line">loss.backward()</div><div class="line">optimizer.step()    <span class="comment"># 一个循环中完成一次权值的更新</span></div></pre></td></tr></table></figure></p>
<h2 id="关于训练数据"><a href="#关于训练数据" class="headerlink" title="关于训练数据"></a>关于训练数据</h2><p>对于训练数据来说主要分为图片数据，语言数据以及文本信息。处理方式一般是先通过某些包将其加载到numpy的数组中，再转换为tensor。</p>
<ul>
<li>处理图像比较好的包是Pillow，OpenCV</li>
<li>处理语音比较好的包是sicpy，librosa</li>
<li>处理文本一般python的加载功能也可以</li>
</ul>
<p>对于图像来说，torchvision这个包中可以加载常用的图像数据，比如Imagenet(数据量大)，CIFAR10(有十种分类)，MNIST(简单的手写0到9数字灰度图)接下来的练习会选择CIFAR10数据集包含了这些分类:‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’。3X32X32是3个颜色通道大小32X32的图片。</p>
<p><img src="/image/cifar10.png"><br>接下来的代码基本和上面已经讲过的一致只是多了使用torchvision来加载数据处理数据的过程和训练模型已经验证模型。以下代码注释内容引用了<a href="www.jianshu.com/p/8da9b24b2fb6">这篇文章</a>。图像预处理可以参考<a href="https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit" target="_blank" rel="external">cs231n的课堂笔记</a></p>
<blockquote>
<p>ToTensor是指把PIL.Image(RGB)或者numpy.ndarray(H x W x C)从0到255的值映射到0到1的范围内并转化为Tensor格式(C x H x W)这里表示channel的C位置发生了改变<br>Normalize(mean, std)是通过下面的公式实现数据的归一化提供一个表示RGB标准差(R,G,B)和表示均值的(R,G,B)，mean是均值std是标准差$ std=\sqrt{ \frac{1}{N}\sum_{i=1}^{N}(x_i-mean)^2}$<br>channel=(channel-mean)/std</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torchvision</div><div class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</div><div class="line"><span class="comment"># transform可以对数据进行预处理,compose函数是去组合需要进行预处理的项目，比如归一化等</span></div><div class="line"><span class="comment"># 经过下面处理的数据取值变为[-1,1]</span></div><div class="line">transform = transforms.Compose([transforms.ToTensor(),</div><div class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</div><div class="line"><span class="comment"># 表示加载训练数据 train=True</span></div><div class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="keyword">True</span>,</div><div class="line">                                        download=<span class="keyword">True</span>, transform=transform)</div><div class="line"><span class="comment"># 将训练数据50000张图片划分为12500份，每份4张。shffule=True表示不同批次的数据遍历需打乱顺序</span></div><div class="line"><span class="comment"># num_workers表示使用两个子进程来加载数据                                    </span></div><div class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>,</div><div class="line">                                          shuffle=<span class="keyword">True</span>, num_workers=<span class="number">2</span>)</div><div class="line"><span class="comment"># 表示加载验证数据 train=False</span></div><div class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="keyword">False</span>,</div><div class="line">                                       download=<span class="keyword">True</span>, transform=transform)</div><div class="line">                                       </div><div class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>,</div><div class="line">                                         shuffle=<span class="keyword">False</span>, num_workers=<span class="number">2</span>)</div><div class="line"></div><div class="line">classes = (<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>,<span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</div></pre></td></tr></table></figure>
<p>momentum是梯度下降法中一种常用的加速技术。对于一般的SGD，其表达式为$x=x-\alpha*dx$,沿负梯度方向下降。而带momentum项的SGD则写生如下形式$v=\beta*v - \alpha*dx;x=x+v$：其中即momentum系数，通俗的理解上面式子就是，如果上一次的momentum（即）与这一次的负梯度方向是相同的，那这次下降的幅度就会加大，所以这样做能够达到加速收敛的过程。下面是这次用到的卷积神经网络代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(Net, self).__init__()</div><div class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</div><div class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</div><div class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</div><div class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</div><div class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</div><div class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = self.pool(F.relu(self.conv1(x)))</div><div class="line">        x = self.pool(F.relu(self.conv2(x)))</div><div class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</div><div class="line">        x = F.relu(self.fc1(x))</div><div class="line">        x = F.relu(self.fc2(x))</div><div class="line">        x = self.fc3(x)</div><div class="line">        <span class="keyword">return</span> x</div><div class="line"></div><div class="line"></div><div class="line">net = Net()</div><div class="line">criterion = nn.CrossEntropyLoss()</div><div class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</div><div class="line"></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">2</span>):  <span class="comment"># loop over the dataset multiple times</span></div><div class="line"></div><div class="line">    running_loss = <span class="number">0.0</span></div><div class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</div><div class="line">        <span class="comment"># 获取输入数据</span></div><div class="line">        inputs, labels = data</div><div class="line"></div><div class="line">        <span class="comment"># 转换输入数据以及标签为Variable</span></div><div class="line">        inputs, labels = Variable(inputs), Variable(labels)</div><div class="line"></div><div class="line">        <span class="comment"># 重置梯度为0</span></div><div class="line">        optimizer.zero_grad()</div><div class="line"></div><div class="line">        <span class="comment"># 前向 + 后向 + SGD更新</span></div><div class="line">        outputs = net(inputs)</div><div class="line">        loss = criterion(outputs, labels)</div><div class="line">        loss.backward()</div><div class="line">        optimizer.step()</div><div class="line"></div><div class="line">        <span class="comment"># print statistics</span></div><div class="line">        running_loss += loss.data[<span class="number">0</span>]</div><div class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:    <span class="comment"># print every 2000 mini-batches</span></div><div class="line">            print(<span class="string">'[%d, %5d] loss: %.3f'</span> %</div><div class="line">                  (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</div><div class="line">            running_loss = <span class="number">0.0</span></div><div class="line"></div><div class="line">print(<span class="string">'Finished Training'</span>)</div></pre></td></tr></table></figure></p>
<p>参考：<br>[1]<a href="http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="external">Deep Learning with PyTorch: A 60 Minute Blitz</a><br>[2]<a href="https://zhuanlan.zhihu.com/p/25572330" target="_blank" rel="external">PyTorch深度学习：60分钟入门(Translation)</a></p>
]]></content>
      
        <categories>
            
            <category> frame </category>
            
        </categories>
        
        
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[循环神经网络简略笔记]]></title>
      <url>/2017/05/11/rnnNote/</url>
      <content type="html"><![CDATA[<h2 id="循环神经网络和卷积神经网络"><a href="#循环神经网络和卷积神经网络" class="headerlink" title="循环神经网络和卷积神经网络"></a>循环神经网络和卷积神经网络</h2><p>我们可以使用卷积神经网络来处理图片的分类，从目前看其包含的特点是输入到卷积层的图片是大小是固定的，待分类的图片与图片之间是相对独立的存在。通过对目前我们了解到的卷积神经网络的描述可以了解到它的模型处理的场景和我们的现实生活还有有比较大的差异。</p>
<a id="more"></a>
<p>现实的生活中我们做出来的判断可能不能单靠一张图片来进行，比如我们需要和其它人交流了解到他说的整段话以后才能明白含义，看一篇文章，看一部视频也是一样的道理。我们因为可以记下交流中过去时刻的信息，最后基于这些记忆的内容来做出判断。</p>
<p>了解到这些后，有必要再分析下卷积神经网络的模型。在卷积神经网络中神经元模型是一个接受输入再输入的形式，可以看出在这样的模型中并不能记忆过去时刻的信息。而接下来的循环神经网络的部分层的神经元模型实现了以往信息对当前状态影响结果的记录。</p>
<h2 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h2><p>循环神经网络的基本机构如下图<br><img src="/image/rnnStru.png"><br>第一次看到这个图可能会感到很困惑，可以暂时不看w和它所在的圆形箭头那这个图便是一个常见的神经元模型，x作为输入进行权值矩阵U后输入到s，然后s通过激活函数激活后输出，输出后的值同样经过一个权值矩阵后输入到o。这里我们暂时忽略偏移bias，这不影响理解。</p>
<p>现在我们来考虑w的存在，假设某一个t时刻，从x输入了一个数据，它经过了权值矩阵后进入s，然后同样是经过激活函数的激活。接着输出的数据会通过v传递下去，同时会经过w这个权值矩阵处理后保存下来。等到t+1时刻有数据从x传入的时候，s就会吸收新传入的数据和上一时刻通过w处理后的信息了。这样就构造了一个可以保存记忆基于记忆来进行处理的神经元模型。</p>
<p>如上所述，在不同时刻数据通过这个结构都在影响着输入，下面按照时间将展开这个结构。<br><img src="/image/rnnUnflod.jpg" width="70%" height="70%"></p>
<ul>
<li>首先图中的W，V，U都是同一个，只是在时间维度上展开了。</li>
<li>从左向右传递上一个结点状态s会传递给下一个结点。</li>
<li>这里o在每个时刻都会依据当前的状态进行判断病输出。</li>
</ul>
<p>这里我们看看展开后的循环神经网络结构。这里的x可以理解为代表输入层，W代表循环层的权值矩阵，o表示最终的输出层。一般会使用sigmoid,tanh或者relu作为激活函数，循环层中我们使用tanh作为激活函数，最终使用softmax作为输出层。<br>$ o_{t}=g(Vs_{t}) $<br>$ s_{t}=f(Ux_{t}+Ws_{t-1}) $</p>
<h2 id="循环神经网络实现"><a href="#循环神经网络实现" class="headerlink" title="循环神经网络实现"></a>循环神经网络实现</h2><p><img src="/image/rnnLayer.png"><br>注意到这里多一个$ h_{t} $符号，它表示t时刻隐藏层的输入$ h_{t}=Ux_{t}+Ws_{t-1} $。还有$ \widehat{y}_{t} $表示t时候模型预测的最终输出值。如果我们要实现的是一个三层结构的循环神经网络并使用它来进行句子中词语的预测。并希望最终达到这样的效果。<br><img src="/image/rnnWords.png"><br>这里的s和e代表一句话的开始和结束。上图展示出来的情况表达的含义是我在每个时间点输入一个句子中的单词然后模型会预测出我下一个单词是什么，比如在t2时刻我输入了x2(我)，这个时候我期望得到的单词是“昨天”。这里举例的这个场景是rnn应用的一个小点，其它还包括机器翻译等。下面基于预测单词的要求来看看完成循环神经网络是什么样的。</p>
<h3 id="输入数据"><a href="#输入数据" class="headerlink" title="输入数据"></a>输入数据</h3><p>既然是预测单词，那我们的输入也是单词的内容。这里以一个句子作为一条训练样本，以句子中的单词作为具体的输入信息。可以看做单词是句子这个训练样本中的元素。按照上面的说法我们还需要给句子标注出开始和结束标志。我们把句子中的没有包含结束标志的部分可以当作是输入数据，把句子中没有包含开始标志的部分看做是验证模型预测输出数据是否正确的标准。因为我们并不能直接把一个单词作为输入或者输出。所以我们需要建立一个词汇表，就像是字典一样，在词汇表中将位置信息和具体的词汇进行映射。将映射后的信息比如[0 0 0 0 1 0 0]就代表“我”，传入到模型中去。</p>
<h3 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h3><p>在隐藏层中我们包含了一个循环，处于隐藏层的神经元接受当前时候的输入$ x_{t} $和上一个时刻的神经元的状态$ s_{t-1} $作为本次的输入，然后经过tanh激活函数进行激活后输出，同时也作为下一个时刻神经元的输入$ s_{t} $。</p>
<p>$ tanh(z) = \frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$<br>$ h_{t}=Ux_{t}+Ws_{t-1} $<br>$ s_{t}=tanh(Ux_{t}+Ws_{t-1}) $</p>
<p>tanh函数的图像是这样，和sigmoid很类似，但是tanh是关于0旋转对称的，它的取值范围是[-1,1]而sigmoid的取值范围是[0,1]。sigmoid的导数是$ {f}’(z)=f(z)(1-f(z)) $而tanh的导数$ {f}’(z)=1-(f(z))^{2} $</p>
<p><img src="/image/tanh.png"></p>
<h3 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h3><p>输出层的输入信息是从隐藏层来的，隐藏层输出的状态通过权值矩阵V处理以后输入到输出层softmax，这里V的主要作用是将隐藏层输出的矩阵信息转换为输出层可以处理的向量信息信息，因为输出层输出的是向量。softmax的公式是<br>$$ \sigma (z)_{j}=\frac{e^{z_{j}}}{\sum_{k=1}^{K}e^{z_{k}}} $$<br>可以看出通过softmax层输出后的是模型认为在这个位置应该出现的单词的概率。也就是说softmax输出的数据中值最大的那个就是模型预测的值。</p>
<h2 id="循环神经网络的训练"><a href="#循环神经网络的训练" class="headerlink" title="循环神经网络的训练"></a>循环神经网络的训练</h2><p>在训练循环神经网络的时候我们使用的是bptt(Backprogation Through Time)算法，基本思想和bp算法是一致的。bptt多考虑了在时间维度上进行循环的影响。一般来说训练一个神经网络我们先获取到经过模型的预测值和实际值之间的差异，这个过程属于一个前向传播的过程(这个差异就是代价函数我们这里使用交叉熵作为代价函数)。然后对预测值和实际值的差异从减小差异的方向来调整各个层的权值，这个过程属于一个后向传播的过程。之后重复这个过程直到模型预测结果复合要求。</p>
<p>后续的进行bptt算法的推导可以参考<a href="http://www.cnblogs.com/wacc/p/5341670.html" target="_blank" rel="external">此篇文档</a>这里只简单描述下，如上所述的</p>
<blockquote>
<p>“处于隐藏层的神经元接受当前时候的输入$ x_{t} $和上一个时刻的神经元的状态$ s_{t-1} $作为本次的输入”</p>
</blockquote>
<p>所以对于反向传播也会存在两个方向的传播，其实也好理解因为有两个方向的传播才能兼顾到U和W两个权值矩阵并对他们进行更新。而一般的非循环神经网络只有一个方向的传递路径。<br><img src="/image/rnnBptt.jpg"></p>
<ul>
<li>红色方向涉及权值矩阵U，表示将梯度传递到上一层网络(从隐藏层的角度看)</li>
<li>绿色方向涉及到权值矩阵W，表示将梯度沿着时间方向传递</li>
</ul>
<p>推导bptt公式还可以参考<a href="https://zybuluo.com/hanbingtao/note/541458" target="_blank" rel="external">此篇文档</a>,这里直接给出来(这里我感觉这个文章的推导中只计算了t时刻的梯度)。在时间维度展开以后对于代价函数我们可以表示为$ E=\sum_{t=0}^{T}E_{t} $我们需要获取的是E对V，W，U的求导结果。那根据上面的公式可以的得到。<br>$ \frac{\partial E}{\partial V}=\sum_{t=0}^{T}\frac{\partial E_{t}}{\partial V} $<br>$ \frac{\partial E}{\partial W}=\sum_{t=0}^{T}\frac{\partial E_{t}}{\partial W} $<br>$ \frac{\partial E}{\partial U}=\sum_{t=0}^{T}\frac{\partial E_{t}}{\partial U} $<br>下面是基于上图和这三个式子的进一步推导。<br>$ \frac{\partial E_{t}}{\partial V}=(\hat{y_{t}}-y_{t})\otimes h_{t} $<br>$ \frac{\partial E}{\partial V}=\sum_{t=0}^{T}(\hat{y_{t}}-y_{t})\otimes h_{t} $<br>以上表示的是权值矩阵V的导数计算公式，这里需要注意$ \frac{\partial E_{t}}{\partial V} $只是和当前的t时刻有关系。这个可以上面的图中看出来对于每个当前时刻的代价函数在将反向传播的时候只会经过对V这个权值矩阵只会经过一次。<br>$ \delta_{k}=(U^{T}\delta_{k+1})\odot (1-h_{k}\odot h_{k}) $<br>$ \delta_{t}=(V^{T}(\hat{y_{t}}-y_{t}))\odot (1-h_{t}\odot h_{t}) $<br>以上两个公式中$\delta_{k}$表示的是中间节点的误差(隐藏层到隐藏层或者隐藏层到输入层)，但是在传递误差的开始节点$\delta_{t}$和$\delta_{k}$是相等的。<br>$ \frac{\partial E_{t}}{\partial W}=\sum_{k=0}^{t}\delta_{k}\otimes h_{k-1} $<br>$ \frac{\partial E}{\partial W}=\sum_{t=0}^{T}\sum_{k=0}^{t}\delta_{k}\otimes h_{k-1} $<br>以上公式表示隐藏层到隐藏层在时间维度展开后计算导数。这里我们根据上图可以看出来$ \frac{\partial E_{t}}{\partial W} $对于$E_{t}$不单单收到当前t时刻的影响，也受到t-1时刻及其以前的时刻的影响，所以我们会把t时刻的起始误差循环的往过去传递以此来获取过去的时刻的梯度，因为W在各个时刻都是共享的。所以对于$E_{t}$来说它的起始误差对W的影响就是把从t开始往前时刻的影响都加起来。从这里的描述可以看出来这个也是当前t时刻的可以使用以前时刻的记忆的来源，因为依然受到了以前时刻的影响。不过根据梯度消失的情况我们越早时间的点对于当前t时刻的影响就越小，获取话说就是记不清太久以前的事情。<br>$ \frac{\partial E_{t}}{\partial U}=\sum_{k=0}^{t}\delta_{k}\otimes x_{k} $<br>$ \frac{\partial E}{\partial U}=\sum_{t=0}^{T}\sum_{k=0}^{t}\delta_{k}\otimes x_{k} $<br>以上公式是表示隐藏层到输入层在时间维度展开后计算导数。误差的传递根据上面的图可以看出来是会往U的方向传递的。因为循环层不仅受到前一个状态的影响也搜到各个时刻的当前输入影响。</p>
<h3 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h3><p>梯度消失在循环神经网络中更容易出现，我们在进行反向传播的时候，比如计算当前时刻t对于W的梯度，本质上对于W的影响是存在于过往的每一个时刻的。比如我们去理解一个句子的信息，在当前t时刻获取的字符要理解其含义可能需要联系下先前任意时刻的信息。但是在我们刚才的描述中也会发现对于t-n时刻，这个t-n的梯度就会比t-n+1时刻小，最终在某个时刻可能就没有梯度了也就是完全遗忘了以前的记忆。对于当前也就失去了影响。</p>
<h3 id="numpy矩阵操作"><a href="#numpy矩阵操作" class="headerlink" title="numpy矩阵操作"></a>numpy矩阵操作</h3><p>这里简单记录下numpy部分矩阵操作。其中A.dot(B)表示A和B进行矩阵乘法，A*B在numpy表示A和B的对应元素相乘，np.outer(A,B)表示A和B进行外积计算，np.dot(A,B)表示A和B进行点积计算</p>
<table>
<thead>
<tr>
<th>numpy操作</th>
<th style="text-align:left">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>A.dot(B)</td>
<td style="text-align:left">A和B进行矩阵乘法</td>
</tr>
<tr>
<td>np.dot(A,B)</td>
<td style="text-align:left">A和B进行矩阵乘法</td>
</tr>
<tr>
<td>A*B</td>
<td style="text-align:left">A和B的对应元素相乘</td>
</tr>
<tr>
<td>np.multiply(U3, U4)</td>
<td style="text-align:left">A和B的对应元素相乘</td>
</tr>
<tr>
<td>np.outer(A,B)</td>
<td style="text-align:left">A和B进行外积计算</td>
</tr>
<tr>
<td>A.T</td>
<td style="text-align:left">A矩阵的转置</td>
</tr>
</tbody>
</table>
<h2 id="长短期记忆"><a href="#长短期记忆" class="headerlink" title="长短期记忆"></a>长短期记忆</h2><p>长短期记忆 Long Short Term Memory(LSTM)可以很好的解决上面提到的循环神经网络中梯度消失问题。LSTM将原来直接操作输入的数据和上一个时刻状态的方式修改为，通过三个门来控制输入数据和上一个时刻的状态的信息。这三个门分别是<strong>遗忘门(forget gate)</strong>，<strong>输入门(input gate)</strong>，<strong>输出门(output gate)</strong>。</p>
<p>LSTM部分详细解释可以参考colah的博客<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM Networks</a>及其<a href="https://www.yunaitong.cn/understanding-lstm-networks.html" target="_blank" rel="external">中文翻译</a></p>
<p>以及这篇文章<a href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/" target="_blank" rel="external">WILDML</a>及其<a href="https://zhuanlan.zhihu.com/p/22371429" target="_blank" rel="external">中文翻译</a></p>
<p>对于LSTM的梯度推导过程可以看下<a href="https://zybuluo.com/hanbingtao/note/581764" target="_blank" rel="external">零基础入门深度学习(6) - 长短时记忆网络(LSTM)</a></p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> rnn </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[卷积神经网络简略笔记]]></title>
      <url>/2017/05/10/cnnNote/</url>
      <content type="html"><![CDATA[<h2 id="卷积神经网络与神经网络"><a href="#卷积神经网络与神经网络" class="headerlink" title="卷积神经网络与神经网络"></a>卷积神经网络与神经网络</h2><p>神经网络和卷积神经网络的关系可以理解过卷积神经网络是为了可以更好处理图片而对神经网络进行了相应的改造和特殊化。这里以卷积来为这个新的神经网络进行命名，可能是卷积神经网络的卷积层在对原始数据的处理上类似于卷积操作。但是如果本身对数学上的卷积操作不怎么了解，其实也不影响对卷积神经网络的理解。<br><a id="more"></a></p>
<h2 id="神经网络处理图片的问题"><a href="#神经网络处理图片的问题" class="headerlink" title="神经网络处理图片的问题"></a>神经网络处理图片的问题</h2><p>我们可以使用神经网络来处理图片，对图片进行学习分类。但是神经网络可能对处理尺寸较小的，不复杂的图片处理更擅长。对于具有相反特点的图片神经网络的学习效果就不理想了。其中的原因包括有神经网络会将图片的所有像素信息都传入到神经网络的输入层中，当图片过大或者需要进行更复杂的分类时会导致模型中出现大量的参数。这样直接导致的是计算量变得极大最终难以计算。而且为了进行更复杂的判断新增的隐藏又可能导致梯度消失的情况。</p>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>为了减少上面问题的影响，发展出了卷积神经网络。最主要的特点是添加了卷积层来对图片进行处理。卷积神经网络模型的样子如下：</p>
<p><img src="/image/simple_conv.png" width="50%" height="50%/"></p>
<p>简单说下上图中convolutionallayer就是卷积层，主要作用是通过过滤器来过滤出原始图片的特征，一般一个卷积层有多个过滤器所以也就能得到多个特征来。接下来是poolinglayer是池化层主要是将卷积层产生的特征信息进行采样以进一步减小图片的尺寸。接着将池化后的数据输入到全连接层，这里的全连接层就和神经网络是一样，接着是通过输出层得到分类结果。</p>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>在神经网络中我们一般使用sigmod作为激活函数，它的图像如下</p>
<p><img src="/image/sigmoid.png" width="50%" height="50%/"></p>
<p>这里可以看出来在偏离图像的中心点0越远则图像的变换的趋势越加缓慢，如果在初始化w和b的过程可能某些神经元获得了一个更大的值。这就引起了梯度的下降变的缓慢解决的方式可以选择换一个激活函数比如relu函数，它的图像如下：</p>
<p><img src="/image/relu.png" width="50%" height="50%"></p>
<h3 id="代价函数-sigmoid-交叉熵"><a href="#代价函数-sigmoid-交叉熵" class="headerlink" title="代价函数 sigmoid+交叉熵"></a>代价函数 sigmoid+交叉熵</h3><p>在先前使用sigmod作为激活函数的神经网络中我们使用代价函数的模式一般是平方差的形式。代价函数的形式是会依赖对激活函数求导，我们知道求导对应到曲线可以理解为曲线在某点的变化率。以下通过一个一个输入一个输出的单个神经元模型来进行解释。以下输入假设x是1，期待y输出为0可以看出求导的结果确实依赖了对sigmod的求导，这样就会出现上面讨论的梯度下降缓慢的问题。<br>$$ C=\frac{(y-a)^{2}}{2} $$</p>
<p>$$ \frac{\partial C}{\partial w}=(a-y){\sigma}’(z)x=a{\sigma}’(z) $$</p>
<p>$$ \frac{\partial C}{\partial b}=(a-y){\sigma}’(z)=a{\sigma}’(z) $$</p>
<p>通过使用交叉熵来作为代价函数：</p>
<p>$$ C=-\frac{1}{n}\sum_{x}[ylna+(1-y)ln(1-a)] $$</p>
<p>可以对w和b分别求导发现结果是和激活函数的导数没有关系的。</p>
<h3 id="softmax-log-likelihood"><a href="#softmax-log-likelihood" class="headerlink" title="softmax+log-likelihood"></a>softmax+log-likelihood</h3><p>除了交叉熵以外还可以通过softmax的方式来解决学习速度衰减的问题。我们仅将输出层从普通的sigmod作为激活函数的层替换为softmax层。softmax输出层同样接受z=wx+b然后通过以下公式来计算输出结果</p>
<p>$$ a_{j}^{L}=\frac{e^{z_{j}^{L}}}{\sum_{k}e^{z_{k}^{L}}} $$</p>
<p>可以看出来这里得到的是某个值占总体的一个比例。配合softmax我们的代价函数需要替换成log-likelihood</p>
<p>$$ C\equiv-lna_{y}^{L} $$</p>
<p>这里表示的是单个输入样本的代价，如果有多个样本的可以对他们的代价求均值，作为总的代价函数。通过代价函数对w和b求导得到公式</p>
<p>$$ \frac{\partial C}{\partial b_{j}^{L}}=a_{j}^{L}-y_{j} $$</p>
<p>$$ \frac{\partial C}{\partial w_{jk}^{L}}=a_{k}^{L-1}(a_{k}^{L}-y_{j}) $$</p>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>一般有l1,l2,dropout的方式来对模型进行正则化，主要目的还是防止模型的过拟合，其中l2正则化方法是对所有的w进行如下处理并把这个部分添加到代价函数中去需要注意的是l2正则化只需包括w不需要包括b。</p>
<p>$$ C=-\frac{1}{n}\sum_{xj}[y_{j}lna_{j}^{L}+(1-y_{j})ln(1-a_{j}^{L})]+\frac{\lambda}{2n}\sum_{w}w^{2} $$</p>
<p>这样起到的作用使的优化这个代价函数，会更倾向于获取一个w并不是那么复杂的模型。一般直观的来看过拟合的模型都是在训练集中表现过于好的函数，而表现的过好的模型一般w都是比较复杂的。我们对于l1正则化不作解释。</p>
<p>接下还有一种方式是dropout，可以理解为丢弃。处理的思想类似我们以前看的集成学习法。这dropout中会随机的丢弃一些神经元(非输入和输出层)，也就是我们每次迭代更新梯度只对模型中的部分神经元进行处理。在一个batch的样本训练并更新完w和b以后我们会重新再进行一次dropout。这样的感觉就像训练了多个子神经网络，最后将他们组合成一个大的神经元来进行使用。是不是和集成学习思想类似？</p>
<h3 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h3><p>在先前的神经网络中一般采用的是符合一个均值是0，标准差是1的高斯分布的随机数去初始化w和b，但是这个并不适合初始化隐藏层比较多的神经网络。一般是将标准差表示为 $1/\sqrt{n_{in}}$ 这里的 $n_{in}$ 表示具有输入权重的神经元个数。</p>
<h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>卷积层是卷积神经网络不同于神经网络的一个重要特点。首先我们输入到卷积神经网络的依然是一个图片，但是作为接受这个数据的卷积层并不是如果神经网络一样将图片的所有像素信息都输入，而是使用一个比输入图片小的多的过滤器来扫描原始输入的图片数据。过滤器可以理解为将原始图片的一小部分作为输入并通过权值和偏移进行计算后输入激活函数得到在新的数据。这个过滤器会按照设置每次滑动并重复刚才的过程。最终得到的就是对应一个过滤器得到的一个特征map。这里需要特别说明单独过滤器滑动的任何区域计算使用的w和b都是一样的，一般的讲这个可以称为<strong>参数共享</strong>。这样的好处是识别某个特征将和这个特征在原始图片出现的位置无关。</p>
<p><img src="/image/no_padding.gif" width="50%" height="50%/"></p>
<p>可以直观的看到经过过滤器处理以后的图像的大小变小了，同时多个过滤器也可以获得到图片的多种特征。同时可以看出来我们需要仔细设计过滤器的大小以及每次滑动的距离以使的在原始图像中每个像素都可以涉及到。但是可能真就存在冲突的情况或者我希望过滤器处理后图片的大小不发生改变，这个时候可以适当的在待过滤的图片边缘添加0来实现。</p>
<p><img src="/image/padding.gif" width="50%" height="50%/"></p>
<p>这些处理得到特征map一般还需要通过池化来进行采样，其中一个目的就是进一步减小图片大小。一般的池化方法就是最大池化。比如使用2*2大小的框格在特制map上每次移动2格，将22在特征map范围上的最大的数据返回。这样处理以后的数据可能是作为下一个卷积层的输入也可能作为全连接层的输入。</p>
<p>在实际中卷积层的输入可能是多个通道的输入，输出也可能是多个通道的。这个时候针对输入的每个通道都会有一个卷积核然后各个输入通道卷积核每次滑动得到的结果相加得到输出的某一层的特征值。如果是多个输出通道那么也就需要多组输入的卷积核。</p>
<p><img src="/image/mulMap.png"></p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> cnn </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
