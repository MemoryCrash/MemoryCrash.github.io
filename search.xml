<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[循环神经网络简略笔记]]></title>
      <url>/2017/05/11/rnnNote/</url>
      <content type="html"><![CDATA[<h2 id="循环神经网络和卷积神经网络"><a href="#循环神经网络和卷积神经网络" class="headerlink" title="循环神经网络和卷积神经网络"></a>循环神经网络和卷积神经网络</h2><p>我们可以使用卷积神经网络来处理图片的分类，从目前看其包含的特点是输入到卷积层的图片是大小是固定的，待分类的图片与图片之间是相对独立的存在。通过对目前我们了解到的卷积神经网络的描述可以了解到它的模型处理的场景和我们的现实生活还有有比较大的差异。</p>
<a id="more"></a>
<p>现实的生活中我们做出来的判断可能不能单靠一张图片来进行，比如我们需要和其它人交流了解到他说的整段话以后才能明白含义，看一篇文章，看一部视频也是一样的道理。我们因为可以记下交流中过去时刻的信息，最后基于这些记忆的内容来做出判断。</p>
<p>了解到这些后，有必要再分析下卷积神经网络的模型。在卷积神经网络中神经元模型是一个接受输入再输入的形式，可以看出在这样的模型中并不能记忆过去时刻的信息。而接下来的循环神经网络的部分层的神经元模型实现了以往信息对当前状态影响结果的记录。</p>
<h2 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h2><p>循环神经网络的基本机构如下图<br><img src="/image/rnnStru.png"><br>第一次看到这个图可能会感到很困惑，可以暂时不看w和它所在的圆形箭头那这个图便是一个常见的神经元模型，x作为输入进行权值矩阵U后输入到s，然后s通过激活函数激活后输出，输出后的值同样经过一个权值矩阵后输入到o。这里我们暂时忽略偏移bias，这不影响理解。</p>
<p>现在我们来考虑w的存在，假设某一个t时刻，从x输入了一个数据，它经过了权值矩阵后进入s，然后同样是经过激活函数的激活。接着输出的数据会通过v传递下去，同时会经过w这个权值矩阵处理后保存下来。等到t+1时刻有数据从x传入的时候，s就会吸收新传入的数据和上一时刻通过w处理后的信息了。这样就构造了一个可以保存记忆基于记忆来进行处理的神经元模型。</p>
<p>如上所述，在不同时刻数据通过这个结构都在影响着输入，下面按照时间将展开这个结构。<br><img src="/image/rnnUnflod.jpg" width="70%" height="70%"></p>
<ul>
<li>首先图中的W，V，U都是同一个，只是在时间维度上展开了。</li>
<li>从左向右传递上一个结点状态s会传递给下一个结点。</li>
<li>这里o在每个时刻都会依据当前的状态进行判断病输出。</li>
</ul>
<p>这里我们看看展开后的循环神经网络结构。这里的x可以理解为代表输入层，W代表循环层的权值矩阵，o表示最终的输出层。一般会使用sigmoid,tanh或者relu作为激活函数，循环层中我们使用tanh作为激活函数，最终使用softmax作为输出层。<br>$ o_{t}=g(Vs_{t}) $<br>$ s_{t}=f(Ux_{t}+Ws_{t-1}) $</p>
<h2 id="循环神经网络实现"><a href="#循环神经网络实现" class="headerlink" title="循环神经网络实现"></a>循环神经网络实现</h2><p><img src="/image/rnnLayer.png"><br>注意到这里多一个$ h_{t} $符号，它表示t时刻隐藏层的输入$ h_{t}=Ux_{t}+Ws_{t-1} $。还有$ \widehat{y}_{t} $表示t时候模型预测的最终输出值。如果我们要实现的是一个三层结构的循环神经网络并使用它来进行句子中词语的预测。并希望最终达到这样的效果。<br><img src="/image/rnnWords.png"><br>这里的s和e代表一句话的开始和结束。上图展示出来的情况表达的含义是我在每个时间点输入一个句子中的单词然后模型会预测出我下一个单词是什么，比如在t2时刻我输入了x2(我)，这个时候我期望得到的单词是“昨天”。这里举例的这个场景是rnn应用的一个小点，其它还包括机器翻译等。下面基于预测单词的要求来看看完成循环神经网络是什么样的。</p>
<h3 id="输入数据"><a href="#输入数据" class="headerlink" title="输入数据"></a>输入数据</h3><p>既然是预测单词，那我们的输入也是单词的内容。这里以一个句子作为一条训练样本，以句子中的单词作为具体的输入信息。可以看做单词是句子这个训练样本中的元素。按照上面的说法我们还需要给句子标注出开始和结束标志。我们把句子中的没有包含结束标志的部分可以当作是输入数据，把句子中没有包含开始标志的部分看做是验证模型预测输出数据是否正确的标准。因为我们并不能直接把一个单词作为输入或者输出。所以我们需要建立一个词汇表，就像是字典一样，在词汇表中将位置信息和具体的词汇进行映射。将映射后的信息比如[0 0 0 0 1 0 0]就代表“我”，传入到模型中去。</p>
<h3 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h3><p>在隐藏层中我们包含了一个循环，处于隐藏层的神经元接受当前时候的输入$ x_{t} $和上一个时刻的神经元的状态$ s_{t-1} $作为本次的输入，然后经过tanh激活函数进行激活后输出，同时也作为下一个时刻神经元的输入$ s_{t} $。</p>
<p>$ tanh(z) = \frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$<br>$ h_{t}=Ux_{t}+Ws_{t-1} $<br>$ s_{t}=tanh(Ux_{t}+Ws_{t-1}) $</p>
<p>tanh函数的图像是这样，和sigmoid很类似，但是tanh是关于0旋转对称的，它的取值范围是[-1,1]而sigmoid的取值范围是[0,1]。sigmoid的导数是$ {f}’(z)=f(z)(1-f(z)) $而tanh的导数$ {f}’(z)=1-(f(z))^{2} $</p>
<p><img src="/image/tanh.png"></p>
<h3 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h3><p>输出层的输入信息是从隐藏层来的，隐藏层输出的状态通过权值矩阵V处理以后输入到输出层softmax，这里V的主要作用是将隐藏层输出的矩阵信息转换为输出层可以处理的向量信息信息，因为输出层输出的是向量。softmax的公式是<br>$$ \sigma (z)_{j}=\frac{e^{z_{j}}}{\sum_{k=1}^{K}e^{z_{k}}} $$<br>可以看出通过softmax层输出后的是模型认为在这个位置应该出现的单词的概率。也就是说softmax输出的数据中值最大的那个就是模型预测的值。</p>
]]></content>
      
        <categories>
            
            <category> deep leaning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> rnn </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[卷积神经网络简略笔记]]></title>
      <url>/2017/05/10/cnnNote/</url>
      <content type="html"><![CDATA[<h2 id="卷积神经网络与神经网络"><a href="#卷积神经网络与神经网络" class="headerlink" title="卷积神经网络与神经网络"></a>卷积神经网络与神经网络</h2><p>神经网络和卷积神经网络的关系可以理解过卷积神经网络是为了可以更好处理图片而对神经网络进行了相应的改造和特殊化。这里以卷积来为这个新的神经网络进行命名，可能是卷积神经网络的卷积层在对原始数据的处理上类似于卷积操作。但是如果本身对数学上的卷积操作不怎么了解，其实也不影响对卷积神经网络的理解。<br><a id="more"></a></p>
<h2 id="神经网络处理图片的问题"><a href="#神经网络处理图片的问题" class="headerlink" title="神经网络处理图片的问题"></a>神经网络处理图片的问题</h2><p>我们可以使用神经网络来处理图片，对图片进行学习分类。但是神经网络可能对处理尺寸较小的，不复杂的图片处理更擅长。对于具有相反特点的图片神经网络的学习效果就不理想了。其中的原因包括有神经网络会将图片的所有像素信息都传入到神经网络的输入层中，当图片过大或者需要进行更复杂的分类时会导致模型中出现大量的参数。这样直接导致的是计算量变得极大最终难以计算。而且为了进行更复杂的判断新增的隐藏又可能导致梯度消失的情况。</p>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>为了减少上面问题的影响，发展出了卷积神经网络。最主要的特点是添加了卷积层来对图片进行处理。卷积神经网络模型的样子如下：</p>
<p><img src="/image/simple_conv.png" width="50%" height="50%/"></p>
<p>简单说下上图中convolutionallayer就是卷积层，主要作用是通过过滤器来过滤出原始图片的特征，一般一个卷积层有多个过滤器所以也就能得到多个特征来。接下来是poolinglayer是池化层主要是将卷积层产生的特征信息进行采样以进一步减小图片的尺寸。接着将池化后的数据输入到全连接层，这里的全连接层就和神经网络是一样，接着是通过输出层得到分类结果。</p>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>在神经网络中我们一般使用sigmod作为激活函数，它的图像如下</p>
<p><img src="/image/sigmoid.png" width="50%" height="50%/"></p>
<p>这里可以看出来在偏离图像的中心点0越远则图像的变换的趋势越加缓慢，如果在初始化w和b的过程可能某些神经元获得了一个更大的值。这就引起了梯度的下降变的缓慢解决的方式可以选择换一个激活函数比如relu函数，它的图像如下：</p>
<p><img src="/image/relu.png" width="50%" height="50%"></p>
<h3 id="代价函数-sigmoid-交叉熵"><a href="#代价函数-sigmoid-交叉熵" class="headerlink" title="代价函数 sigmoid+交叉熵"></a>代价函数 sigmoid+交叉熵</h3><p>在先前使用sigmod作为激活函数的神经网络中我们使用代价函数的模式一般是平方差的形式。代价函数的形式是会依赖对激活函数求导，我们知道求导对应到曲线可以理解为曲线在某点的变化率。以下通过一个一个输入一个输出的单个神经元模型来进行解释。以下输入假设x是1，期待y输出为0可以看出求导的结果确实依赖了对sigmod的求导，这样就会出现上面讨论的梯度下降缓慢的问题。<br>$$ C=\frac{(y-a)^{2}}{2} $$</p>
<p>$$ \frac{\partial C}{\partial w}=(a-y){\sigma}’(z)x=a{\sigma}’(z) $$</p>
<p>$$ \frac{\partial C}{\partial b}=(a-y){\sigma}’(z)=a{\sigma}’(z) $$</p>
<p>通过使用交叉熵来作为代价函数：</p>
<p>$$ C=-\frac{1}{n}\sum_{x}[ylna+(1-y)ln(1-a)] $$</p>
<p>可以对w和b分别求导发现结果是和激活函数的导数没有关系的。</p>
<h3 id="softmax-log-likelihood"><a href="#softmax-log-likelihood" class="headerlink" title="softmax+log-likelihood"></a>softmax+log-likelihood</h3><p>除了交叉熵以外还可以通过softmax的方式来解决学习速度衰减的问题。我们仅将输出层从普通的sigmod作为激活函数的层替换为softmax层。softmax输出层同样接受z=wx+b然后通过以下公式来计算输出结果</p>
<p>$$ a_{j}^{L}=\frac{e^{z_{j}^{L}}}{\sum_{k}e^{z_{k}^{L}}} $$</p>
<p>可以看出来这里得到的是某个值占总体的一个比例。配合softmax我们的代价函数需要替换成log-likelihood</p>
<p>$$ C\equiv-lna_{y}^{L} $$</p>
<p>这里表示的是单个输入样本的代价，如果有多个样本的可以对他们的代价求均值，作为总的代价函数。通过代价函数对w和b求导得到公式</p>
<p>$$ \frac{\partial C}{\partial b_{j}^{L}}=a_{j}^{L}-y_{j} $$</p>
<p>$$ \frac{\partial C}{\partial w_{jk}^{L}}=a_{k}^{L-1}(a_{k}^{L}-y_{j}) $$</p>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>一般有l1,l2,dropout的方式来对模型进行正则化，主要目的还是防止模型的过拟合，其中l2正则化方法是对所有的w进行如下处理并把这个部分添加到代价函数中去需要注意的是l2正则化只需包括w不需要包括b。</p>
<p>$$ C=-\frac{1}{n}\sum_{xj}[y_{j}lna_{j}^{L}+(1-y_{j})ln(1-a_{j}^{L})]+\frac{\lambda}{2n}\sum_{w}w^{2} $$</p>
<p>这样起到的作用使的优化这个代价函数，会更倾向于获取一个w并不是那么复杂的模型。一般直观的来看过拟合的模型都是在训练集中表现过于好的函数，而表现的过好的模型一般w都是比较复杂的。我们对于l1正则化不作解释。</p>
<p>接下还有一种方式是dropout，可以理解为丢弃。处理的思想类似我们以前看的集成学习法。这dropout中会随机的丢弃一些神经元(非输入和输出层)，也就是我们每次迭代更新梯度只对模型中的部分神经元进行处理。在一个batch的样本训练并更新完w和b以后我们会重新再进行一次dropout。这样的感觉就像训练了多个子神经网络，最后将他们组合成一个大的神经元来进行使用。是不是和集成学习思想类似？</p>
<h3 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h3><p>在先前的神经网络中一般采用的是符合一个均值是0，标准差是1的高斯分布的随机数去初始化w和b，但是这个并不适合初始化隐藏层比较多的神经网络。一般是将标准差表示为 $1/\sqrt{n_{in}}$ 这里的 $n_{in}$ 表示具有输入权重的神经元个数。</p>
<h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>卷积层是卷积神经网络不同于神经网络的一个重要特点。首先我们输入到卷积神经网络的依然是一个图片，但是作为接受这个数据的卷积层并不是如果神经网络一样将图片的所有像素信息都输入，而是使用一个比输入图片小的多的过滤器来扫描原始输入的图片数据。过滤器可以理解为将原始图片的一小部分作为输入并通过权值和偏移进行计算后输入激活函数得到在新的数据。这个过滤器会按照设置每次滑动并重复刚才的过程。最终得到的就是对应一个过滤器得到的一个特征map。这里需要特别说明单独过滤器滑动的任何区域计算使用的w和b都是一样的，一般的讲这个可以称为<strong>参数共享</strong>。这样的好处是识别某个特征将和这个特征在原始图片出现的位置无关。</p>
<p><img src="/image/no_padding.gif" width="50%" height="50%/"></p>
<p>可以直观的看到经过过滤器处理以后的图像的大小变小了，同时多个过滤器也可以获得到图片的多种特征。同时可以看出来我们需要仔细设计过滤器的大小以及每次滑动的距离以使的在原始图像中每个像素都可以涉及到。但是可能真就存在冲突的情况或者我希望过滤器处理后图片的大小不发生改变，这个时候可以适当的在待过滤的图片边缘添加0来实现。</p>
<p><img src="/image/padding.gif" width="50%" height="50%/"></p>
<p>这些处理得到特征map一般还需要通过池化来进行采样，其中一个目的就是进一步减小图片大小。一般的池化方法就是最大池化。比如使用2*2大小的框格在特制map上每次移动2格，将22在特征map范围上的最大的数据返回。这样处理以后的数据可能是作为下一个卷积层的输入也可能作为全连接层的输入。</p>
]]></content>
      
        <categories>
            
            <category> deep leaning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> cnn </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
