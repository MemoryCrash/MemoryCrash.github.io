<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[Transfer Learning tutorial(翻译)]]></title>
      <url>/2017/05/21/TransLearning/</url>
      <content type="html"><![CDATA[<p>原文链接：<a href="http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html" target="_blank" rel="external">Transfer Learning tutorial</a><br>作者：<a href="https://chsasank.github.io" target="_blank" rel="external">Sasank Chilamkurthy</a><br><a id="more"></a></p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> transfer learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Learning PyTorch with Example(翻译)]]></title>
      <url>/2017/05/20/pytorch1/</url>
      <content type="html"><![CDATA[<p>原文链接：<a href="http://pytorch.org/tutorials/beginner/pytorch_with_examples.html" target="_blank" rel="external">Learning PyTorch with Examples</a><br>作者：<a href="https://github.com/jcjohnson/pytorch-examples" target="_blank" rel="external">Justin Johnos</a></p>
<p>这份学习指导通过下面的例子介绍<a href="https://github.com/pytorch/pytorch" target="_blank" rel="external">PyTorch</a>的一些基本概念。<br>PyTorch提供了以下两个核心的特性：   </p>
<ul>
<li>一个和numpy相似的多维Tensor(张量)，但可在GPU上执行</li>
<li>对构建的神经网络进行自动求导<a id="more"></a>
</li>
</ul>
<p>我们讲解的例子为激活函数是ReLU的全连接神经网络。这个神经网络将只包含一个隐藏层，使用欧式距离作为输出值和实际值度量，我们将使用梯度下降去最小化欧式距离代价函数。</p>
<h2 id="Tensor-张量"><a href="#Tensor-张量" class="headerlink" title="Tensor(张量)"></a>Tensor(张量)</h2><h3 id="热身-numpy"><a href="#热身-numpy" class="headerlink" title="热身:numpy"></a>热身:numpy</h3><p>在正式介绍PyTorch前，我们先通过numpy来实现这个神经网络。<br>Numpy提供了一个n维数组对象，并提供了许多函数来操作这个数组。Numpy是一个通用科学运算框架。它本身对计算图，深度学习和梯度是一无所知。然而我们可以轻松的通过随机数据去训练一个使用numpy实现了前向和后向传播的神经网络。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random input and output data</span></div><div class="line">x = np.random.randn(N, D_in)</div><div class="line">y = np.random.randn(N, D_out)</div><div class="line"></div><div class="line"><span class="comment"># Randomly initialize weights</span></div><div class="line">w1 = np.random.randn(D_in, H)</div><div class="line">w2 = np.random.randn(H, D_out)</div><div class="line"></div><div class="line">learning_rate = <span class="number">1e-6</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: compute predicted y</span></div><div class="line">    h = x.dot(w1)</div><div class="line">    h_relu = np.maximum(h, <span class="number">0</span>)</div><div class="line">    y_pred = h_relu.dot(w2)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss</span></div><div class="line">    loss = np.square(y_pred - y).sum()</div><div class="line">    print(t, loss)</div><div class="line"></div><div class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></div><div class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</div><div class="line">    grad_w2 = h_relu.T.dot(grad_y_pred)</div><div class="line">    grad_h_relu = grad_y_pred.dot(w2.T)</div><div class="line">    grad_h = grad_h_relu.copy()</div><div class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></div><div class="line">    grad_w1 = x.T.dot(grad_h)</div><div class="line"></div><div class="line">    <span class="comment"># Update weights</span></div><div class="line">    w1 -= learning_rate * grad_w1</div><div class="line">    w2 -= learning_rate * grad_w2</div></pre></td></tr></table></figure></p>
<h3 id="PyTorch-Tensor"><a href="#PyTorch-Tensor" class="headerlink" title="PyTorch:Tensor"></a>PyTorch:Tensor</h3><p>Numpy是一个伟大的框架，但是它并不能利用GPUs来对数学计算加速。对于现代的深度神经网络，GPUs通常可以提供50倍甚至更多的提速，因此很可惜numpy不能满足现代深度学习的需求。</p>
<p>接下来我们介绍一个最重要的PyTorch概念:<strong>Tensor(张量)</strong>。在概念上PyTorch的Tensor和numpy的array相似：Tensor是一个n维数组，PyTorch提供了很多函数去操作它。和numpy的array一样，PyTorch Tensor同样对深度学习或者计算图或者梯度等概念一无所知；它同样是一个通用的科学运算工具。</p>
<p>然而不似numpy，PyTorch Tensor 能利用GPUs去加速他们的数学运算。为了可以在GPU上运行PyTorch Tensor，你只需要将它简单的转换为一个新的数据类型。</p>
<p>下面我们使用Pytorch Tensor去训练一个两层的神经网络。像上面的numpy的例子我们需要手动实现神经网络的前向和后向传播。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"></div><div class="line"></div><div class="line">dtype = torch.FloatTensor</div><div class="line"><span class="comment"># 取消下面的注释，程序运行在GPU上</span></div><div class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random input and output data</span></div><div class="line">x = torch.randn(N, D_in).type(dtype)</div><div class="line">y = torch.randn(N, D_out).type(dtype)</div><div class="line"></div><div class="line"><span class="comment"># Randomly initialize weights</span></div><div class="line">w1 = torch.randn(D_in, H).type(dtype)</div><div class="line">w2 = torch.randn(H, D_out).type(dtype)</div><div class="line"></div><div class="line">learning_rate = <span class="number">1e-6</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: compute predicted y</span></div><div class="line">    h = x.mm(w1)</div><div class="line">    h_relu = h.clamp(min=<span class="number">0</span>)</div><div class="line">    y_pred = h_relu.mm(w2)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss</span></div><div class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</div><div class="line">    print(t, loss)</div><div class="line"></div><div class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></div><div class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</div><div class="line">    grad_w2 = h_relu.t().mm(grad_y_pred)</div><div class="line">    grad_h_relu = grad_y_pred.mm(w2.t())</div><div class="line">    grad_h = grad_h_relu.clone()</div><div class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></div><div class="line">    grad_w1 = x.t().mm(grad_h)</div><div class="line"></div><div class="line">    <span class="comment"># Update weights using gradient descent</span></div><div class="line">    w1 -= learning_rate * grad_w1</div><div class="line">    w2 -= learning_rate * grad_w2</div></pre></td></tr></table></figure>
<h2 id="Autograd-自动求导"><a href="#Autograd-自动求导" class="headerlink" title="Autograd(自动求导)"></a>Autograd(自动求导)</h2><h3 id="PyTorch-Variables-and-autograd"><a href="#PyTorch-Variables-and-autograd" class="headerlink" title="PyTorch:Variables and autograd"></a>PyTorch:Variables and autograd</h3><p>在上面的例子中，我们不得不实现神经网络的前向和后向传播。手动实现后向传播对于一个简单的两层神经网络并不是一个困难的事情，但对于一个大型复杂的神经网络将变的非常困难。</p>
<p>幸运的是，我们可以使用<a href="https://en.wikipedia.org/wiki/Automatic_differentiation" target="_blank" rel="external">自动求导</a>来自动的进行神经网络的反向传播过程。在PyTorch中的<strong>autograd</strong>包提供了此功能。当使用自动求导时，神经网络的前向传播将会定义<strong>计算图</strong>；在计算图中的节点就是Tensor，然后连线就是根据输入Tensor产生输出Tensor的函数。反向传播过程通过这个计算图可以很容易计算梯度。</p>
<p>这听起来似乎很复杂，但在练习中使用起来却很简单。我们将PyTorch的Tensor包含在<strong>Variable</strong>对象中；Variable在计算图中实际表示一个节点。如果x是一个Variable那么x.data是Tensor，x.grad是另外一个保存了x的标量梯度值的Variable。</p>
<p>PyTorch Variable拥有和PyTorch Tensor相同的API：(大部分)任何你在Tensor上可以做的操作同样可以在Variable生效；不同点在于，使用Variable可以定义计算图，允许你自动的计算梯度。</p>
<p>下面是我们使用PyTorch Variable和自动梯度计算实现的两层神经网络：现在我们不再需要手动实现神经网络的反向传播。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line">dtype = torch.FloatTensor</div><div class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold input and outputs, and wrap them in Variables.</span></div><div class="line"><span class="comment"># Setting requires_grad=False indicates that we do not need to compute gradients</span></div><div class="line"><span class="comment"># with respect to these Variables during the backward pass.</span></div><div class="line">x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=<span class="keyword">False</span>)</div><div class="line">y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors for weights, and wrap them in Variables.</span></div><div class="line"><span class="comment"># Setting requires_grad=True indicates that we want to compute gradients with</span></div><div class="line"><span class="comment"># respect to these Variables during the backward pass.</span></div><div class="line"><span class="comment"># 需要计算梯度的Variable需要将requires_grad置为True</span></div><div class="line">w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=<span class="keyword">True</span>)</div><div class="line">w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">learning_rate = <span class="number">1e-6</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: compute predicted y using operations on Variables; these</span></div><div class="line">    <span class="comment"># are exactly the same operations we used to compute the forward pass using</span></div><div class="line">    <span class="comment"># Tensors, but we do not need to keep references to intermediate values since</span></div><div class="line">    <span class="comment"># we are not implementing the backward pass by hand.</span></div><div class="line">    <span class="comment"># mm表示矩阵乘法</span></div><div class="line">    y_pred = x.mm(w1).clamp(min=<span class="number">0</span>).mm(w2)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss using operations on Variables.</span></div><div class="line">    <span class="comment"># Now loss is a Variable of shape (1,) and loss.data is a Tensor of shape</span></div><div class="line">    <span class="comment"># (1,); loss.data[0] is a scalar value holding the loss.</span></div><div class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Use autograd to compute the backward pass. This call will compute the</span></div><div class="line">    <span class="comment"># gradient of loss with respect to all Variables with requires_grad=True.</span></div><div class="line">    <span class="comment"># After this call w1.grad and w2.grad will be Variables holding the gradient</span></div><div class="line">    <span class="comment"># of the loss with respect to w1 and w2 respectively.</span></div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    <span class="comment"># Update weights using gradient descent; w1.data and w2.data are Tensors,</span></div><div class="line">    <span class="comment"># w1.grad and w2.grad are Variables and w1.grad.data and w2.grad.data are</span></div><div class="line">    <span class="comment"># Tensors.</span></div><div class="line">    w1.data -= learning_rate * w1.grad.data</div><div class="line">    w2.data -= learning_rate * w2.grad.data</div><div class="line"></div><div class="line">    <span class="comment"># Manually zero the gradients after updating weights</span></div><div class="line">    w1.grad.data.zero_()</div><div class="line">    w2.grad.data.zero_()</div></pre></td></tr></table></figure>
<h3 id="PyTorch-定义一个新的自动梯度计算函数"><a href="#PyTorch-定义一个新的自动梯度计算函数" class="headerlink" title="PyTorch:定义一个新的自动梯度计算函数"></a>PyTorch:定义一个新的自动梯度计算函数</h3><p>掩盖在自动计算梯度下，实际都包含着两个关于Tensor的基本函数操作。一是<strong>前向</strong>函数通过输入的Tensor计算输出Tensor，一是<strong>反向</strong>函数接受输出Tensor的标量梯度，然后根据这些值计算输入Tensor的梯度。</p>
<p>在PyTorch中我们可以轻易的定义我们自己的自动计算梯度操作，通过重定义<strong>torch.autograd.Fuction</strong>然后实现<strong>forward</strong>和<strong>backward</strong>函数。然后我们可以构造一个实例，然后像函数一样去调用它，通过Variable保存输入数据。</p>
<p>在下面的例子中我们自定义了一个ReLu的自动梯度计算函数，使用它来实现我们的两层神经网络：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyReLU</span><span class="params">(torch.autograd.Function)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    We can implement our own custom autograd Functions by subclassing</div><div class="line">    torch.autograd.Function and implementing the forward and backward passes</div><div class="line">    which operate on Tensors.</div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        In the forward pass we receive a Tensor containing the input and return a</div><div class="line">        Tensor containing the output. You can cache arbitrary Tensors for use in the</div><div class="line">        backward pass using the save_for_backward method.</div><div class="line">        """</div><div class="line">        self.save_for_backward(input)</div><div class="line">        <span class="keyword">return</span> input.clamp(min=<span class="number">0</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad_output)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        In the backward pass we receive a Tensor containing the gradient of the loss</div><div class="line">        with respect to the output, and we need to compute the gradient of the loss</div><div class="line">        with respect to the input.</div><div class="line">        """</div><div class="line">        <span class="comment"># 这里说明下ReLU在进行反向传播的时候对于是0的部分是没有梯度的。</span></div><div class="line">        input, = self.saved_tensors</div><div class="line">        grad_input = grad_output.clone()</div><div class="line">        grad_input[input &lt; <span class="number">0</span>] = <span class="number">0</span></div><div class="line">        <span class="keyword">return</span> grad_input</div><div class="line"></div><div class="line"></div><div class="line">dtype = torch.FloatTensor</div><div class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold input and outputs, and wrap them in Variables.</span></div><div class="line">x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=<span class="keyword">False</span>)</div><div class="line">y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors for weights, and wrap them in Variables.</span></div><div class="line">w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=<span class="keyword">True</span>)</div><div class="line">w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">learning_rate = <span class="number">1e-6</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Construct an instance of our MyReLU class to use in our network</span></div><div class="line">    relu = MyReLU()</div><div class="line"></div><div class="line">    <span class="comment"># Forward pass: compute predicted y using operations on Variables; we compute</span></div><div class="line">    <span class="comment"># ReLU using our custom autograd operation.</span></div><div class="line">    y_pred = relu(x.mm(w1)).mm(w2)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss</span></div><div class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Use autograd to compute the backward pass.</span></div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    <span class="comment"># Update weights using gradient descent</span></div><div class="line">    w1.data -= learning_rate * w1.grad.data</div><div class="line">    w2.data -= learning_rate * w2.grad.data</div><div class="line"></div><div class="line">    <span class="comment"># Manually zero the gradients after updating weights</span></div><div class="line">    w1.grad.data.zero_()</div><div class="line">    w2.grad.data.zero_()</div></pre></td></tr></table></figure>
<h3 id="TensorFlow-Static-Graphs"><a href="#TensorFlow-Static-Graphs" class="headerlink" title="TensorFlow:Static Graphs"></a>TensorFlow:Static Graphs</h3><p>PyTorch的autograd看起来和TensorFlow很相似：在这两个框架我们都定义计算图，然后使用自动微分技术计算梯度。两者计算图最大的区别是TenorFlow的计算图是<strong>静态</strong>的PyTorch是<strong>动图</strong>的计算图。</p>
<p>在TensorFlow中，我们定义好计算图后可以一边又一边的执行计算图，输入不同的输入数据给它。在PyTorch中，每个前向操作定义一个新的计算图。</p>
<p>静态图优势在于你可以先期进行图的优化；举个例讲一个框架可以决定有效的融合一些图操作，或者提出跨机器或者GPUs的分布式计算图策略。如果你一次次的重复使用一个计算图，那么这个潜在的前期图优化代价，将分散到每次的重复上(这段翻译的有点问题)。</p>
<p>一方面静态和动态的计算图结构差别还在控制流上。在一些模型中我们希望对每个数据点做梯度计算;比如在循环神经网络中一个数据点我们会按照时间步骤来展开；这些展开可以作为循环来计算。静态图需要将这个循环结构变成计算图的一部分；因此TensorFlow提供来一个<strong>tf.scan</strong>操作来将循环结构嵌入到计算图中。在这个情况下使用动态图处理上会更简单：因为我们是在运行时创建计算图，我们使用一般必要的流控方式来计算不同输入数据。</p>
<p>同上面的PyTorch自动梯度计算例子相比，下面是我们使用TensorFlow来实现一个简单的两层网络：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># First we set up the computational graph:</span></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create placeholders for the input and target data; these will be filled</span></div><div class="line"><span class="comment"># with real data when we execute the graph.</span></div><div class="line">x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, D_in))</div><div class="line">y = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, D_out))</div><div class="line"></div><div class="line"><span class="comment"># Create Variables for the weights and initialize them with random data.</span></div><div class="line"><span class="comment"># A TensorFlow Variable persists its value across executions of the graph.</span></div><div class="line">w1 = tf.Variable(tf.random_normal((D_in, H)))</div><div class="line">w2 = tf.Variable(tf.random_normal((H, D_out)))</div><div class="line"></div><div class="line"><span class="comment"># Forward pass: Compute the predicted y using operations on TensorFlow Tensors.</span></div><div class="line"><span class="comment"># Note that this code does not actually perform any numeric operations; it</span></div><div class="line"><span class="comment"># merely sets up the computational graph that we will later execute.</span></div><div class="line">h = tf.matmul(x, w1)</div><div class="line">h_relu = tf.maximum(h, tf.zeros(<span class="number">1</span>))</div><div class="line">y_pred = tf.matmul(h_relu, w2)</div><div class="line"></div><div class="line"><span class="comment"># Compute loss using operations on TensorFlow Tensors</span></div><div class="line">loss = tf.reduce_sum((y - y_pred) ** <span class="number">2.0</span>)</div><div class="line"></div><div class="line"><span class="comment"># Compute gradient of the loss with respect to w1 and w2.</span></div><div class="line">grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])</div><div class="line"></div><div class="line"><span class="comment"># Update the weights using gradient descent. To actually update the weights</span></div><div class="line"><span class="comment"># we need to evaluate new_w1 and new_w2 when executing the graph. Note that</span></div><div class="line"><span class="comment"># in TensorFlow the the act of updating the value of the weights is part of</span></div><div class="line"><span class="comment"># the computational graph; in PyTorch this happens outside the computational</span></div><div class="line"><span class="comment"># graph.</span></div><div class="line">learning_rate = <span class="number">1e-6</span></div><div class="line">new_w1 = w1.assign(w1 - learning_rate * grad_w1)</div><div class="line">new_w2 = w2.assign(w2 - learning_rate * grad_w2)</div><div class="line"></div><div class="line"><span class="comment"># Now we have built our computational graph, so we enter a TensorFlow session to</span></div><div class="line"><span class="comment"># actually execute the graph.</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    <span class="comment"># Run the graph once to initialize the Variables w1 and w2.</span></div><div class="line">    sess.run(tf.global_variables_initializer())</div><div class="line"></div><div class="line">    <span class="comment"># Create numpy arrays holding the actual data for the inputs x and targets</span></div><div class="line">    <span class="comment"># y</span></div><div class="line">    x_value = np.random.randn(N, D_in)</div><div class="line">    y_value = np.random.randn(N, D_out)</div><div class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">        <span class="comment"># Execute the graph many times. Each time it executes we want to bind</span></div><div class="line">        <span class="comment"># x_value to x and y_value to y, specified with the feed_dict argument.</span></div><div class="line">        <span class="comment"># Each time we execute the graph we want to compute the values for loss,</span></div><div class="line">        <span class="comment"># new_w1, and new_w2; the values of these Tensors are returned as numpy</span></div><div class="line">        <span class="comment"># arrays.</span></div><div class="line">        loss_value, _, _ = sess.run([loss, new_w1, new_w2],</div><div class="line">                                    feed_dict=&#123;x: x_value, y: y_value&#125;)</div><div class="line">        print(loss_value)</div></pre></td></tr></table></figure></p>
<h2 id="nn-module-神经网络模块"><a href="#nn-module-神经网络模块" class="headerlink" title="nn module(神经网络模块)"></a>nn module(神经网络模块)</h2><h3 id="PyTorch-nn"><a href="#PyTorch-nn" class="headerlink" title="PyTorch:nn"></a>PyTorch:nn</h3><p>计算图和自动梯度计算是非常有用的范式去定义复杂的操作和自动的执行导数计算；然而对于大型神经网络来说这样未加处理的自动梯度计算就显的太低层了。</p>
<p>当构建一个神经网络时，我们经常考虑在<strong>layer(层)</strong>中安排计算，这些层中有些包含了需要在学习过程中需要进行优化的<strong>leanable parameters(可学习参数)</strong>。</p>
<p>在TensorFlow中，提供了像<strong>Keras</strong>,<strong>TensorFlow-Slim</strong>,和<strong>TFLearn</strong>包他们对原始计算图方式进行了高层抽象，对于神经网络的构建非常有用。</p>
<p>在PyTorch，nn包为了相同的目的诞生。nn包定义了一个<strong>Modules</strong>集，它门大致等于神经网络的层。一个Module接受输入的Variable然后计算输出Variable，但也可以保存中间状态就像Variable包含可学习参数。nn包也定义了在训练神经网络时常用的损失函数集。</p>
<p>在这个例子中我们使用nn包去实现我们的两层神经网络：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables.</span></div><div class="line">x = Variable(torch.randn(N, D_in))</div><div class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Use the nn package to define our model as a sequence of layers. nn.Sequential</span></div><div class="line"><span class="comment"># is a Module which contains other Modules, and applies them in sequence to</span></div><div class="line"><span class="comment"># produce its output. Each Linear Module computes output from input using a</span></div><div class="line"><span class="comment"># linear function, and holds internal Variables for its weight and bias.</span></div><div class="line">model = torch.nn.Sequential(</div><div class="line">    torch.nn.Linear(D_in, H),</div><div class="line">    torch.nn.ReLU(),</div><div class="line">    torch.nn.Linear(H, D_out),</div><div class="line">)</div><div class="line"></div><div class="line"><span class="comment"># The nn package also contains definitions of popular loss functions; in this</span></div><div class="line"><span class="comment"># case we will use Mean Squared Error (MSE) as our loss function.</span></div><div class="line">loss_fn = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</div><div class="line"></div><div class="line">learning_rate = <span class="number">1e-4</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: compute predicted y by passing x to the model. Module objects</span></div><div class="line">    <span class="comment"># override the __call__ operator so you can call them like functions. When</span></div><div class="line">    <span class="comment"># doing so you pass a Variable of input data to the Module and it produces</span></div><div class="line">    <span class="comment"># a Variable of output data.</span></div><div class="line">    y_pred = model(x)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss. We pass Variables containing the predicted and true</span></div><div class="line">    <span class="comment"># values of y, and the loss function returns a Variable containing the</span></div><div class="line">    <span class="comment"># loss.</span></div><div class="line">    loss = loss_fn(y_pred, y)</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Zero the gradients before running the backward pass.</span></div><div class="line">    model.zero_grad()</div><div class="line"></div><div class="line">    <span class="comment"># Backward pass: compute gradient of the loss with respect to all the learnable</span></div><div class="line">    <span class="comment"># parameters of the model. Internally, the parameters of each Module are stored</span></div><div class="line">    <span class="comment"># in Variables with requires_grad=True, so this call will compute gradients for</span></div><div class="line">    <span class="comment"># all learnable parameters in the model.</span></div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    <span class="comment"># Update the weights using gradient descent. Each parameter is a Variable, so</span></div><div class="line">    <span class="comment"># we can access its data and gradients like we did before.</span></div><div class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</div><div class="line">        param.data -= learning_rate * param.grad.data</div></pre></td></tr></table></figure></p>
<h3 id="PyTorch-optim"><a href="#PyTorch-optim" class="headerlink" title="PyTorch:optim"></a>PyTorch:optim</h3><p>到目前为止我们通过手动的转换Vraiable保存的可学习参数<strong>.data</strong>来更新模型权重。这个并不是一个大的负担对于这个简单的随机梯度下降算法，但是在练习中我们通常使用更复杂的优化器比如AdaGrad，RMSProp，Adam等。</p>
<p>在PyTorch中optim包抽象了优化算法的想法并提供了通常使用的优化算法。</p>
<p>下面的例子中我们像先前一样使用nn包去定义我们的模型，但是我们使用optim包的Adam算法去优化我们的模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables.</span></div><div class="line">x = Variable(torch.randn(N, D_in))</div><div class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Use the nn package to define our model and loss function.</span></div><div class="line">model = torch.nn.Sequential(</div><div class="line">    torch.nn.Linear(D_in, H),</div><div class="line">    torch.nn.ReLU(),</div><div class="line">    torch.nn.Linear(H, D_out),</div><div class="line">)</div><div class="line">loss_fn = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Use the optim package to define an Optimizer that will update the weights of</span></div><div class="line"><span class="comment"># the model for us. Here we will use Adam; the optim package contains many other</span></div><div class="line"><span class="comment"># optimization algoriths. The first argument to the Adam constructor tells the</span></div><div class="line"><span class="comment"># optimizer which Variables it should update.</span></div><div class="line">learning_rate = <span class="number">1e-4</span></div><div class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: compute predicted y by passing x to the model.</span></div><div class="line">    y_pred = model(x)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss.</span></div><div class="line">    loss = loss_fn(y_pred, y)</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Before the backward pass, use the optimizer object to zero all of the</span></div><div class="line">    <span class="comment"># gradients for the variables it will update (which are the learnable weights</span></div><div class="line">    <span class="comment"># of the model)</span></div><div class="line">    optimizer.zero_grad()</div><div class="line"></div><div class="line">    <span class="comment"># Backward pass: compute gradient of the loss with respect to model</span></div><div class="line">    <span class="comment"># parameters</span></div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    <span class="comment"># Calling the step function on an Optimizer makes an update to its</span></div><div class="line">    <span class="comment"># parameters</span></div><div class="line">    optimizer.step()</div></pre></td></tr></table></figure>
<h3 id="PyTorch-Custom-nn-Modules"><a href="#PyTorch-Custom-nn-Modules" class="headerlink" title="PyTorch:Custom nn Modules"></a>PyTorch:Custom nn Modules</h3><p>有些时候你可能像实现一个比一系列已经存在的Modules更复杂的模型；基于此你可以通过继承nn.Module然后定义forward来接受输入的Variable，使用其它模块或者别的自动梯度计算方式来生成Variable。</p>
<p>在这个例子中我们实现了我们两层的神经网络作为一个定制的Module子类：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayerNet</span><span class="params">(torch.nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        In the constructor we instantiate two nn.Linear modules and assign them as</div><div class="line">        member variables.</div><div class="line">        """</div><div class="line">        super(TwoLayerNet, self).__init__()</div><div class="line">        self.linear1 = torch.nn.Linear(D_in, H)</div><div class="line">        self.linear2 = torch.nn.Linear(H, D_out)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        In the forward function we accept a Variable of input data and we must return</div><div class="line">        a Variable of output data. We can use Modules defined in the constructor as</div><div class="line">        well as arbitrary operators on Variables.</div><div class="line">        """</div><div class="line">        h_relu = self.linear1(x).clamp(min=<span class="number">0</span>)</div><div class="line">        y_pred = self.linear2(h_relu)</div><div class="line">        <span class="keyword">return</span> y_pred</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables</span></div><div class="line">x = Variable(torch.randn(N, D_in))</div><div class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Construct our model by instantiating the class defined above</span></div><div class="line">model = TwoLayerNet(D_in, H, D_out)</div><div class="line"></div><div class="line"><span class="comment"># Construct our loss function and an Optimizer. The call to model.parameters()</span></div><div class="line"><span class="comment"># in the SGD constructor will contain the learnable parameters of the two</span></div><div class="line"><span class="comment"># nn.Linear modules which are members of the model.</span></div><div class="line">criterion = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</div><div class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-4</span>)</div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: Compute predicted y by passing x to the model</span></div><div class="line">    y_pred = model(x)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss</span></div><div class="line">    loss = criterion(y_pred, y)</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Zero gradients, perform a backward pass, and update the weights.</span></div><div class="line">    optimizer.zero_grad()</div><div class="line">    loss.backward()</div><div class="line">    optimizer.step()</div></pre></td></tr></table></figure></p>
<h3 id="PyTorch-Control-Flow-Weight-Sharing"><a href="#PyTorch-Control-Flow-Weight-Sharing" class="headerlink" title="PyTorch:Control Flow + Weight Sharing"></a>PyTorch:Control Flow + Weight Sharing</h3><p>作为一个动态图和共享权重的例子，我们实现了一个非常奇怪的模型：一个ReLU作为激活函数的全连接层神经网络，它的每个前向传播选择0到4之间的一个随机数作为需要使用的中间层数，重复使用相同的权重多次去计算隐藏层(这段翻译的不好没有怎么理解)。</p>
<p>对这样的模型我们使用普通的Python流控去实现循环，然后在定义前向传播时在最深处的层通过简单的重复使用相同模块多次实现权重共享。</p>
<p>我们能轻易的实现这个模型通过继承Module。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DynamicNet</span><span class="params">(torch.nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        In the constructor we construct three nn.Linear instances that we will use</div><div class="line">        in the forward pass.</div><div class="line">        """</div><div class="line">        super(DynamicNet, self).__init__()</div><div class="line">        self.input_linear = torch.nn.Linear(D_in, H)</div><div class="line">        self.middle_linear = torch.nn.Linear(H, H)</div><div class="line">        self.output_linear = torch.nn.Linear(H, D_out)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3</div><div class="line">        and reuse the middle_linear Module that many times to compute hidden layer</div><div class="line">        representations.</div><div class="line"></div><div class="line">        Since each forward pass builds a dynamic computation graph, we can use normal</div><div class="line">        Python control-flow operators like loops or conditional statements when</div><div class="line">        defining the forward pass of the model.</div><div class="line"></div><div class="line">        Here we also see that it is perfectly safe to reuse the same Module many</div><div class="line">        times when defining a computational graph. This is a big improvement from Lua</div><div class="line">        Torch, where each Module could be used only once.</div><div class="line">        """</div><div class="line">        h_relu = self.input_linear(x).clamp(min=<span class="number">0</span>)</div><div class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(random.randint(<span class="number">0</span>, <span class="number">3</span>)):</div><div class="line">            h_relu = self.middle_linear(h_relu).clamp(min=<span class="number">0</span>)</div><div class="line">        y_pred = self.output_linear(h_relu)</div><div class="line">        <span class="keyword">return</span> y_pred</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables</span></div><div class="line">x = Variable(torch.randn(N, D_in))</div><div class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Construct our model by instantiating the class defined above</span></div><div class="line">model = DynamicNet(D_in, H, D_out)</div><div class="line"></div><div class="line"><span class="comment"># Construct our loss function and an Optimizer. Training this strange model with</span></div><div class="line"><span class="comment"># vanilla stochastic gradient descent is tough, so we use momentum</span></div><div class="line">criterion = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</div><div class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-4</span>, momentum=<span class="number">0.9</span>)</div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: Compute predicted y by passing x to the model</span></div><div class="line">    y_pred = model(x)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss</span></div><div class="line">    loss = criterion(y_pred, y)</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Zero gradients, perform a backward pass, and update the weights.</span></div><div class="line">    optimizer.zero_grad()</div><div class="line">    loss.backward()</div><div class="line">    optimizer.step()</div></pre></td></tr></table></figure>]]></content>
      
        <categories>
            
            <category> frame </category>
            
        </categories>
        
        
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[pytorch学习笔记]]></title>
      <url>/2017/05/18/pytorch/</url>
      <content type="html"><![CDATA[<h2 id="pytorch基本操作"><a href="#pytorch基本操作" class="headerlink" title="pytorch基本操作"></a>pytorch基本操作</h2><p>在pytorch中会使用到Tensor张量，它可以用来表示多维数组并且如果存在GPU的话可以进行利用。<br><a id="more"></a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch   </div><div class="line">x = torch.Tensor(<span class="number">5</span>, <span class="number">3</span>) <span class="comment">#构造一个未初始化的矩阵，注意是未初始化的矩阵.    </span></div><div class="line"><span class="comment">#矩阵加法</span></div><div class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>) <span class="comment">#构造一个使用随机数初始化的矩阵，注意返回的这个矩阵是Tensor</span></div><div class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</div><div class="line"><span class="comment">#以下两种方式都可以</span></div><div class="line">x + y</div><div class="line">torch.add(x, y)</div><div class="line"><span class="comment">#如果需要输出tensor</span></div><div class="line">result = torch.Tensor(<span class="number">5</span>, <span class="number">3</span>)</div><div class="line">torch.add(x, y, out=result)</div><div class="line"><span class="comment">#这种方法就会直接将y和x相加后的值更新到y中</span></div><div class="line"><span class="comment">#其中带有“_”的运算上的操作都带有相似的含义</span></div><div class="line">y.add_(x)</div></pre></td></tr></table></figure></p>
<p><em>注意：如果是维数相同的矩阵相加那么就是矩阵对应元素相加，如果是矩阵和标量相加那么就是矩阵每个元素都和这个标量相加。</em></p>
<table>
<thead>
<tr>
<th style="text-align:left">操作</th>
<th style="text-align:left">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">x+y</td>
<td style="text-align:left">相加，需要定义另外的变量接受返回值</td>
</tr>
<tr>
<td style="text-align:left">torch_add(x, y)</td>
<td style="text-align:left">含义同上</td>
</tr>
<tr>
<td style="text-align:left">x.add_(y)</td>
<td style="text-align:left">将y加到x上并更新x的值</td>
</tr>
<tr>
<td style="text-align:left">torch.add(x, y, out=result)</td>
<td style="text-align:left">相加后的值更新到自定义的Tensor变量result</td>
</tr>
</tbody>
</table>
<h3 id="Tensor与array转换"><a href="#Tensor与array转换" class="headerlink" title="Tensor与array转换"></a>Tensor与array转换</h3><p>在Torch中进行tensor和numpy的array的转换非常方便。转换后两者会共享同一块存储空间，意思是<strong>一个发生改变另外一个也会改变</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="comment">#生成一个tensor矩阵a</span></div><div class="line">a = torch.ones(<span class="number">5</span>)</div><div class="line"><span class="comment"># 利用Tensor对象a的numpy()方法，完成tensor到numpy的转换</span></div><div class="line">b = a.numpy()</div><div class="line"><span class="comment">#生成一个numpy矩阵c</span></div><div class="line">c = np.ones(<span class="number">5</span>)</div><div class="line"><span class="comment"># 完成从numpy array到tensor的转换</span></div><div class="line">d = torch.from_numpy(c)</div></pre></td></tr></table></figure>
<p><em>注意：使用x.cuda()可以将tensor移动到GPU上进行运算，这个是说得通过这样才能保障这个部分在GPU上进行运算吗？</em></p>
<h2 id="Autograd-自动求导"><a href="#Autograd-自动求导" class="headerlink" title="Autograd(自动求导)"></a>Autograd(自动求导)</h2><p>在pytorch中提供了一个自动求导的功能，这个功能依赖于torch中的autograd.Variable这个类。一般通过一个tensor和requires_grad的表示是否需要计算梯度的标志来实例化。Variable这个的内部简单看包括了data对应传入的tensor，grad对应计算得到的梯度，creator对应这个Variable的创建操作，如果是开发人员自定义的话这个Creator的值为None。</p>
<p><img src="/image/Variable.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line">x = Variable(torch.ones(<span class="number">2</span>, <span class="number">2</span>), requires_grad=<span class="keyword">True</span>)</div><div class="line">print(x.creator)</div><div class="line">&gt;<span class="keyword">None</span></div><div class="line">y = x + <span class="number">2</span></div><div class="line">print(y.creator)</div><div class="line"><span class="comment"># 在经过一个x+2这个操作得到的Variable变量y，y的creator属性表示它是通过一个加操作得到。</span></div><div class="line">&gt;&lt;torch.autograd._functions.basic_ops.AddConstant object at <span class="number">0x104ba7908</span>&gt;</div></pre></td></tr></table></figure>
<p>通过Variable中的data属性和creator属性就可以构建出一个计算图出来。从一个creator为None的地方输入，根据creator来衔接最后得到输出结果。以下绘制了一个简略图以说明<br><img src="/image/autograd.png"><br>在定义了Variable并围绕它进行一系列的操作后得到z，然后通过z.backward()调用次方法进行反向传播自动得到x的梯度，通过x.grad属性就可以获取到。这个注意到creator不为None的对象在反向传播过程中不会返回梯度。<br><em>注意：这里通过z=Mean(Y)得到是一个标量，也就是一些列的计算最后的输出只有一个那么进行backward()时就默认从这里开始，但是如果最后得到的结果有多个元素组成，那么就需要指定一个和输出结果tensor匹配的grad_output参数backward(grad_output)</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line">A = torch.ones(<span class="number">4</span>)</div><div class="line">x = Variable(A, requires_grad=<span class="keyword">True</span>)</div><div class="line">y = x + <span class="number">2</span></div><div class="line">gradients = torch.FloatTensor([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>])</div><div class="line">y.backward(gradients)</div></pre></td></tr></table></figure></p>
<p>另外这个grad_output参数还有个作用举个<a href="http://quabr.com/43451125/pytorch-what-are-the-gradient-arguments" target="_blank" rel="external">例子</a>如果输入的数据x，然后得到了一个中间结果y=[y1, y2, y3]，中间结果被使用来计算最终输出结果z那么：<br>$\frac{\partial z}{\partial x}=\frac{\partial z}{\partial y1}\frac{\partial y1}{\partial x}+\frac{\partial z}{\partial y2}\frac{\partial y2}{\partial x}+\frac{\partial z}{\partial y3}\frac{\partial y3}{\partial x}$这样在grad_output=[$\frac{\partial z}{\partial y1}$, $\frac{\partial z}{\partial y2}$, $\frac{\partial z}{\partial y3}$]再y.backward(grad_output)就可以得到最终的$\frac{\partial z}{\partial x}$了。</p>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>接下来是通过pytorch来实现一个简易卷积神经网络来对minist手写数字进行分类。对于卷积神经网络可以通过<a href="http://www.memorycrash.cn/2017/05/10/cnnNote/" target="_blank" rel="external">前面的文章</a>进行了解。<br><img src="/image/pytorchCnn.png"><br>对于一个卷积神经网络一般有以下模块组成：</p>
<ul>
<li>输入数据集合</li>
<li>若干卷积层+激活层+池化层</li>
<li>全连接层</li>
<li>softmax输出层</li>
</ul>
<p>对于一个卷积神经网络的训练一般是这样：</p>
<ol>
<li>forward前向传播，最后通过交叉熵代价函数得到代价也就是预测值和期望值之间的差距</li>
<li>backward反向传播，将误差信息往会传递获得神经元间连接的权值对应的偏导数</li>
<li>基于SGD(随机梯度下降),根据这些得到的偏导数刷新这些权值(weight=weight-learning_rate*gradient)，返回1如此循环以期望降低代价</li>
</ol>
<p>接下来定义一个卷积神经网络<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(Net, self).__init__() <span class="comment"># 调用Net的父类的__init__()函数进行初始化</span></div><div class="line">        <span class="comment"># 虽然我们使用的是SGD但是在实际中的SGD每次输入的并不是单个样本，而是一个小批次的样本</span></div><div class="line">        <span class="comment"># 1 图片输入通道，6个输出通道(这个6个可以理解为一个本卷积层有6个核)，5x5的感受野</span></div><div class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</div><div class="line">        <span class="comment"># 第2类的卷积层承接上一层的信息所以有6个输入通道，同时有16个核，5x5的感受野</span></div><div class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</div><div class="line">        <span class="comment"># 全连接层接受卷积层的数据y=Wx+b</span></div><div class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</div><div class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</div><div class="line">        <span class="comment"># 最后的输出是10个节点因为要识别0到9的数字</span></div><div class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="comment"># 使用最大池化的池化层，每次池化的窗口是2x2。这里先通过卷积层进行卷积计算后通过</span></div><div class="line">        <span class="comment"># relu来作为激活层进行激活后通过最大池化层池化(采样)</span></div><div class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</div><div class="line">        <span class="comment"># 如果池化的窗口是一个方阵可以只指定一个数字</span></div><div class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</div><div class="line">        <span class="comment"># torch 的 view 类似 numpy 的reshape用来调整形状，指定某个位置为-1表示将这个位置</span></div><div class="line">        <span class="comment"># 应该填写的内容交给系统自行计算。这里是为了方便输入数据到全连接层将调整后列数设置为</span></div><div class="line">        <span class="comment"># 16*5*5的大小，行数由系统计算得到</span></div><div class="line">        x = x.view(<span class="number">-1</span>, self.num_flat_features(x))</div><div class="line">        <span class="comment"># 使用relu作为激活函数的全连接层</span></div><div class="line">        x = F.relu(self.fc1(x))</div><div class="line">        x = F.relu(self.fc2(x))</div><div class="line">        x = self.fc3(x)</div><div class="line">        <span class="keyword">return</span> x</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="comment"># x[TensorSamples,nChannels,Height,Width]</span></div><div class="line">        <span class="comment"># x.size()[1:]得到x[nChannels,Height,Width]</span></div><div class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># 获取除开批次后的其它维度</span></div><div class="line">        num_features = <span class="number">1</span></div><div class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</div><div class="line">            num_features *= s</div><div class="line">        <span class="keyword">return</span> num_features</div><div class="line">        </div><div class="line">net = Net()</div><div class="line">print(net)</div></pre></td></tr></table></figure></p>
<h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h3><p>这里考虑用来计算模型预测值和期望值之间度量的代价函数，nn.MSELoss是一个均方误差代价函数$E=\frac{1}{N}\sum_{i}(\hat{y_i}-y_i)^2$还有需要注意的是我们需要训练的参数可以通过net.parameters()获得，得到的，然后转换成list后可以按照定义的顺序来获取到训练参数。下面的链式顺序表示从输入到获得loss的过程。</p>
<blockquote>
<p>input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d<br>     -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear<br>     -&gt; MSELoss<br>     -&gt; loss  </p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"><span class="comment"># 以下代码只是举例如何使用代价函数</span></div><div class="line"><span class="comment"># 手动构造一个测试数据</span></div><div class="line">input = Variable(torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>))</div><div class="line"><span class="comment"># 使用数据初始化一个神经网络</span></div><div class="line">output = net(input)</div><div class="line"><span class="comment"># 手动初始化一个目标值</span></div><div class="line">target = Variable(torch.range(<span class="number">1</span>, <span class="number">10</span>))  </div><div class="line"><span class="comment"># 定义一个均方误差代价函数</span></div><div class="line">criterion = nn.MSELoss()</div><div class="line"><span class="comment"># 获得代价值</span></div><div class="line">loss = criterion(output, target)</div><div class="line">loss.backward()</div></pre></td></tr></table></figure>
<h3 id="更新权值"><a href="#更新权值" class="headerlink" title="更新权值"></a>更新权值</h3><p>这里更新权值的方式是调用已经封装好的函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch, optim <span class="keyword">as</span> optim</div><div class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</div><div class="line"></div><div class="line"> <span class="comment">#选择我们进行优化的方式SGD随机梯度下降，传入需要处理的参数已经学习率</span></div><div class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</div><div class="line"></div><div class="line"> <span class="comment">#构建一个训练的循环</span></div><div class="line">optimizer.zero_grad()   <span class="comment"># 将梯度先值0，因为还没有开始进行训练</span></div><div class="line">output = net(input)</div><div class="line">loss = criterion(output, target)</div><div class="line">loss.backward()</div><div class="line">optimizer.step()    <span class="comment"># 一个循环中完成一次权值的更新</span></div></pre></td></tr></table></figure></p>
<h2 id="关于训练数据"><a href="#关于训练数据" class="headerlink" title="关于训练数据"></a>关于训练数据</h2><p>对于训练数据来说主要分为图片数据，语言数据以及文本信息。处理方式一般是先通过某些包将其加载到numpy的数组中，再转换为tensor。</p>
<ul>
<li>处理图像比较好的包是Pillow，OpenCV</li>
<li>处理语音比较好的包是sicpy，librosa</li>
<li>处理文本一般python的加载功能也可以</li>
</ul>
<p>对于图像来说，torchvision这个包中可以加载常用的图像数据，比如Imagenet(数据量大)，CIFAR10(有十种分类)，MNIST(简单的手写0到9数字灰度图)接下来的练习会选择CIFAR10数据集包含了这些分类:‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’。3X32X32是3个颜色通道大小32X32的图片。</p>
<p><img src="/image/cifar10.png"><br>接下来的代码基本和上面已经讲过的一致只是多了使用torchvision来加载数据处理数据的过程和训练模型已经验证模型。以下代码注释内容引用了<a href="www.jianshu.com/p/8da9b24b2fb6">这篇文章</a>。图像预处理可以参考<a href="https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit" target="_blank" rel="external">cs231n的课堂笔记</a></p>
<blockquote>
<p>ToTensor是指把PIL.Image(RGB)或者numpy.ndarray(H x W x C)从0到255的值映射到0到1的范围内并转化为Tensor格式(C x H x W)这里表示channel的C位置发生了改变<br>Normalize(mean, std)是通过下面的公式实现数据的归一化提供一个表示RGB标准差(R,G,B)和表示均值的(R,G,B)，mean是均值std是标准差$ std=\sqrt{ \frac{1}{N}\sum_{i=1}^{N}(x_i-mean)^2}$<br>channel=(channel-mean)/std</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torchvision</div><div class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</div><div class="line"><span class="comment"># transform可以对数据进行预处理,compose函数是去组合需要进行预处理的项目，比如归一化等</span></div><div class="line"><span class="comment"># 经过下面处理的数据取值变为[-1,1]</span></div><div class="line">transform = transforms.Compose([transforms.ToTensor(),</div><div class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</div><div class="line"><span class="comment"># 表示加载训练数据 train=True</span></div><div class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="keyword">True</span>,</div><div class="line">                                        download=<span class="keyword">True</span>, transform=transform)</div><div class="line"><span class="comment"># 将训练数据50000张图片划分为12500份，每份4张。shffule=True表示不同批次的数据遍历需打乱顺序</span></div><div class="line"><span class="comment"># num_workers表示使用两个子进程来加载数据                                    </span></div><div class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>,</div><div class="line">                                          shuffle=<span class="keyword">True</span>, num_workers=<span class="number">2</span>)</div><div class="line"><span class="comment"># 表示加载验证数据 train=False</span></div><div class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="keyword">False</span>,</div><div class="line">                                       download=<span class="keyword">True</span>, transform=transform)</div><div class="line">                                       </div><div class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>,</div><div class="line">                                         shuffle=<span class="keyword">False</span>, num_workers=<span class="number">2</span>)</div><div class="line"></div><div class="line">classes = (<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>,<span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</div></pre></td></tr></table></figure>
<p>momentum是梯度下降法中一种常用的加速技术。对于一般的SGD，其表达式为$x=x-\alpha*dx$,沿负梯度方向下降。而带momentum项的SGD则写生如下形式$v=\beta*v - \alpha*dx;x=x+v$：其中即momentum系数，通俗的理解上面式子就是，如果上一次的momentum（即）与这一次的负梯度方向是相同的，那这次下降的幅度就会加大，所以这样做能够达到加速收敛的过程。下面是这次用到的卷积神经网络代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(Net, self).__init__()</div><div class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</div><div class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</div><div class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</div><div class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</div><div class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</div><div class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = self.pool(F.relu(self.conv1(x)))</div><div class="line">        x = self.pool(F.relu(self.conv2(x)))</div><div class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</div><div class="line">        x = F.relu(self.fc1(x))</div><div class="line">        x = F.relu(self.fc2(x))</div><div class="line">        x = self.fc3(x)</div><div class="line">        <span class="keyword">return</span> x</div><div class="line"></div><div class="line"></div><div class="line">net = Net()</div><div class="line">criterion = nn.CrossEntropyLoss()</div><div class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</div><div class="line"></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">2</span>):  <span class="comment"># loop over the dataset multiple times</span></div><div class="line"></div><div class="line">    running_loss = <span class="number">0.0</span></div><div class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</div><div class="line">        <span class="comment"># 获取输入数据</span></div><div class="line">        inputs, labels = data</div><div class="line"></div><div class="line">        <span class="comment"># 转换输入数据以及标签为Variable</span></div><div class="line">        inputs, labels = Variable(inputs), Variable(labels)</div><div class="line"></div><div class="line">        <span class="comment"># 重置梯度为0</span></div><div class="line">        optimizer.zero_grad()</div><div class="line"></div><div class="line">        <span class="comment"># 前向 + 后向 + SGD更新</span></div><div class="line">        outputs = net(inputs)</div><div class="line">        loss = criterion(outputs, labels)</div><div class="line">        loss.backward()</div><div class="line">        optimizer.step()</div><div class="line"></div><div class="line">        <span class="comment"># print statistics</span></div><div class="line">        running_loss += loss.data[<span class="number">0</span>]</div><div class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:    <span class="comment"># print every 2000 mini-batches</span></div><div class="line">            print(<span class="string">'[%d, %5d] loss: %.3f'</span> %</div><div class="line">                  (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</div><div class="line">            running_loss = <span class="number">0.0</span></div><div class="line"></div><div class="line">print(<span class="string">'Finished Training'</span>)</div></pre></td></tr></table></figure></p>
<p>参考：<br>[1]<a href="http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="external">Deep Learning with PyTorch: A 60 Minute Blitz</a><br>[2]<a href="https://zhuanlan.zhihu.com/p/25572330" target="_blank" rel="external">PyTorch深度学习：60分钟入门(Translation)</a></p>
]]></content>
      
        <categories>
            
            <category> frame </category>
            
        </categories>
        
        
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[循环神经网络简略笔记]]></title>
      <url>/2017/05/11/rnnNote/</url>
      <content type="html"><![CDATA[<h2 id="循环神经网络和卷积神经网络"><a href="#循环神经网络和卷积神经网络" class="headerlink" title="循环神经网络和卷积神经网络"></a>循环神经网络和卷积神经网络</h2><p>我们可以使用卷积神经网络来处理图片的分类，从目前看其包含的特点是输入到卷积层的图片是大小是固定的，待分类的图片与图片之间是相对独立的存在。通过对目前我们了解到的卷积神经网络的描述可以了解到它的模型处理的场景和我们的现实生活还有有比较大的差异。</p>
<a id="more"></a>
<p>现实的生活中我们做出来的判断可能不能单靠一张图片来进行，比如我们需要和其它人交流了解到他说的整段话以后才能明白含义，看一篇文章，看一部视频也是一样的道理。我们因为可以记下交流中过去时刻的信息，最后基于这些记忆的内容来做出判断。</p>
<p>了解到这些后，有必要再分析下卷积神经网络的模型。在卷积神经网络中神经元模型是一个接受输入再输入的形式，可以看出在这样的模型中并不能记忆过去时刻的信息。而接下来的循环神经网络的部分层的神经元模型实现了以往信息对当前状态影响结果的记录。</p>
<h2 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h2><p>循环神经网络的基本机构如下图<br><img src="/image/rnnStru.png"><br>第一次看到这个图可能会感到很困惑，可以暂时不看w和它所在的圆形箭头那这个图便是一个常见的神经元模型，x作为输入进行权值矩阵U后输入到s，然后s通过激活函数激活后输出，输出后的值同样经过一个权值矩阵后输入到o。这里我们暂时忽略偏移bias，这不影响理解。</p>
<p>现在我们来考虑w的存在，假设某一个t时刻，从x输入了一个数据，它经过了权值矩阵后进入s，然后同样是经过激活函数的激活。接着输出的数据会通过v传递下去，同时会经过w这个权值矩阵处理后保存下来。等到t+1时刻有数据从x传入的时候，s就会吸收新传入的数据和上一时刻通过w处理后的信息了。这样就构造了一个可以保存记忆基于记忆来进行处理的神经元模型。</p>
<p>如上所述，在不同时刻数据通过这个结构都在影响着输入，下面按照时间将展开这个结构。<br><img src="/image/rnnUnflod.jpg" width="70%" height="70%"></p>
<ul>
<li>首先图中的W，V，U都是同一个，只是在时间维度上展开了。</li>
<li>从左向右传递上一个结点状态s会传递给下一个结点。</li>
<li>这里o在每个时刻都会依据当前的状态进行判断病输出。</li>
</ul>
<p>这里我们看看展开后的循环神经网络结构。这里的x可以理解为代表输入层，W代表循环层的权值矩阵，o表示最终的输出层。一般会使用sigmoid,tanh或者relu作为激活函数，循环层中我们使用tanh作为激活函数，最终使用softmax作为输出层。<br>$ o_{t}=g(Vs_{t}) $<br>$ s_{t}=f(Ux_{t}+Ws_{t-1}) $</p>
<h2 id="循环神经网络实现"><a href="#循环神经网络实现" class="headerlink" title="循环神经网络实现"></a>循环神经网络实现</h2><p><img src="/image/rnnLayer.png"><br>注意到这里多一个$ h_{t} $符号，它表示t时刻隐藏层的输入$ h_{t}=Ux_{t}+Ws_{t-1} $。还有$ \widehat{y}_{t} $表示t时候模型预测的最终输出值。如果我们要实现的是一个三层结构的循环神经网络并使用它来进行句子中词语的预测。并希望最终达到这样的效果。<br><img src="/image/rnnWords.png"><br>这里的s和e代表一句话的开始和结束。上图展示出来的情况表达的含义是我在每个时间点输入一个句子中的单词然后模型会预测出我下一个单词是什么，比如在t2时刻我输入了x2(我)，这个时候我期望得到的单词是“昨天”。这里举例的这个场景是rnn应用的一个小点，其它还包括机器翻译等。下面基于预测单词的要求来看看完成循环神经网络是什么样的。</p>
<h3 id="输入数据"><a href="#输入数据" class="headerlink" title="输入数据"></a>输入数据</h3><p>既然是预测单词，那我们的输入也是单词的内容。这里以一个句子作为一条训练样本，以句子中的单词作为具体的输入信息。可以看做单词是句子这个训练样本中的元素。按照上面的说法我们还需要给句子标注出开始和结束标志。我们把句子中的没有包含结束标志的部分可以当作是输入数据，把句子中没有包含开始标志的部分看做是验证模型预测输出数据是否正确的标准。因为我们并不能直接把一个单词作为输入或者输出。所以我们需要建立一个词汇表，就像是字典一样，在词汇表中将位置信息和具体的词汇进行映射。将映射后的信息比如[0 0 0 0 1 0 0]就代表“我”，传入到模型中去。</p>
<h3 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h3><p>在隐藏层中我们包含了一个循环，处于隐藏层的神经元接受当前时候的输入$ x_{t} $和上一个时刻的神经元的状态$ s_{t-1} $作为本次的输入，然后经过tanh激活函数进行激活后输出，同时也作为下一个时刻神经元的输入$ s_{t} $。</p>
<p>$ tanh(z) = \frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$<br>$ h_{t}=Ux_{t}+Ws_{t-1} $<br>$ s_{t}=tanh(Ux_{t}+Ws_{t-1}) $</p>
<p>tanh函数的图像是这样，和sigmoid很类似，但是tanh是关于0旋转对称的，它的取值范围是[-1,1]而sigmoid的取值范围是[0,1]。sigmoid的导数是$ {f}’(z)=f(z)(1-f(z)) $而tanh的导数$ {f}’(z)=1-(f(z))^{2} $</p>
<p><img src="/image/tanh.png"></p>
<h3 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h3><p>输出层的输入信息是从隐藏层来的，隐藏层输出的状态通过权值矩阵V处理以后输入到输出层softmax，这里V的主要作用是将隐藏层输出的矩阵信息转换为输出层可以处理的向量信息信息，因为输出层输出的是向量。softmax的公式是<br>$$ \sigma (z)_{j}=\frac{e^{z_{j}}}{\sum_{k=1}^{K}e^{z_{k}}} $$<br>可以看出通过softmax层输出后的是模型认为在这个位置应该出现的单词的概率。也就是说softmax输出的数据中值最大的那个就是模型预测的值。</p>
<h2 id="循环神经网络的训练"><a href="#循环神经网络的训练" class="headerlink" title="循环神经网络的训练"></a>循环神经网络的训练</h2><p>在训练循环神经网络的时候我们使用的是bptt(Backprogation Through Time)算法，基本思想和bp算法是一致的。bptt多考虑了在时间维度上进行循环的影响。一般来说训练一个神经网络我们先获取到经过模型的预测值和实际值之间的差异，这个过程属于一个前向传播的过程(这个差异就是代价函数我们这里使用交叉熵作为代价函数)。然后对预测值和实际值的差异从减小差异的方向来调整各个层的权值，这个过程属于一个后向传播的过程。之后重复这个过程直到模型预测结果复合要求。</p>
<p>后续的进行bptt算法的推导可以参考<a href="http://www.cnblogs.com/wacc/p/5341670.html" target="_blank" rel="external">此篇文档</a>这里只简单描述下，如上所述的</p>
<blockquote>
<p>“处于隐藏层的神经元接受当前时候的输入$ x_{t} $和上一个时刻的神经元的状态$ s_{t-1} $作为本次的输入”</p>
</blockquote>
<p>所以对于反向传播也会存在两个方向的传播，其实也好理解因为有两个方向的传播才能兼顾到U和W两个权值矩阵并对他们进行更新。而一般的非循环神经网络只有一个方向的传递路径。<br><img src="/image/rnnBptt.jpg"></p>
<ul>
<li>红色方向涉及权值矩阵U，表示将梯度传递到上一层网络(从隐藏层的角度看)</li>
<li>绿色方向涉及到权值矩阵W，表示将梯度沿着时间方向传递</li>
</ul>
<p>推导bptt公式还可以参考<a href="https://zybuluo.com/hanbingtao/note/541458" target="_blank" rel="external">此篇文档</a>,这里直接给出来(这里我感觉这个文章的推导中只计算了t时刻的梯度)。在时间维度展开以后对于代价函数我们可以表示为$ E=\sum_{t=0}^{T}E_{t} $我们需要获取的是E对V，W，U的求导结果。那根据上面的公式可以的得到。<br>$ \frac{\partial E}{\partial V}=\sum_{t=0}^{T}\frac{\partial E_{t}}{\partial V} $<br>$ \frac{\partial E}{\partial W}=\sum_{t=0}^{T}\frac{\partial E_{t}}{\partial W} $<br>$ \frac{\partial E}{\partial U}=\sum_{t=0}^{T}\frac{\partial E_{t}}{\partial U} $<br>下面是基于上图和这三个式子的进一步推导。<br>$ \frac{\partial E_{t}}{\partial V}=(\hat{y_{t}}-y_{t})\otimes h_{t} $<br>$ \frac{\partial E}{\partial V}=\sum_{t=0}^{T}(\hat{y_{t}}-y_{t})\otimes h_{t} $<br>以上表示的是权值矩阵V的导数计算公式，这里需要注意$ \frac{\partial E_{t}}{\partial V} $只是和当前的t时刻有关系。这个可以上面的图中看出来对于每个当前时刻的代价函数在将反向传播的时候只会经过对V这个权值矩阵只会经过一次。<br>$ \delta_{k}=(U^{T}\delta_{k+1})\odot (1-h_{k}\odot h_{k}) $<br>$ \delta_{t}=(V^{T}(\hat{y_{t}}-y_{t}))\odot (1-h_{t}\odot h_{t}) $<br>以上两个公式中$\delta_{k}$表示的是中间节点的误差(隐藏层到隐藏层或者隐藏层到输入层)，但是在传递误差的开始节点$\delta_{t}$和$\delta_{k}$是相等的。<br>$ \frac{\partial E_{t}}{\partial W}=\sum_{k=0}^{t}\delta_{k}\otimes h_{k-1} $<br>$ \frac{\partial E}{\partial W}=\sum_{t=0}^{T}\sum_{k=0}^{t}\delta_{k}\otimes h_{k-1} $<br>以上公式表示隐藏层到隐藏层在时间维度展开后计算导数。这里我们根据上图可以看出来$ \frac{\partial E_{t}}{\partial W} $对于$E_{t}$不单单收到当前t时刻的影响，也受到t-1时刻及其以前的时刻的影响，所以我们会把t时刻的起始误差循环的往过去传递以此来获取过去的时刻的梯度，因为W在各个时刻都是共享的。所以对于$E_{t}$来说它的起始误差对W的影响就是把从t开始往前时刻的影响都加起来。从这里的描述可以看出来这个也是当前t时刻的可以使用以前时刻的记忆的来源，因为依然受到了以前时刻的影响。不过根据梯度消失的情况我们越早时间的点对于当前t时刻的影响就越小，获取话说就是记不清太久以前的事情。<br>$ \frac{\partial E_{t}}{\partial U}=\sum_{k=0}^{t}\delta_{k}\otimes x_{k} $<br>$ \frac{\partial E}{\partial U}=\sum_{t=0}^{T}\sum_{k=0}^{t}\delta_{k}\otimes x_{k} $<br>以上公式是表示隐藏层到输入层在时间维度展开后计算导数。误差的传递根据上面的图可以看出来是会往U的方向传递的。因为循环层不仅受到前一个状态的影响也搜到各个时刻的当前输入影响。</p>
<h3 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h3><p>梯度消失在循环神经网络中更容易出现，我们在进行反向传播的时候，比如计算当前时刻t对于W的梯度，本质上对于W的影响是存在于过往的每一个时刻的。比如我们去理解一个句子的信息，在当前t时刻获取的字符要理解其含义可能需要联系下先前任意时刻的信息。但是在我们刚才的描述中也会发现对于t-n时刻，这个t-n的梯度就会比t-n+1时刻小，最终在某个时刻可能就没有梯度了也就是完全遗忘了以前的记忆。对于当前也就失去了影响。</p>
<h3 id="numpy矩阵操作"><a href="#numpy矩阵操作" class="headerlink" title="numpy矩阵操作"></a>numpy矩阵操作</h3><p>这里简单记录下numpy部分矩阵操作。其中A.dot(B)表示A和B进行矩阵乘法，A*B在numpy表示A和B的对应元素相乘，np.outer(A,B)表示A和B进行外积计算，np.dot(A,B)表示A和B进行点积计算</p>
<table>
<thead>
<tr>
<th>numpy操作</th>
<th style="text-align:left">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>A.dot(B)</td>
<td style="text-align:left">A和B进行矩阵乘法</td>
</tr>
<tr>
<td>np.dot(A,B)</td>
<td style="text-align:left">A和B进行矩阵乘法</td>
</tr>
<tr>
<td>A*B</td>
<td style="text-align:left">A和B的对应元素相乘</td>
</tr>
<tr>
<td>np.multiply(U3, U4)</td>
<td style="text-align:left">A和B的对应元素相乘</td>
</tr>
<tr>
<td>np.outer(A,B)</td>
<td style="text-align:left">A和B进行外积计算</td>
</tr>
<tr>
<td>A.T</td>
<td style="text-align:left">A矩阵的转置</td>
</tr>
</tbody>
</table>
<h2 id="长短期记忆"><a href="#长短期记忆" class="headerlink" title="长短期记忆"></a>长短期记忆</h2><p>长短期记忆 Long Short Term Memory(LSTM)可以很好的解决上面提到的循环神经网络中梯度消失问题。LSTM将原来直接操作输入的数据和上一个时刻状态的方式修改为，通过三个门来控制输入数据和上一个时刻的状态的信息。这三个门分别是<strong>遗忘门(forget gate)</strong>，<strong>输入门(input gate)</strong>，<strong>输出门(output gate)</strong>。</p>
<p>LSTM部分详细解释可以参考colah的博客<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM Networks</a>及其<a href="https://www.yunaitong.cn/understanding-lstm-networks.html" target="_blank" rel="external">中文翻译</a></p>
<p>以及这篇文章<a href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/" target="_blank" rel="external">WILDML</a>及其<a href="https://zhuanlan.zhihu.com/p/22371429" target="_blank" rel="external">中文翻译</a></p>
<p>对于LSTM的梯度推导过程可以看下<a href="https://zybuluo.com/hanbingtao/note/581764" target="_blank" rel="external">零基础入门深度学习(6) - 长短时记忆网络(LSTM)</a></p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> rnn </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[卷积神经网络简略笔记]]></title>
      <url>/2017/05/10/cnnNote/</url>
      <content type="html"><![CDATA[<h2 id="卷积神经网络与神经网络"><a href="#卷积神经网络与神经网络" class="headerlink" title="卷积神经网络与神经网络"></a>卷积神经网络与神经网络</h2><p>神经网络和卷积神经网络的关系可以理解过卷积神经网络是为了可以更好处理图片而对神经网络进行了相应的改造和特殊化。这里以卷积来为这个新的神经网络进行命名，可能是卷积神经网络的卷积层在对原始数据的处理上类似于卷积操作。但是如果本身对数学上的卷积操作不怎么了解，其实也不影响对卷积神经网络的理解。<br><a id="more"></a></p>
<h2 id="神经网络处理图片的问题"><a href="#神经网络处理图片的问题" class="headerlink" title="神经网络处理图片的问题"></a>神经网络处理图片的问题</h2><p>我们可以使用神经网络来处理图片，对图片进行学习分类。但是神经网络可能对处理尺寸较小的，不复杂的图片处理更擅长。对于具有相反特点的图片神经网络的学习效果就不理想了。其中的原因包括有神经网络会将图片的所有像素信息都传入到神经网络的输入层中，当图片过大或者需要进行更复杂的分类时会导致模型中出现大量的参数。这样直接导致的是计算量变得极大最终难以计算。而且为了进行更复杂的判断新增的隐藏又可能导致梯度消失的情况。</p>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>为了减少上面问题的影响，发展出了卷积神经网络。最主要的特点是添加了卷积层来对图片进行处理。卷积神经网络模型的样子如下：</p>
<p><img src="/image/simple_conv.png" width="50%" height="50%/"></p>
<p>简单说下上图中convolutionallayer就是卷积层，主要作用是通过过滤器来过滤出原始图片的特征，一般一个卷积层有多个过滤器所以也就能得到多个特征来。接下来是poolinglayer是池化层主要是将卷积层产生的特征信息进行采样以进一步减小图片的尺寸。接着将池化后的数据输入到全连接层，这里的全连接层就和神经网络是一样，接着是通过输出层得到分类结果。</p>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>在神经网络中我们一般使用sigmod作为激活函数，它的图像如下</p>
<p><img src="/image/sigmoid.png" width="50%" height="50%/"></p>
<p>这里可以看出来在偏离图像的中心点0越远则图像的变换的趋势越加缓慢，如果在初始化w和b的过程可能某些神经元获得了一个更大的值。这就引起了梯度的下降变的缓慢解决的方式可以选择换一个激活函数比如relu函数，它的图像如下：</p>
<p><img src="/image/relu.png" width="50%" height="50%"></p>
<h3 id="代价函数-sigmoid-交叉熵"><a href="#代价函数-sigmoid-交叉熵" class="headerlink" title="代价函数 sigmoid+交叉熵"></a>代价函数 sigmoid+交叉熵</h3><p>在先前使用sigmod作为激活函数的神经网络中我们使用代价函数的模式一般是平方差的形式。代价函数的形式是会依赖对激活函数求导，我们知道求导对应到曲线可以理解为曲线在某点的变化率。以下通过一个一个输入一个输出的单个神经元模型来进行解释。以下输入假设x是1，期待y输出为0可以看出求导的结果确实依赖了对sigmod的求导，这样就会出现上面讨论的梯度下降缓慢的问题。<br>$$ C=\frac{(y-a)^{2}}{2} $$</p>
<p>$$ \frac{\partial C}{\partial w}=(a-y){\sigma}’(z)x=a{\sigma}’(z) $$</p>
<p>$$ \frac{\partial C}{\partial b}=(a-y){\sigma}’(z)=a{\sigma}’(z) $$</p>
<p>通过使用交叉熵来作为代价函数：</p>
<p>$$ C=-\frac{1}{n}\sum_{x}[ylna+(1-y)ln(1-a)] $$</p>
<p>可以对w和b分别求导发现结果是和激活函数的导数没有关系的。</p>
<h3 id="softmax-log-likelihood"><a href="#softmax-log-likelihood" class="headerlink" title="softmax+log-likelihood"></a>softmax+log-likelihood</h3><p>除了交叉熵以外还可以通过softmax的方式来解决学习速度衰减的问题。我们仅将输出层从普通的sigmod作为激活函数的层替换为softmax层。softmax输出层同样接受z=wx+b然后通过以下公式来计算输出结果</p>
<p>$$ a_{j}^{L}=\frac{e^{z_{j}^{L}}}{\sum_{k}e^{z_{k}^{L}}} $$</p>
<p>可以看出来这里得到的是某个值占总体的一个比例。配合softmax我们的代价函数需要替换成log-likelihood</p>
<p>$$ C\equiv-lna_{y}^{L} $$</p>
<p>这里表示的是单个输入样本的代价，如果有多个样本的可以对他们的代价求均值，作为总的代价函数。通过代价函数对w和b求导得到公式</p>
<p>$$ \frac{\partial C}{\partial b_{j}^{L}}=a_{j}^{L}-y_{j} $$</p>
<p>$$ \frac{\partial C}{\partial w_{jk}^{L}}=a_{k}^{L-1}(a_{k}^{L}-y_{j}) $$</p>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>一般有l1,l2,dropout的方式来对模型进行正则化，主要目的还是防止模型的过拟合，其中l2正则化方法是对所有的w进行如下处理并把这个部分添加到代价函数中去需要注意的是l2正则化只需包括w不需要包括b。</p>
<p>$$ C=-\frac{1}{n}\sum_{xj}[y_{j}lna_{j}^{L}+(1-y_{j})ln(1-a_{j}^{L})]+\frac{\lambda}{2n}\sum_{w}w^{2} $$</p>
<p>这样起到的作用使的优化这个代价函数，会更倾向于获取一个w并不是那么复杂的模型。一般直观的来看过拟合的模型都是在训练集中表现过于好的函数，而表现的过好的模型一般w都是比较复杂的。我们对于l1正则化不作解释。</p>
<p>接下还有一种方式是dropout，可以理解为丢弃。处理的思想类似我们以前看的集成学习法。这dropout中会随机的丢弃一些神经元(非输入和输出层)，也就是我们每次迭代更新梯度只对模型中的部分神经元进行处理。在一个batch的样本训练并更新完w和b以后我们会重新再进行一次dropout。这样的感觉就像训练了多个子神经网络，最后将他们组合成一个大的神经元来进行使用。是不是和集成学习思想类似？</p>
<h3 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h3><p>在先前的神经网络中一般采用的是符合一个均值是0，标准差是1的高斯分布的随机数去初始化w和b，但是这个并不适合初始化隐藏层比较多的神经网络。一般是将标准差表示为 $1/\sqrt{n_{in}}$ 这里的 $n_{in}$ 表示具有输入权重的神经元个数。</p>
<h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>卷积层是卷积神经网络不同于神经网络的一个重要特点。首先我们输入到卷积神经网络的依然是一个图片，但是作为接受这个数据的卷积层并不是如果神经网络一样将图片的所有像素信息都输入，而是使用一个比输入图片小的多的过滤器来扫描原始输入的图片数据。过滤器可以理解为将原始图片的一小部分作为输入并通过权值和偏移进行计算后输入激活函数得到在新的数据。这个过滤器会按照设置每次滑动并重复刚才的过程。最终得到的就是对应一个过滤器得到的一个特征map。这里需要特别说明单独过滤器滑动的任何区域计算使用的w和b都是一样的，一般的讲这个可以称为<strong>参数共享</strong>。这样的好处是识别某个特征将和这个特征在原始图片出现的位置无关。</p>
<p><img src="/image/no_padding.gif" width="50%" height="50%/"></p>
<p>可以直观的看到经过过滤器处理以后的图像的大小变小了，同时多个过滤器也可以获得到图片的多种特征。同时可以看出来我们需要仔细设计过滤器的大小以及每次滑动的距离以使的在原始图像中每个像素都可以涉及到。但是可能真就存在冲突的情况或者我希望过滤器处理后图片的大小不发生改变，这个时候可以适当的在待过滤的图片边缘添加0来实现。</p>
<p><img src="/image/padding.gif" width="50%" height="50%/"></p>
<p>这些处理得到特征map一般还需要通过池化来进行采样，其中一个目的就是进一步减小图片大小。一般的池化方法就是最大池化。比如使用2*2大小的框格在特制map上每次移动2格，将22在特征map范围上的最大的数据返回。这样处理以后的数据可能是作为下一个卷积层的输入也可能作为全连接层的输入。</p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> cnn </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
