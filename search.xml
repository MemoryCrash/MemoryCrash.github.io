<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[循环神经网络简略笔记]]></title>
      <url>/2017/05/11/rnnNote/</url>
      <content type="html"><![CDATA[<h2 id="循环神经网络和卷积神经网络"><a href="#循环神经网络和卷积神经网络" class="headerlink" title="循环神经网络和卷积神经网络"></a>循环神经网络和卷积神经网络</h2><a id="more"></a>]]></content>
      
        <categories>
            
            <category> deep leaning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> rnn </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[卷积神经网络简略笔记]]></title>
      <url>/2017/05/10/cnnNote/</url>
      <content type="html"><![CDATA[<h2 id="卷积神经网络与神经网络的关系"><a href="#卷积神经网络与神经网络的关系" class="headerlink" title="卷积神经网络与神经网络的关系"></a>卷积神经网络与神经网络的关系</h2><p>神经网络和卷积神经网络的关系可以理解过卷积神经网络是为了可以更好处理图片而对神经网络进行了相应的改造和特殊化。这里以卷积来为这个新的神经网络进行命名，可能是卷积神经网络的卷积层在对原始数据的处理上类似于卷积操作。但是如果本身对数学上的卷积操作不怎么了解，其实也不影响对卷积神经网络的理解。<br><a id="more"></a></p>
<h2 id="神经网络处理图片的问题"><a href="#神经网络处理图片的问题" class="headerlink" title="神经网络处理图片的问题"></a>神经网络处理图片的问题</h2><p>我们可以使用神经网络来处理图片，对图片进行学习分类。但是神经网络可能对处理尺寸较小的，不复杂的图片处理更擅长。对于具有相反特点的图片神经网络的学习效果就不理想了。其中的原因包括有神经网络会将图片的所有像素信息都传入到神经网络的输入层中，当图片过大或者需要进行更复杂的分类时会导致模型中出现大量的参数。这样直接导致的是计算量变得极大最终难以计算。而且为了进行更复杂的判断新增的隐藏又可能导致梯度消失的情况。</p>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>为了减少上面问题的影响，发展出了卷积神经网络。最主要的特点是添加了卷积层来对图片进行处理。卷积神经网络模型的样子如下：</p>
<p><img src="/image/simple_conv.png" width="50%" height="50%/"></p>
<p>简单说下上图中convolutionallayer就是卷积层，主要作用是通过过滤器来过滤出原始图片的特征，一般一个卷积层有多个过滤器所以也就能得到多个特征来。接下来是poolinglayer是池化层主要是将卷积层产生的特征信息进行采样以进一步减小图片的尺寸。接着将池化后的数据输入到全连接层，这里的全连接层就和神经网络是一样，接着是通过输出层得到分类结果。</p>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>在神经网络中我们一般使用sigmod作为激活函数，它的图像如下</p>
<p><img src="/image/sigmoid.png" width="50%" height="50%/"></p>
<p>这里可以看出来在偏离图像的中心点0越远则图像的变换的趋势越加缓慢，如果在初始化w和b的过程可能某些神经元获得了一个更大的值。这就引起了梯度的下降变的缓慢解决的方式可以选择换一个激活函数比如relu函数，它的图像如下：</p>
<p><img src="/image/relu.png" width="50%" height="50%"></p>
<h3 id="代价函数-sigmoid-交叉熵"><a href="#代价函数-sigmoid-交叉熵" class="headerlink" title="代价函数 sigmoid+交叉熵"></a>代价函数 sigmoid+交叉熵</h3><p>在先前使用sigmod作为激活函数的神经网络中我们使用代价函数的模式一般是平方差的形式。代价函数的形式是会依赖对激活函数求导，我们知道求导对应到曲线可以理解为曲线在某点的变化率。以下通过一个一个输入一个输出的单个神经元模型来进行解释。以下输入假设x是1，期待y输出为0可以看出求导的结果确实依赖了对sigmod的求导，这样就会出现上面讨论的梯度下降缓慢的问题。<br>$$ C=\frac{(y-a)^{2}}{2} $$</p>
<p>$$ \frac{\partial C}{\partial w}=(a-y){\sigma}’(z)x=a{\sigma}’(z) $$</p>
<p>$$ \frac{\partial C}{\partial b}=(a-y){\sigma}’(z)=a{\sigma}’(z) $$</p>
<p>通过使用交叉熵来作为代价函数：</p>
<p>$$ C=-\frac{1}{n}\sum_{x}[ylna+(1-y)ln(1-a)] $$</p>
<p>可以对w和b分别求导发现结果是和激活函数的导数没有关系的。</p>
<h3 id="softmax-log-likelihood"><a href="#softmax-log-likelihood" class="headerlink" title="softmax+log-likelihood"></a>softmax+log-likelihood</h3><p>除了交叉熵以外还可以通过softmax的方式来解决学习速度衰减的问题。我们仅将输出层从普通的sigmod作为激活函数的层替换为softmax层。softmax输出层同样接受z=wx+b然后通过以下公式来计算输出结果</p>
<p>$$ a_{j}^{L}=\frac{e^{z_{j}^{L}}}{\sum_{k}e^{z_{k}^{L}}} $$</p>
<p>可以看出来这里得到的是某个值占总体的一个比例。配合softmax我们的代价函数需要替换成log-likelihood</p>
<p>$$ C\equiv-lna_{y}^{L} $$</p>
<p>这里表示的是单个输入样本的代价，如果有多个样本的可以对他们的代价求均值，作为总的代价函数。通过代价函数对w和b求导得到公式</p>
<p>$$ \frac{\partial C}{\partial b_{j}^{L}}=a_{j}^{L}-y_{j} $$</p>
<p>$$ \frac{\partial C}{\partial w_{jk}^{L}}=a_{k}^{L-1}(a_{k}^{L}-y_{j}) $$</p>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>一般有l1,l2,dropout的方式来对模型进行正则化，主要目的还是防止模型的过拟合，其中l2正则化方法是对所有的w进行如下处理并把这个部分添加到代价函数中去需要注意的是l2正则化只需包括w不需要包括b。</p>
<p>$$ C=-\frac{1}{n}\sum_{xj}[y_{j}lna_{j}^{L}+(1-y_{j})ln(1-a_{j}^{L})]+\frac{\lambda}{2n}\sum_{w}w^{2} $$</p>
<p>这样起到的作用使的优化这个代价函数，会更倾向于获取一个w并不是那么复杂的模型。一般直观的来看过拟合的模型都是在训练集中表现过于好的函数，而表现的过好的模型一般w都是比较复杂的。我们对于l1正则化不作解释。</p>
<p>接下还有一种方式是dropout，可以理解为丢弃。处理的思想类似我们以前看的集成学习法。这dropout中会随机的丢弃一些神经元(非输入和输出层)，也就是我们每次迭代更新梯度只对模型中的部分神经元进行处理。在一个batch的样本训练并更新完w和b以后我们会重新再进行一次dropout。这样的感觉就像训练了多个子神经网络，最后将他们组合成一个大的神经元来进行使用。是不是和集成学习思想类似？</p>
<h3 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h3><p>在先前的神经网络中一般采用的是符合一个均值是0，标准差是1的高斯分布的随机数去初始化w和b，但是这个并不适合初始化隐藏层比较多的神经网络。一般是将标准差表示为 $1/\sqrt{n_{in}}$ 这里的 $n_{in}$ 表示具有输入权重的神经元个数。</p>
<h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>卷积层是卷积神经网络不同于神经网络的一个重要特点。首先我们输入到卷积神经网络的依然是一个图片，但是作为接受这个数据的卷积层并不是如果神经网络一样将图片的所有像素信息都输入，而是使用一个比输入图片小的多的过滤器来扫描原始输入的图片数据。过滤器可以理解为将原始图片的一小部分作为输入并通过权值和偏移进行计算后输入激活函数得到在新的数据。这个过滤器会按照设置每次滑动并重复刚才的过程。最终得到的就是对应一个过滤器得到的一个特征map。这里需要特别说明单独过滤器滑动的任何区域计算使用的w和b都是一样的，一般的讲这个可以称为<strong>参数共享</strong>。这样的好处是识别某个特征将和这个特征在原始图片出现的位置无关。</p>
<p><img src="/image/no_padding.gif" width="50%" height="50%/"></p>
<p>可以直观的看到经过过滤器处理以后的图像的大小变小了，同时多个过滤器也可以获得到图片的多种特征。同时可以看出来我们需要仔细设计过滤器的大小以及每次滑动的距离以使的在原始图像中每个像素都可以涉及到。但是可能真就存在冲突的情况或者我希望过滤器处理后图片的大小不发生改变，这个时候可以适当的在待过滤的图片边缘添加0来实现。</p>
<p><img src="/image/padding.gif" width="50%" height="50%/"></p>
<p>这些处理得到特征map一般还需要通过池化来进行采样，其中一个目的就是进一步减小图片大小。一般的池化方法就是最大池化。比如使用2*2大小的框格在特制map上每次移动2格，将22在特征map范围上的最大的数据返回。这样处理以后的数据可能是作为下一个卷积层的输入也可能作为全连接层的输入。</p>
]]></content>
      
        <categories>
            
            <category> deep leaning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> cnn </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
