<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[Batch Normalization 学习笔记]]></title>
      <url>/2017/06/15/BatchNorma/</url>
      <content type="html"><![CDATA[<p>参考文件列表：<br>[0] <a href="https://www.youtube.com/watch?v=-5hESl-Lj-4" target="_blank" rel="external">什么是 Batch Normalization 批标准化 (深度学习 deep learning)</a><br>[1] <a href="https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：反向传播笔记</a><br>[2] <a href="https://www.zhihu.com/question/38102762" target="_blank" rel="external">深度学习中 Batch Normalization为什么效果好？-魏秀参答</a><br>[3] <a href="http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html" target="_blank" rel="external">Must Know Tips/Tricks in Deep Neural Networks (by Xiu-Shen Wei)</a><br>[4] <a href="http://blog.csdn.net/hjimce/article/details/50866313" target="_blank" rel="external">深度学习（二十九）Batch Normalization 学习笔记</a><br>[5] <a href="http://www.jianshu.com/p/0312e04e4e83" target="_blank" rel="external">谈谈Tensorflow的Batch Normalization</a><br>[6] <a href="https://r2rt.com/implementing-batch-normalization-in-tensorflow.html" target="_blank" rel="external">Implementing Batch Normalization in Tensorflow</a><br>[7] <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html" target="_blank" rel="external">Understanding the backward pass through Batch Normalization Layer</a><br>[8] <a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="external">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a><br> <a id="more"></a><br>感激上面的这些参考文件，对于了解和学习Batch Normalization起到了很大的帮助。如果有人意外的看到了我这边笔记，我的建议是可以先看上面的这些参考文件。下面的内容主要是为了方便自己查阅Batch Normalization的内容而做的相关笔记。</p>
<h2 id="神经网络训练"><a href="#神经网络训练" class="headerlink" title="神经网络训练"></a>神经网络训练</h2><p>我们基本上可以对神经网络的形成一个印象，就是其训练过程对超参数配置有较为严格的要求，比如一般需要一个比较小的学习率，但是学习率小了也意味着训练时间的变长。同时需要警惕梯度消散的问题，因此添加了drop out或者l2正则化来减少影响，选择合适的激活函数比如relu。同时也包括对于输入数据的预处理比如“白化”。可以看出这个过程充满了各种技巧。</p>
<p>后面就有人去思考一些简单的方法来改变这个复杂的训练过程，其中一个就是Batch Normalization。这个可以同数据预处理中的归一化处理联系起来，一般我们经过归一化处理后的数据输入数据可以得到一个更好的训练结果(我猜)，既然可以对输入数据做这样的处理，那是不是也可以对卷积层，全连接层也做这样的处理呢？</p>
<h2 id="Batch-Noramalization"><a href="#Batch-Noramalization" class="headerlink" title="Batch Noramalization"></a>Batch Noramalization</h2><p>在对数据进行预处理的时候，我们将它们进行归一化后，这样如果是一个有效的变化区间在-1到1之间的tanh激活函数就很有利。如果没有这样操作输入的数据如果过大能直接落到了激活函数对于梯度感觉不明显的区域后续的训练就会变的很慢。所以实际上我们也是将这样的处理方式推广到了卷积层或者全连接层。</p>
<p>在训练的过程中数据是一批批的进入。所以我们也是对数据进行一批批的归一化处理。具体来说是对每个神经元接受的数据进行归一化，而且是在数据进入到激活函数前完成归一化操作。也就是对$Wx+b$这个阶段。</p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> batch norm </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[matplotlib笔记]]></title>
      <url>/2017/06/07/matplot/</url>
      <content type="html"><![CDATA[<p>在机器学习或者深度学习中，matplotlib都是一个可视化数据的好工具。它可以辅助开发者了解数据的情况，下面我们就简单的以一些列子来学习matplotlib。<br>以下例子中的代码和图形引用自此<a href="https://zhuanlan.zhihu.com/p/26660699" target="_blank" rel="external">文章</a></p>
<a id="more"></a>
<h2 id="显示多子图"><a href="#显示多子图" class="headerlink" title="显示多子图"></a>显示多子图</h2><p>在matplotlib中，整个图像为一个Figure对象。在Figure对象中可以包含一个，或者多个Axes对象。每个Axes对象都是一个拥有自己坐标系统的绘图区域。</p>
<p><code>ax.imshow(images[i].reshape(img_shape), cmap=&#39;binary&#39;)</code>这个函数是在axes中绘制图像，第一个参数是传入来整理过形状的绘制数据，cmap是color map可以<a href="http://matplotlib.org/examples/color/colormaps_reference.html" target="_blank" rel="external">对照</a>选择你喜欢的颜色，这里选择的 binary 就是普通的黑白。在python3中很多使用来<code>format</code>来进行格式化可以参考此<a href="关于format的用法可以参考http://blog.xiayf.cn/2013/01/26/python-string-format/">文章</a></p>
<p>绘图举例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_images</span><span class="params">(images, cls_true, cls_pred=None)</span>:</span></div><div class="line">    <span class="keyword">assert</span> len(images) == len(cls_true) == <span class="number">9</span></div><div class="line">    <span class="comment"># 创建一个拥有子图的图片，返回图片对象和轴</span></div><div class="line">    fig, axes = plt.subplots(<span class="number">3</span>, <span class="number">3</span>)</div><div class="line">    <span class="comment"># 调整子图在整个图片中的宽和高的间隔</span></div><div class="line">    fig.subplots_adjust(hspace=<span class="number">0.2</span>, wspace=<span class="number">0.3</span>)</div><div class="line">    <span class="comment"># 使用flat属性来提取子对象，flat数组元素的迭代器</span></div><div class="line">    <span class="comment"># enumerate 返回位置索引和该位置上的对象</span></div><div class="line">    <span class="keyword">for</span> i, ax <span class="keyword">in</span> enumerate(axes.flat):</div><div class="line">        <span class="comment"># 在ax中展示图形。输入第一个参数是子图片的尺寸。cmap是color map</span></div><div class="line">        <span class="comment"># 选择你喜欢的颜色，这里选择的 binary 就是普通的黑白</span></div><div class="line">        ax.imshow(images[i].reshape(img_shape), cmap=<span class="string">'binary'</span>)</div><div class="line">        <span class="comment"># ax.imshow(images[i].reshape(img_shape), cmap='summer')</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> cls_pred <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            xlabel = <span class="string">"True:&#123;0&#125;"</span>.format(cls_true[i])</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            xlabel = <span class="string">"True:&#123;0&#125;, Pred:&#123;1&#125;"</span>.format(cls_true[i], cls_pred[i])</div><div class="line">        <span class="comment"># 在x方向设置label</span></div><div class="line">        ax.set_xlabel(xlabel)</div><div class="line">        <span class="comment"># 设定x和y轴的标签，这里[]是空意思就是不现实这些标签。</span></div><div class="line">        ax.set_xticks([])</div><div class="line">        ax.set_yticks([])</div><div class="line">    <span class="comment"># 展示图片</span></div><div class="line">    plt.show()</div></pre></td></tr></table></figure>
<h2 id="绘制混淆矩阵"><a href="#绘制混淆矩阵" class="headerlink" title="绘制混淆矩阵"></a>绘制混淆矩阵</h2><p>在机器学习或者深度学习中我们也存在需要去查看绘制的一个混淆矩阵的需求，用来可视化预测值和实际值之间的差异。方法是先<a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html" target="_blank" rel="external">生成一个混淆矩阵</a>，再将它绘制出来。下面的例子中涉及到一个<code>plt.tight_layout</code>是用来对图片进行<a href="https://stackoverflow.com/questions/9603230/how-to-use-matplotlib-tight-layout-with-figure" target="_blank" rel="external">细微调整</a>。同时说明下<code>plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=plt.cm.Blues)</code>这里的cmap可以填写字符串<code>Blues</code>也可以填写<code>plt.cm.Blues</code>。具体颜色可以参考此<a href="http://matplotlib.org/examples/color/colormaps_reference.html" target="_blank" rel="external">表</a>。</p>
<p>举例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_confusion_matrix</span><span class="params">()</span>:</span></div><div class="line">    cls_true = data.test.cls</div><div class="line"></div><div class="line">    cls_pred = session.run(y_pred_cls, feed_dict=feed_dict_test)</div><div class="line">    <span class="comment"># 输入分布是实际的标签值，和模型预测的标签值。</span></div><div class="line">    cm = confusion_matrix(y_true=cls_true, y_pred=cls_pred)</div><div class="line">    <span class="comment"># 绘制图片interpolation='nearest'是图像处理的一种方式</span></div><div class="line">    <span class="comment"># 选择你喜欢的颜色，这里选择的 binary 就是普通的黑白</span></div><div class="line">    plt.imshow(cm, interpolation=<span class="string">'nearest'</span>, cmap=plt.cm.Blues)</div><div class="line">    <span class="comment"># 自动调整图片如果是多个图片的话会自动调整图片的间距具体可以查看此链接</span></div><div class="line">    plt.tight_layout()</div><div class="line">    <span class="comment"># 添加一个色条</span></div><div class="line">    plt.colorbar()</div><div class="line">    tick_marks = np.arange(num_classes)</div><div class="line">    <span class="comment"># 将tick_marks对应位置表示为num_classes，也就是画出标尺</span></div><div class="line">    plt.xticks(tick_marks, range(num_classes))</div><div class="line">    plt.yticks(tick_marks, range(num_classes))</div><div class="line"></div><div class="line">    <span class="comment"># 设置x轴标签为预测</span></div><div class="line">    plt.xlabel(<span class="string">'Predicted'</span>)</div><div class="line">    <span class="comment"># 设置y轴标签为真实</span></div><div class="line">    plt.ylabel(<span class="string">'True'</span>)</div><div class="line">    <span class="comment"># 展示图片</span></div><div class="line">    plt.show()</div></pre></td></tr></table></figure>
<p><img src="/image/confusion.png"></p>
<p>举例：<br>下图是可视化权值的分布情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 绘制权值变化</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_weights</span><span class="params">()</span>:</span></div><div class="line">    w = session.run(weights)</div><div class="line">    <span class="comment"># 获取权值的最大和最小值作为后续画图的范围使用。</span></div><div class="line">    w_min = np.min(w)</div><div class="line">    w_max = np.max(w)</div><div class="line"></div><div class="line">    fig, axes = plt.subplots(<span class="number">3</span>, <span class="number">4</span>)</div><div class="line">    fig.subplots_adjust(hspace=<span class="number">0.3</span>, wspace=<span class="number">0.3</span>)</div><div class="line"> </div><div class="line">    <span class="keyword">for</span> i, ax <span class="keyword">in</span> enumerate(axes.flat):</div><div class="line">        <span class="keyword">if</span> i&lt;<span class="number">10</span>:</div><div class="line">            <span class="comment"># 一个数字一个子图</span></div><div class="line">            image = w[:, i].reshape(img_shape)</div><div class="line"> </div><div class="line">            ax.set_xlabel(<span class="string">"Weights: &#123;0&#125;"</span>.format(i))</div><div class="line">            <span class="comment"># 可以在上文中的颜色列表中看到'seismic'是一种一半蓝色一半红色的渐变颜色</span></div><div class="line">            <span class="comment"># 用来做对比很合适，这里vmin=w_min, vmax=w_max也设定了每个子图的变化</span></div><div class="line">            <span class="comment"># 都是在一个范围的。</span></div><div class="line">            ax.imshow(image, vmin=w_min, vmax=w_max, cmap=<span class="string">'seismic'</span>)</div><div class="line"></div><div class="line">        ax.set_xticks([])</div><div class="line">        ax.set_yticks([]) </div><div class="line"></div><div class="line">    plt.show()</div></pre></td></tr></table></figure>
<p><img src="/image/plotweight.png"></p>
]]></content>
      
        <categories>
            
            <category> tool </category>
            
        </categories>
        
        
        <tags>
            
            <tag> matplot </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[迁移学习Tensorflow]]></title>
      <url>/2017/06/06/TransLearnByTF/</url>
      <content type="html"><![CDATA[<p>参考：<a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/08_Transfer_Learning.ipynb" target="_blank" rel="external">Transfer Learning</a><br>在先前的<a href="http://www.memorycrash.cn/2017/05/21/TransLearning/" target="_blank" rel="external">文章</a>有涉及到使用Pytorch实现迁移学习。迁移学习的思路是基于一个在大量数据上训练好的神经网络，通过它作为一个特征提取器，将提取出来的特征输入一个新的神经网络中进一步训练得到我们需要的结果。这样做比起完全的从头开始训练一个神经网络可以节约很大一部分的时间。<br><a id="more"></a></p>
<h2 id="基于Tensorflow"><a href="#基于Tensorflow" class="headerlink" title="基于Tensorflow"></a>基于Tensorflow</h2><p>我们准备基于<code>inception v3</code>的迁移学习再去训练<code>cifar10</code>的分类，如何去实现？我们首先选择<code>inception v3</code> 这个基于imageNet的大量文件训练好的神经网络。这个网络并不会在tensorflow中提供，需要单独下载。参考文件中提供了一个<a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/inception.py" target="_blank" rel="external">inception.py</a>文件里面集成了<code>inception v3</code>的下载地址和解析加载模型的方法。下面给出inception v3的大致结构图。<br><img src="/image/transLearnTF/inceptionV3Trans.png"><br>从这个图中根据图例我们可以看出来最后是选择了<code>AvgPool</code>的输出作为特征进行重新训练。<br><strong>注意1</strong>：graph.get_tensor_by_name(name)可以根据名称来获得输出Tensor。我们基于这个函数实现特征的获取。<a href="https://stackoverflow.com/questions/35336648/list-of-tensor-names-in-graph-in-tensorflow/" target="_blank" rel="external">参考stackoverflow</a> 。<br><strong>注意2</strong>：我们需要获取的层是除开作为输出softmax层和全连接层后的最后一层。通过graph.get_operations()来获取层名称信息，我们需要的层是<code>pool3:0</code>。<a href="https://stackoverflow.com/questions/35336648/list-of-tensor-names-in-graph-in-tensorflow/" target="_blank" rel="external">参考1 stackoverflow</a><a href="https://github.com/AKSHAYUBHAT/VisualSearchServer/blob/master/notebooks/notebook_network.ipynb" target="_blank" rel="external">参考2 inception v3模型结构信息</a>。</p>
<h3 id="训练数据处理"><a href="#训练数据处理" class="headerlink" title="训练数据处理"></a>训练数据处理</h3><p>训练数据就是<code>cifar10</code>这里参考文件中同样提供来一个<a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/cifar10.py" target="_blank" rel="external">cifar10.py</a>文件来实现<code>cifar10</code>数据的下载。<code>cifar10</code>是一个拥有6万数据量的10个分类的数据集，相比于<code>imageNet</code>的数据少了很多。我们要做的事下载数据后将这些数据进行特征提取也就是输入到<code>inception v3</code>中去处理，将<code>pool3:0</code>位置的tensor数据取出来并进行保存。这些保存好的数据就是我们迁移到新的神经网络上的输入数据。   </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> cifar10 <span class="comment"># 这是在利用自行编辑的数据下载文件</span></div><div class="line"><span class="keyword">from</span> cifar10 <span class="keyword">import</span> num_classes</div><div class="line"><span class="comment"># 下载数据并解压</span></div><div class="line">cifar10.maybe_download_and_extract()</div><div class="line"><span class="comment"># 获取分类名</span></div><div class="line">class_names = cifar10.load_class_names()</div><div class="line"><span class="comment"># 分别加载训练和验证数据</span></div><div class="line">images_train, cls_train, labels_train = cifar10.load_training_data()</div><div class="line">images_test, cls_test, labels_test = cifar10.load_test_data()</div></pre></td></tr></table></figure>
<h3 id="加载模型"><a href="#加载模型" class="headerlink" title="加载模型"></a>加载模型</h3><p>利用<code>inception.py</code>文件加载模型数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> incepton <span class="comment"># 这是在利用自行编辑的inception下载处理文件</span></div><div class="line"><span class="keyword">from</span> inception <span class="keyword">import</span> transfer_values_cache</div><div class="line"></div><div class="line"><span class="comment"># 下载inception文件，具体实现细节可以看上文中的链接了解</span></div><div class="line">inception.maybe_download()</div><div class="line"><span class="comment"># 创建incepton模型对象</span></div><div class="line">model = inception.Inception()</div><div class="line"><span class="comment"># 设置transfer-value(可以理解为上文中特征提取值)的保存路径</span></div><div class="line">file_path_cache_train = os.path.join(cifar10.data_path, <span class="string">'inception_cifar10_train.pkl'</span>)</div><div class="line">file_path_cache_test = os.path.join(cifar10.data_path, <span class="string">'inception_cifar10_test.pkl'</span>)</div><div class="line"></div><div class="line"><span class="comment"># Scale images because Inception needs pixels to be between 0 and 255,</span></div><div class="line"><span class="comment"># while the CIFAR-10 functions return pixels between 0.0 and 1.0</span></div><div class="line">images_scaled = images_train * <span class="number">255.0</span></div><div class="line"></div><div class="line"><span class="comment"># If transfer-values have already been calculated then reload them,</span></div><div class="line"><span class="comment"># otherwise calculate them and save them to a cache-file.</span></div><div class="line">transfer_values_train = transfer_values_cache(cache_path=file_path_cache_train,</div><div class="line">                                              images=images_scaled,</div><div class="line">                                              model=model)</div><div class="line">                                              </div><div class="line">images_scaled = images_test * <span class="number">255.0</span></div><div class="line"></div><div class="line"><span class="comment"># If transfer-values have already been calculated then reload them,</span></div><div class="line"><span class="comment"># otherwise calculate them and save them to a cache-file.</span></div><div class="line">transfer_values_test = transfer_values_cache(cache_path=file_path_cache_test,</div><div class="line">                                             images=images_scaled,</div><div class="line">                                             model=model)</div></pre></td></tr></table></figure>
<h3 id="Transfer-values分析"><a href="#Transfer-values分析" class="headerlink" title="Transfer-values分析"></a>Transfer-values分析</h3><h4 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h4><p>使用PCA去分析并绘制出<code>cifar10</code>经过特征提取后的文件分布情况是否存在规律。因为特征提取后的文件依然是个高纬度的数据，要在图形画出来我们将维度降低到2维。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div><div class="line"></div><div class="line">pca = PCA(n_components=<span class="number">2</span>)</div><div class="line"><span class="comment"># 只选取3000个数据来观察</span></div><div class="line">transfer_values = transfer_values_train[<span class="number">0</span>:<span class="number">3000</span>]</div><div class="line">cls = cls_train[<span class="number">0</span>:<span class="number">3000</span>]</div><div class="line">transfer_values_reduced = pca.fit_transform(transfer_values)</div></pre></td></tr></table></figure>
<p>但是这样绘制出来的效果似乎并不好。</p>
<h4 id="t-SNE-进行分析"><a href="#t-SNE-进行分析" class="headerlink" title="t-SNE 进行分析"></a>t-SNE 进行分析</h4><p>还有一种方法是使用t-SNE来进行分析，但是这个方法运行速度比较慢，所以可以先使用PCA预降维，再使用t-SNE进行处理。后可以看出来数据的分布基本很有规律。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</div><div class="line"></div><div class="line">pca = PCA(n_components=<span class="number">50</span>)</div><div class="line">transfer_values_50d = pca.fit_transform(transfer_values)</div><div class="line">tsne = TSNE(n_components=<span class="number">2</span>)</div><div class="line">transfer_values_reduced = tsne.fit_transform(transfer_values_50d)</div></pre></td></tr></table></figure>
<p><img src="/image/transLearnTF/tSNEAnly.png"></p>
<p><strong>注意</strong>：以上详细代码请<a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/08_Transfer_Learning.ipynb" target="_blank" rel="external">参考</a>。</p>
<h3 id="构建新的神经网络"><a href="#构建新的神经网络" class="headerlink" title="构建新的神经网络"></a>构建新的神经网络</h3><p>这里使用Placeholder变量的方式来输入训练数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">transfer_len = model.transfer_len</div><div class="line">x = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, transfer_len], name=<span class="string">'x'</span>)</div><div class="line">y_true = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, num_classes], name=<span class="string">'y_true'</span>)</div></pre></td></tr></table></figure>
<p>然后使用<code>Pretty Tensor</code>(可以通过pip安装)来构建一个新的神经网络。这样的构建方式更加简单方便。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment"># Wrap the transfer-values as a Pretty Tensor object.</span></div><div class="line">x_pretty = pt.wrap(x)</div><div class="line"></div><div class="line"><span class="keyword">with</span> pt.defaults_scope(activation_fn=tf.nn.relu):</div><div class="line">    y_pred, loss = x_pretty.\</div><div class="line">        fully_connected(size=<span class="number">1024</span>, name=<span class="string">'layer_fc1'</span>).\</div><div class="line">        softmax_classifier(num_classes=num_classes, labels=y_true)</div></pre></td></tr></table></figure>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>使用下面的代码随机的选择数据批量的数据进行训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_batch</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># Number of images (transfer-values) in the training-set.</span></div><div class="line">    num_images = len(transfer_values_train)</div><div class="line"></div><div class="line">    <span class="comment"># Create a random index.</span></div><div class="line">    <span class="comment"># 下面的这个函数将随机生成train_batch_size个数据</span></div><div class="line">    idx = np.random.choice(num_images,</div><div class="line">                           size=train_batch_size,</div><div class="line">                           replace=<span class="keyword">False</span>)</div><div class="line"></div><div class="line">    <span class="comment"># Use the random index to select random x and y-values.</span></div><div class="line">    <span class="comment"># We use the transfer-values instead of images as x-values.</span></div><div class="line">    x_batch = transfer_values_train[idx]</div><div class="line">    y_batch = labels_train[idx]</div><div class="line"></div><div class="line">    <span class="keyword">return</span> x_batch, y_batch</div></pre></td></tr></table></figure>
<p><strong>注意</strong>：对应英文文档还有一个比较好的<a href="https://zhuanlan.zhihu.com/p/27093918" target="_blank" rel="external">中文翻译</a>可以对照学习。</p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> tensorflow </tag>
            
            <tag> transfer learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Tensorflow学习笔记]]></title>
      <url>/2017/06/04/tensorflow/</url>
      <content type="html"><![CDATA[<h2 id="Tensorflow基本使用"><a href="#Tensorflow基本使用" class="headerlink" title="Tensorflow基本使用"></a>Tensorflow基本使用</h2><ul>
<li>使用图(graph)来表示计算任务。</li>
<li>在被称之为<code>会话(Session)的上下文(context)</code>中执行图。</li>
<li>使用tensor来表示数据。</li>
<li>通过<code>变量(Variable)</code>维护状态。</li>
<li>通过feed和fetch可以为任意的操作(arbitrary operation)赋值或者从其中获取数据。<a id="more"></a>
在Tensorflow图由它的操作和变量/常量构成。操作就比如普通的矩阵乘法<code>matmul</code>等，变量指那些可以用来保存状态并进行更新的值<code>Variable</code>，常量是<code>constant</code>。这个图可以理解为一个管道结构，我们的目的是根据这个图来计算更新变量或者得到这个图的输出值，但是仅仅靠一个图结构还不能完成这个工作。我们还需要输入数据，类比着看就是有了管道以后还需要向管道中通水。</li>
</ul>
<p>这就引出了Tensorflow的<code>Session</code>我们的图结构需要在这个<code>Session</code>中运行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="comment"># 创建一个1X2的常量矩阵</span></div><div class="line">matrix1 = tf.constant([[<span class="number">3.</span>, <span class="number">3.</span>]])</div><div class="line"><span class="comment"># 创建一个2X1的常量矩阵</span></div><div class="line">matrix2 = tf.constant([[<span class="number">2.</span>], [<span class="number">2.</span>]])</div><div class="line"><span class="comment"># 执行矩阵乘法</span></div><div class="line">product = tf.matmul(matrix1, matrix2)</div><div class="line"><span class="comment"># 创建session这里使用了with as 的方法，如果没有的话记得sess.close()关闭session</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">	<span class="comment"># 执行图</span></div><div class="line">	result = sess.run(product)</div><div class="line">	<span class="comment"># 打印结果</span></div><div class="line">	print(result)</div></pre></td></tr></table></figure>
<p>tensorflow会在运行的机器上寻找GPU如果有的话会优先使用GPU，如果没有会找CPU使用。如果有多个GPU可用，tensorflow会默认使用第一个找到的GPU，当然你可以指定。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">	<span class="keyword">with</span> tf.device(<span class="string">"/gpu:1"</span>):</div><div class="line">	...</div><div class="line">	...</div></pre></td></tr></table></figure></p>
<ul>
<li><code>&quot;/cpu:0&quot;</code>:机器的CPU。</li>
<li><code>&quot;/gpu:0&quot;</code>:机器的第一个GPU。</li>
<li><code>&quot;/gpu:1&quot;</code>:机器的第二个GPU。</li>
</ul>
<p>对于变量还需要特别说明下，变量在使用前都需要进行初始化。当在图中使用一个创建一个变量的时候我们会给这个变量指定一个初始值或者初始化的方式，但是要记住的是这个时候只是一个管道结构而已实际上这个变量还并没有执行初始化操作，我们需要在使用变量前通过<code>tf.initialize_all_variables() sess.run(init_op)</code>操作将所有的变量进行一次执行初始化操作。</p>
<p>有时候也存在使用一个已经定义的变量来初始化另外一个变量的情况，这个时候需要使用<code>initialized_value()</code><br>举例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create a variable with a random value.</span></div><div class="line">weights = tf.Variable(tf.random_normal([<span class="number">784</span>, <span class="number">200</span>], stddev=<span class="number">0.35</span>),name=<span class="string">"weights"</span>)</div><div class="line"><span class="comment"># Create another variable with the same value as 'weights'.</span></div><div class="line">w2 = tf.Variable(weights.initialized_value(), name=<span class="string">"w2"</span>)</div><div class="line"><span class="comment"># Create another variable with twice the value of 'weights'</span></div><div class="line">w_twice = tf.Variable(weights.initialized_value() * <span class="number">0.2</span>, name=<span class="string">"w_twice"</span>)</div></pre></td></tr></table></figure></p>
<h2 id="搭建神经网络注意项"><a href="#搭建神经网络注意项" class="headerlink" title="搭建神经网络注意项"></a>搭建神经网络注意项</h2><p>具体搭建过程参考这篇<a href="http://www.tensorfly.cn/tfdoc/tutorials/mnist_pros.html" target="_blank" rel="external">文章</a>，<br>tf.Session() 和 tf.InteractiveSession()的区别：Session()的方式可以使用python中with…as的形式，这样就可以不用单独执行close()操作，但是在每次需要执行图计算的时候需要以sess.run()的形式进行，对于tf.InteractiveSession()来说，不能使用with…as的形式，所以在最后都需要进行显式的close()操作，但是它可以让每次要进行图计算的时候只进行Tensor.eval()即可，这样更灵活。</p>
<p>在tensorflow中使用tensor来表示所有数据，这点和pytorch不同，tensorflow不需要通过一个变量来单独定义tensor，因为在tensorflow看来这些数据都是tensor。</p>
<h3 id="占位符"><a href="#占位符" class="headerlink" title="占位符"></a>占位符</h3><p>tf.placeholder作用是为一个将被频繁使用的tensor插入一个占位符。这个占位符实际也是一个tensor只是不能直接使用。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">placeholder(dtype, shape=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</div></pre></td></tr></table></figure></p>
<p><strong>注意</strong>直接使用placeholder定义的tensor会返回错误，你需要在<code>Session.run()</code>，<code>Tensor.eval()</code>或者<code>Operation.run()</code>通过<code>feed_dict</code>参数传入符合placeholder定义的tensor才行。<br>举例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">x = tf.placeholder(tf.float32, shape=(<span class="number">1024</span>, <span class="number">1024</span>))</div><div class="line">y = tf.matmul(x, x)</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  print(sess.run(y))  <span class="comment"># ERROR: will fail because x was not fed.</span></div><div class="line"></div><div class="line">  rand_array = np.random.rand(<span class="number">1024</span>, <span class="number">1024</span>)</div><div class="line">  print(sess.run(y, feed_dict=&#123;x: rand_array&#125;))  <span class="comment"># Will succeed.</span></div></pre></td></tr></table></figure>
<p>参数：  </p>
<ul>
<li><code>dtype</code>：tensor的元素类型。      </li>
<li><code>shape</code>：tensor的尺寸，如果将某个大小填写为None表示这个位置由系统根据条件给出。类似reshape操作中如果是tf.reshape(a, [-1,9])中-1表示系统自行根据元素个数决定行数，只要保证列数位9即可。   </li>
<li><code>name</code>：操作的名称</li>
</ul>
<h3 id="部分函数"><a href="#部分函数" class="headerlink" title="部分函数"></a>部分函数</h3><p><code>tf.reduce_sum()</code>作为求和函数<br><code>tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)</code>表示一个使用梯度下降的优化方法。优化的步长是0.01，优化的对象是最小化cross_entropy函数，这个函数cross_entropy需要你自行定义。</p>
<p>举例：这里表现的是神经网络中进行训练的代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cross_entropy)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</div><div class="line">  <span class="comment"># 将输入数据按照batch来拆分</span></div><div class="line">  batch = mnist.train.next_batch(<span class="number">50</span>)</div><div class="line">  <span class="comment"># 将一个batch的输入输入到train操作中</span></div><div class="line">  train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>]&#125;)</div></pre></td></tr></table></figure></p>
<p><strong>注意</strong> feed_dict这个不仅可以用来作为占位符的具体输入，也可以用来替换其它tensor。</p>
<p><code>tf.argmax(input, axis=None, name=None, dimension=None)</code>可以用来获取最大值所在的位置，如果输入是一个向量，axis填写位0。axis为1返回每行的最大值位置索引。通过这个函数可以实现对模型的正确率的评估。</p>
<h3 id="权值初始化"><a href="#权值初始化" class="headerlink" title="权值初始化"></a>权值初始化</h3><p>因为是一个神经网络将权值初始化为0学习过程将无法开始，这里我们使用<code>tf.truncated_normal(shape, mean=0.0, stddev=1.0)</code>(这里只列出需要的两个参数)来进行初始化，因为希望初始化后的值小点，所以将stddev标准差设置为0.1也就是使用更加集中在0附近的点的进行初始化。在pytorch中似乎是不需要进行我们进行权值的初始化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></div><div class="line">  initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</div><div class="line">  <span class="keyword">return</span> tf.Variable(initial)</div></pre></td></tr></table></figure>
<h3 id="卷积层和池化层"><a href="#卷积层和池化层" class="headerlink" title="卷积层和池化层"></a>卷积层和池化层</h3><p>使用<code>tf.nn.conv2d()</code>来定义卷积层，主要用来获取特征。<br>使用<code>tf.nn.max_pool()</code>来定义池化层，池化层的作用主要用来进行下采样，进一步降低数据量。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">conv2d(input, filter, strides, padding)这里只介绍这几个参数。</div></pre></td></tr></table></figure></p>
<ul>
<li>input：Tensor。必须是：half，float，float64 类型。一个4D tensor。</li>
<li>filter：Tensor。类型和input一致。一个4D tensor，大小满足[filter_height, filter_width, in_channels, out_channels]</li>
<li>strides：ints的列表。长度为4的1D tensor。表示输入的每个维度的滑动距离。对应这里[image-number, filter_height, filter_width, input-channel]其中mage-number,input-channel设置为1的话表示一个图片一个通道的去处理。</li>
<li>padding：类型字符层，可选“SAME”“VALID”，比如6的长度，窗口是4每次滑动4。如果是“SAME”就会补差的值，如果是“VALID”就会抛弃差的值。</li>
</ul>
<h3 id="变量的保存和加载"><a href="#变量的保存和加载" class="headerlink" title="变量的保存和加载"></a>变量的保存和加载</h3><p><code>tf.train.Saver()</code>创建一个<code>Saver</code>来管理模型中的所有变量。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">saver = tf.train.Saver()</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  <span class="comment"># Save the variables to disk.</span></div><div class="line">  save_path = saver.save(sess, <span class="string">"/tmp/model.ckpt"</span>)</div></pre></td></tr></table></figure></p>
<p>使用同一个<code>Saver</code>对象来恢复变量。注意，当你从文件中恢复变量时，不需要事先对它们做初始化。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">saver = tf.train.Saver()</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  saver.restore(sess, <span class="string">"/tmp/model.ckpt"</span>)</div></pre></td></tr></table></figure></p>
<h2 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h2><h3 id="读取csv文件"><a href="#读取csv文件" class="headerlink" title="读取csv文件"></a>读取csv文件</h3><p><a href="http://www.tensorfly.cn/tfdoc/how_tos/reading_data.html" target="_blank" rel="external">参考文件</a>有些情况下我们会收到.csv的数据文件。现在得考虑如何将这些数据读入到模型中以便开始训练。<br>一般读取文件的步骤：</p>
<ul>
<li>生成文件名列表</li>
<li>根据文件名列表生成文件名队列</li>
<li>选择合适的阅读器</li>
<li>选择合适的解析器</li>
</ul>
<p>中间会将文件名队列放在另外的线程中。这个线程负责将文件逐次输入到文件名称队列中。这个过程和阅读器工作独立。<br><strong>生成文件列表</strong>：使用列表生成式，或者手动写入<code>[&quot;file0&quot;, &quot;file1&quot;]</code>。<br><strong>根据文件名列表生成文件名队列</strong>：<code>tf.train.string_input_producer()</code>如果设置<code>shuffle=True</code>将对文件名进行乱序处理。<br><strong>选择合适的阅读器</strong>：reader = tf.TextLineReader()，reader输入文件队列，输出key，value。key对应文件，value对应文件的内容。<br><strong>选择合适的阅读器</strong>：tf.decode_csv()解析csv文件，输入value<br><strong>启Coordinator</strong>：coord=tf.train.Coordinator()作用是在出现错误的时可以正确的关闭下面的线程<br><strong>启动队列runner</strong>：threads=tf.train.start_queue_runners(coord=coord)这样文件名队列才开始入队数据，后续才能进行出队操作。<br><strong>进行操作</strong>：session.run()<br><strong>关闭线程</strong>：申请停止线程coord.request_stop()，等待threads线程结束coord.join(threads)<br>举例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 创建文件队列</span></div><div class="line">filename_queue = tf.train.string_input_producer([<span class="string">"file0.csv"</span>, <span class="string">"file1.csv"</span>])</div><div class="line"><span class="comment"># 创建阅读器</span></div><div class="line">reader = tf.TextLineReader()</div><div class="line"><span class="comment"># 阅读器读取文件返回key 和 value</span></div><div class="line">key, value = reader.read(filename_queue)</div><div class="line"></div><div class="line"><span class="comment"># 设置读取文件每一行记录的时候如果有位置为空时的默认值</span></div><div class="line">record_defaults = [[<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">1</span>]]</div><div class="line"><span class="comment"># csv解析器解析value</span></div><div class="line">col1, col2, col3, col4, col5 = tf.decode_csv(value, record_defaults=record_defaults)</div><div class="line"><span class="comment"># 将col1, col2, col3, col4按照0行进行连接为新的特征</span></div><div class="line"><span class="comment"># 举例a=[[1]] b=[[2]]按照行连接后为[[1],[2]]</span></div><div class="line">features = tf.concat(<span class="number">0</span>, [col1, col2, col3, col4])</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  <span class="comment">#创建线程协调</span></div><div class="line">  coord = tf.train.Coordinator()</div><div class="line">  <span class="comment"># 开启线程</span></div><div class="line">  threads = tf.train.start_queue_runners(coord=coord)</div><div class="line"></div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1200</span>):</div><div class="line">    <span class="comment"># 返回特征和标签</span></div><div class="line">    example, label = sess.run([features, col5])</div><div class="line"></div><div class="line">  <span class="comment"># 停止线程</span></div><div class="line">  coord.request_stop()</div><div class="line">  <span class="comment"># 等待线程停止</span></div><div class="line">  coord.join(threads)</div></pre></td></tr></table></figure>
<h3 id="通过TFRecord读取数据"><a href="#通过TFRecord读取数据" class="headerlink" title="通过TFRecord读取数据"></a>通过TFRecord读取数据</h3><p><a href="http://www.tensorfly.cn/tfdoc/how_tos/reading_data.html" target="_blank" rel="external">参考文件</a>还有种更节约空间的Tensorflow自己的文件格式TFRecord。我们要完成的事情包括将原始文件转换为TFRecord和从TFRecord种读取数据出来。下面是以图片csv文件转换为TFRecord来说明。下面的代码来自参考文件。<br><strong>转换为TFRecord文件</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_to</span><span class="params">(data_set, name)</span>:</span></div><div class="line">  <span class="string">"""Converts a dataset to tfrecords."""</span></div><div class="line">  images = data_set.images</div><div class="line">  labels = data_set.labels</div><div class="line">  num_examples = data_set.num_examples</div><div class="line"></div><div class="line">  <span class="keyword">if</span> images.shape[<span class="number">0</span>] != num_examples:<span class="keyword">raise</span> ValueError(<span class="string">'Images size %d does not match label size %d.'</span> %(images.shape[<span class="number">0</span>], num_examples))</div><div class="line">  rows = images.shape[<span class="number">1</span>]</div><div class="line">  cols = images.shape[<span class="number">2</span>]</div><div class="line">  depth = images.shape[<span class="number">3</span>]</div><div class="line"></div><div class="line">  filename = os.path.join(FLAGS.directory, name + <span class="string">'.tfrecords'</span>)</div><div class="line">  print(<span class="string">'Writing'</span>, filename)</div><div class="line">  writer = tf.python_io.TFRecordWriter(filename)</div><div class="line">  <span class="keyword">for</span> index <span class="keyword">in</span> range(num_examples):</div><div class="line">    <span class="comment"># 将图片转换为字符串</span></div><div class="line">    image_raw = images[index].tostring()</div><div class="line">    <span class="comment"># 将图片信息转换到example</span></div><div class="line">    example = tf.train.Example(features=tf.train.Features(feature=&#123;</div><div class="line">        <span class="string">'height'</span>: _int64_feature(rows),</div><div class="line">        <span class="string">'width'</span>: _int64_feature(cols),</div><div class="line">        <span class="string">'depth'</span>: _int64_feature(depth),</div><div class="line">        <span class="string">'label'</span>: _int64_feature(int(labels[index])),</div><div class="line">        <span class="string">'image_raw'</span>: _bytes_feature(image_raw)&#125;))</div><div class="line">    <span class="comment"># 写入文件</span></div><div class="line">    writer.write(example.SerializeToString())</div><div class="line">  writer.close()</div></pre></td></tr></table></figure></p>
<p><strong>从TFRecord文件读取</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_and_decode</span><span class="params">(filename_queue)</span>:</span></div><div class="line">  <span class="comment"># 创建阅读器</span></div><div class="line">  reader = tf.TFRecordReader()</div><div class="line">  _, serialized_example = reader.read(filename_queue)</div><div class="line">  <span class="comment"># 创建解析器</span></div><div class="line">  features = tf.parse_single_example(</div><div class="line">      serialized_example,</div><div class="line">      <span class="comment"># Defaults are not specified since both keys are required.</span></div><div class="line">      features=&#123;</div><div class="line">          <span class="string">'image_raw'</span>: tf.FixedLenFeature([], tf.string),</div><div class="line">          <span class="string">'label'</span>: tf.FixedLenFeature([], tf.int64),</div><div class="line">      &#125;)</div><div class="line"></div><div class="line">  <span class="comment"># Convert from a scalar string tensor (whose single string has</span></div><div class="line">  <span class="comment"># length mnist.IMAGE_PIXELS) to a uint8 tensor with shape</span></div><div class="line">  <span class="comment"># [mnist.IMAGE_PIXELS].</span></div><div class="line">  image = tf.decode_raw(features[<span class="string">'image_raw'</span>], tf.uint8)</div><div class="line">  image.set_shape([mnist.IMAGE_PIXELS])</div><div class="line"></div><div class="line">  <span class="comment"># OPTIONAL: Could reshape into a 28x28 image and apply distortions</span></div><div class="line">  <span class="comment"># here.  Since we are not applying any distortions in this</span></div><div class="line">  <span class="comment"># example, and the next step expects the image to be flattened</span></div><div class="line">  <span class="comment"># into a vector, we don't bother.</span></div><div class="line"></div><div class="line">  <span class="comment"># Convert from [0, 255] -&gt; [-0.5, 0.5] floats.</span></div><div class="line">  image = tf.cast(image, tf.float32) * (<span class="number">1.</span> / <span class="number">255</span>) - <span class="number">0.5</span></div><div class="line"></div><div class="line">  <span class="comment"># Convert label from a scalar uint8 tensor to an int32 scalar.</span></div><div class="line">  label = tf.cast(features[<span class="string">'label'</span>], tf.int32)</div><div class="line"></div><div class="line">  <span class="keyword">return</span> image, label</div></pre></td></tr></table></figure></p>
<p>参考文档:<a href="http://www.tensorfly.cn/tfdoc/get_started/basic_usage.html" target="_blank" rel="external">Tensorflow官方文档中文版</a></p>
]]></content>
      
        <categories>
            
            <category> frame </category>
            
        </categories>
        
        
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Translation with a Sequence to Sequence Network and Attention(翻译)]]></title>
      <url>/2017/05/26/TransRnn/</url>
      <content type="html"><![CDATA[<p>原文链接：<a href="http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html" target="_blank" rel="external">Translation with a Sequence to Sequence Network and Attention</a><br>作者：<a href="https://github.com/spro/practical-pytorch" target="_blank" rel="external">Sean Robertson</a></p>
<p>在这个项目中我们将训练一个神经网络将法语翻译为英语。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">[KEY: &gt; input, = target, &lt; output]</div><div class="line"></div><div class="line">&gt; il est en train de peindre un tableau .</div><div class="line">= he is painting a picture .</div><div class="line">&lt; he is painting a picture .</div><div class="line"></div><div class="line">&gt; pourquoi ne pas essayer ce vin delicieux ?</div><div class="line">= why not try that delicious wine ?</div><div class="line">&lt; why not try that delicious wine ?</div><div class="line"></div><div class="line">&gt; elle n est pas poete mais romanciere .</div><div class="line">= she is not a poet but a novelist .</div><div class="line">&lt; she not not a poet but a novelist .</div><div class="line"></div><div class="line">&gt; vous etes trop maigre .</div><div class="line">= you re too skinny .</div><div class="line">&lt; you re all alone .</div></pre></td></tr></table></figure></p>
<p>…不同程度的成功。<br><a id="more"></a><br>一个简单但是有力的<a href="http://arxiv.org/abs/1409.3215" target="_blank" rel="external">序列到序列网络</a>的想法让翻译变的可能，它由两个循环神经网络组成一起将一个序列翻译为另外一个序列。一个编码网络将输入序列凝聚为向量，一个解码网络将向量张开为新的序列。</p>
<p><img src="/image/seq2seq/seq2seq.png"></p>
<p>为了改善上面的模型我们将使用<a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="external">注意力机制</a>，它将使得解码器学会将注意力集中在输入学历的特定部分。</p>
<h2 id="Recommended-Reading-推荐阅读"><a href="#Recommended-Reading-推荐阅读" class="headerlink" title="Recommended Reading(推荐阅读)"></a>Recommended Reading(推荐阅读)</h2><p>我假设你目前至少安装来PyTorch，了解Python，并且理解Tensor：  </p>
<ul>
<li><a href="http://pytorch.org" target="_blank" rel="external">http://pytorch.org</a>从这里获取安装指导</li>
<li><a href="http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="external">Deep Learning with PyTorch: A 60 Minute Blitz</a>一般从这里开始PyToch</li>
<li><a href="http://pytorch.org/tutorials/beginner/pytorch_with_examples.html" target="_blank" rel="external">Learning PyTorch with Examples</a>获取更有广度和深度的概述</li>
<li><a href="http://pytorch.org/tutorials/beginner/former_torchies_tutorial.html" target="_blank" rel="external">PyTorch for former Torch users</a>如果你是Lua Torch用户</li>
</ul>
<p>了解序列到序列神经网络和它们是如何工作对后面的学习将非常有用。</p>
<ul>
<li><a href="http://arxiv.org/abs/1406.1078" target="_blank" rel="external">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a></li>
<li><a href="http://arxiv.org/abs/1409.3215" target="_blank" rel="external">Sequence to Sequence Learning with Neural Networks</a></li>
<li><a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="external">Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
<li><a href="http://arxiv.org/abs/1506.05869" target="_blank" rel="external">A Neural Conversational Model</a></li>
</ul>
<p>你将会发现前面的指导<a href="http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html" target="_blank" rel="external">Classifying Names with a Character-Level RNN</a>和<a href="http://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html" target="_blank" rel="external">Generating Names with a Character-Level RNN</a>很有用因为这两篇文章的一些概念和本文编码解码模型中的概念非常相似。</p>
<p>关于这个主题更多的信息，可以阅读下面的这些论文：</p>
<ul>
<li><a href="http://arxiv.org/abs/1406.1078" target="_blank" rel="external">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a></li>
<li><a href="http://arxiv.org/abs/1409.3215" target="_blank" rel="external">Sequence to Sequence Learning with Neural Networks</a></li>
<li><a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="external">Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
<li><a href="http://arxiv.org/abs/1506.05869" target="_blank" rel="external">A Neural Conversational Model</a></li>
</ul>
<p><strong>Requirements(依赖)</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</div><div class="line"><span class="keyword">from</span> io <span class="keyword">import</span> open</div><div class="line"><span class="keyword">import</span> unicodedata</div><div class="line"><span class="keyword">import</span> string</div><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">import</span> random</div><div class="line"></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"></div><div class="line">use_cuda = torch.cuda.is_available()</div></pre></td></tr></table></figure></p>
<h2 id="Loading-data-files-加载文件"><a href="#Loading-data-files-加载文件" class="headerlink" title="Loading data files(加载文件)"></a>Loading data files(加载文件)</h2><p>这个项目的数据是数以千计的英语到法语翻译组合的数据集。</p>
<p>在问答平台<a href="http://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages" target="_blank" rel="external">Open Data Stack Exchange</a>得到了一个开源翻译网站<a href="http://tatoeba.org/" target="_blank" rel="external">http://tatoeba.org/</a>可以下载获取数据在这个路径<a href="http://tatoeba.org/eng/downloads" target="_blank" rel="external">http://tatoeba.org/eng/downloads</a>-更好的是，一些人做了些额外的工作将切分语言组合到独立的文件：<br><a href="http://www.manythings.org/anki/" target="_blank" rel="external">http://www.manythings.org/anki/</a></p>
<p>英语到法语对因为太大不便放到库中，因此在继续开始前下载<code>data/eng-fra.txt</code>文件。这个文件使用tab键来分割翻译对：</p>
<blockquote>
<p>I am cold.    Je suis froid.</p>
</blockquote>
<p><strong>Note</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">从[这里](https://download.pytorch.org/tutorial/data.zip)下载文件并解压到当前路径。</div></pre></td></tr></table></figure></p>
<p>同在字符级RNN指导中字符编码相似，我们表示语言中的词汇使用的是one-hot向量，或者是特大的只有一个位置为1(表示词汇出现的位置)其余为0的向量。同语言中可能出现的字符相比，这里会存在更多的词汇，因此编码的向量将非常庞大。然而我们将做些简化处理裁剪一些词汇每个语言只使用几千个词语。</p>
<p><img src="/image/seq2seq/word-encoding.png"></p>
<p>我们需要每个词汇有个单独的索引用来作为神经网络的输入和目标。为了保持对这些索引的跟踪我们会使用一个很有用的类<code>lang</code>包含了词-&gt;索引(<code>word2index</code>)和索引-&gt;词(index2word)字典，同时还有对词计数的<code>word2count</code>后续用来对数量很少的词进行替换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">SOS_token = <span class="number">0</span></div><div class="line">EOS_token = <span class="number">1</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Lang</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></div><div class="line">        self.name = name</div><div class="line">        self.word2index = &#123;&#125;</div><div class="line">        self.word2count = &#123;&#125;</div><div class="line">        self.index2word = &#123;<span class="number">0</span>: <span class="string">"SOS"</span>, <span class="number">1</span>: <span class="string">"EOS"</span>&#125;</div><div class="line">        self.n_words = <span class="number">2</span>  <span class="comment"># Count SOS and EOS</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addSentence</span><span class="params">(self, sentence)</span>:</span></div><div class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sentence.split(<span class="string">' '</span>):</div><div class="line">            self.addWord(word)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addWord</span><span class="params">(self, word)</span>:</span></div><div class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> self.word2index:</div><div class="line">            self.word2index[word] = self.n_words</div><div class="line">            self.word2count[word] = <span class="number">1</span></div><div class="line">            self.index2word[self.n_words] = word</div><div class="line">            self.n_words += <span class="number">1</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self.word2count[word] += <span class="number">1</span></div></pre></td></tr></table></figure>
<p>这些文件都是Unicode编码，为了简单我们将Unicode字符转换为ASCII，将字符统一为小写，剔除标点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Turn a Unicode string to plain ASCII, thanks to</span></div><div class="line"><span class="comment"># http://stackoverflow.com/a/518232/2809427</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">unicodeToAscii</span><span class="params">(s)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(</div><div class="line">        c <span class="keyword">for</span> c <span class="keyword">in</span> unicodedata.normalize(<span class="string">'NFD'</span>, s)</div><div class="line">        <span class="keyword">if</span> unicodedata.category(c) != <span class="string">'Mn'</span></div><div class="line">    )</div><div class="line"></div><div class="line"><span class="comment"># Lowercase, trim, and remove non-letter characters</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalizeString</span><span class="params">(s)</span>:</span></div><div class="line">    s = unicodeToAscii(s.lower().strip())</div><div class="line">    s = re.sub(<span class="string">r"([.!?])"</span>, <span class="string">r" \1"</span>, s)</div><div class="line">    s = re.sub(<span class="string">r"[^a-zA-Z.!?]+"</span>, <span class="string">r" "</span>, s)</div><div class="line">    <span class="keyword">return</span> s</div></pre></td></tr></table></figure>
<p>为了读取数据文件我们将文件拆解为行，将行信息拆解为对。这些文件都是英语-&gt;其它语言，因此为了我们能从其它语言-&gt;英语我添加了一个<code>reverse</code>标志来转换翻译语言之间的方向。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">readLangs</span><span class="params">(lang1, lang2, reverse=False)</span>:</span></div><div class="line">    print(<span class="string">"Reading lines..."</span>)</div><div class="line"></div><div class="line">    <span class="comment"># Read the file and split into lines</span></div><div class="line">    lines = open(<span class="string">'data/%s-%s.txt'</span> % (lang1, lang2), encoding=<span class="string">'utf-8'</span>).\</div><div class="line">        read().strip().split(<span class="string">'\n'</span>)</div><div class="line"></div><div class="line">    <span class="comment"># Split every line into pairs and normalize</span></div><div class="line">    pairs = [[normalizeString(s) <span class="keyword">for</span> s <span class="keyword">in</span> l.split(<span class="string">'\t'</span>)] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</div><div class="line"></div><div class="line">    <span class="comment"># Reverse pairs, make Lang instances</span></div><div class="line">    <span class="keyword">if</span> reverse:</div><div class="line">        pairs = [list(reversed(p)) <span class="keyword">for</span> p <span class="keyword">in</span> pairs]</div><div class="line">        input_lang = Lang(lang2)</div><div class="line">        output_lang = Lang(lang1)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        input_lang = Lang(lang1)</div><div class="line">        output_lang = Lang(lang2)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> input_lang, output_lang, pairs</div></pre></td></tr></table></figure>
<p>在数据文件中有非常多的各种长度的句子然而我们希望训练可以更简单更快的进行，所以我们将对数据进行裁剪只保留简短和简单的句子。这里限制最大句子长度是10个单词(包括了结束符号)然后我们将过滤出那些将翻译为”I am”或者“He is”等样式的句子(撇号在先前已被替换)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">MAX_LENGTH = <span class="number">10</span></div><div class="line"></div><div class="line">eng_prefixes = (</div><div class="line">    <span class="string">"i am "</span>, <span class="string">"i m "</span>,</div><div class="line">    <span class="string">"he is"</span>, <span class="string">"he s "</span>,</div><div class="line">    <span class="string">"she is"</span>, <span class="string">"she s"</span>,</div><div class="line">    <span class="string">"you are"</span>, <span class="string">"you re "</span>,</div><div class="line">    <span class="string">"we are"</span>, <span class="string">"we re "</span>,</div><div class="line">    <span class="string">"they are"</span>, <span class="string">"they re "</span></div><div class="line">)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">filterPair</span><span class="params">(p)</span>:</span></div><div class="line">    <span class="keyword">return</span> len(p[<span class="number">0</span>].split(<span class="string">' '</span>)) &lt; MAX_LENGTH <span class="keyword">and</span> \</div><div class="line">        len(p[<span class="number">1</span>].split(<span class="string">' '</span>)) &lt; MAX_LENGTH <span class="keyword">and</span> \</div><div class="line">        p[<span class="number">1</span>].startswith(eng_prefixes)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">filterPairs</span><span class="params">(pairs)</span>:</span></div><div class="line">    <span class="keyword">return</span> [pair <span class="keyword">for</span> pair <span class="keyword">in</span> pairs <span class="keyword">if</span> filterPair(pair)]</div></pre></td></tr></table></figure>
<p>整个准备数据的过程如下：</p>
<ul>
<li>读取文件将文件信息按行拆分，再将行信息拆分层句子对</li>
<li>格式化文档，通过句子长度和内容过滤文档</li>
<li>从句子对中制作词汇表</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepareData</span><span class="params">(lang1, lang2, reverse=False)</span>:</span></div><div class="line">    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)</div><div class="line">    print(<span class="string">"Read %s sentence pairs"</span> % len(pairs))</div><div class="line">    pairs = filterPairs(pairs)</div><div class="line">    print(<span class="string">"Trimmed to %s sentence pairs"</span> % len(pairs))</div><div class="line">    print(<span class="string">"Counting words..."</span>)</div><div class="line">    <span class="keyword">for</span> pair <span class="keyword">in</span> pairs:</div><div class="line">        input_lang.addSentence(pair[<span class="number">0</span>])</div><div class="line">        output_lang.addSentence(pair[<span class="number">1</span>])</div><div class="line">    print(<span class="string">"Counted words:"</span>)</div><div class="line">    print(input_lang.name, input_lang.n_words)</div><div class="line">    print(output_lang.name, output_lang.n_words)</div><div class="line">    <span class="keyword">return</span> input_lang, output_lang, pairs</div><div class="line"></div><div class="line"></div><div class="line">input_lang, output_lang, pairs = prepareData(<span class="string">'eng'</span>, <span class="string">'fra'</span>, <span class="keyword">True</span>)</div><div class="line">print(random.choice(pairs))</div></pre></td></tr></table></figure>
<p><strong>Out</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">Reading lines...</div><div class="line">Read 135842 sentence pairs</div><div class="line">Trimmed to 10853 sentence pairs</div><div class="line">Counting words...</div><div class="line">Counted words:</div><div class="line">fra 4489</div><div class="line">eng 2925</div><div class="line">[&apos;nous sommes faits l un pour l autre .&apos;, &apos;we re meant for each other .&apos;]</div></pre></td></tr></table></figure></p>
<h2 id="The-Seq2Seq-Model-序列到序列模型"><a href="#The-Seq2Seq-Model-序列到序列模型" class="headerlink" title="The Seq2Seq Model(序列到序列模型)"></a>The Seq2Seq Model(序列到序列模型)</h2><p>循环神经网络，或者叫RNN，是一种操作序列和使用自身的输出作为后续步骤输入的神经网络。</p>
<p>一个<a href="http://arxiv.org/abs/1409.3215" target="_blank" rel="external">序列到序列网络</a>，或者seq2seq神经网络，或者<a href="https://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="external">编码解码神经网络</a>，是一个包含了两个RNN分别作为编码器和解码器的模型。编码器读取输入序列输出单独的向量，然后解码器读取此向量产生输出序列。</p>
<p><img src="/image/seq2seq/seq2seq.png"></p>
<p>不同于单个用来进行序列预测的RNN，每个输入对应了一个输出，seq2seq模型将我们从序列长度中释放出来，这对于两种语言之间的翻译非常理想。</p>
<p>考虑这样的句子”Je ne suis pas le chat noir”-&gt;”I am not the black cat”。大部分输入序列的词可以直接翻译到输出序列，但有些细微的区别，e.g.“chat(猫) noir(黑)”和“black (黑)cat(猫)”。因为“ne/pas”结构在输入序列中也会多一个词出来。这个会给根据输入序列来直接产生一个正确的翻译结果造成困难。</p>
<p>使用seq2seq模型编码器创建了一个单独的向量，在理想情况下，编码输入序列的“含义”到一个单独的向量-一个在N为句子空间中的一个点。</p>
<h3 id="The-Encoder-编码器"><a href="#The-Encoder-编码器" class="headerlink" title="The Encoder(编码器)"></a>The Encoder(编码器)</h3><p>seq2seq网络的编码器是一个RNN它对输入句子的每个单词都输出一些值。每个输入编码器的单词输出一个向量和一个隐藏状态，然后下一个输入的单词将使用这个隐藏状态。</p>
<p><img src="/image/seq2seq/encoder-network.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderRNN</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, n_layers=<span class="number">1</span>)</span>:</span></div><div class="line">        super(EncoderRNN, self).__init__()</div><div class="line">        self.n_layers = n_layers</div><div class="line">        self.hidden_size = hidden_size</div><div class="line"></div><div class="line">        self.embedding = nn.Embedding(input_size, hidden_size)</div><div class="line">        self.gru = nn.GRU(hidden_size, hidden_size)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden)</span>:</span></div><div class="line">        embedded = self.embedding(input).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>)</div><div class="line">        output = embedded</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_layers):</div><div class="line">            output, hidden = self.gru(output, hidden)</div><div class="line">        <span class="keyword">return</span> output, hidden</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></div><div class="line">        result = Variable(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size))</div><div class="line">        <span class="keyword">if</span> use_cuda:</div><div class="line">            <span class="keyword">return</span> result.cuda()</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> result</div></pre></td></tr></table></figure>
<h3 id="The-Decoder-解码器"><a href="#The-Decoder-解码器" class="headerlink" title="The Decoder(解码器)"></a>The Decoder(解码器)</h3><p>解码器是另外一个RNN它接收编码器输出的向量然后输出单词的序列来进行翻译。</p>
<h4 id="Simple-Decoder-简单解码器"><a href="#Simple-Decoder-简单解码器" class="headerlink" title="Simple Decoder(简单解码器)"></a>Simple Decoder(简单解码器)</h4><p>在最简单的seq2seq解码器中我们仅仅使用编码器的最后一层输出。最后一层输出有时也叫做<em>上下文向量</em>它是整个输入序列的编码。这个上下文向量用来初始化解码器隐藏状态。</p>
<p>在每一个解码步骤，解码器得到的是输入字符和隐藏状态。初始化的输入字符是字符串开始标志<code>&lt;SOS&gt;</code>，第一个隐藏状态是上下文向量(编码器的最后一个隐藏状态)。</p>
<p><img src="/image/seq2seq/decoder-network.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderRNN</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_size, output_size, n_layers=<span class="number">1</span>)</span>:</span></div><div class="line">        super(DecoderRNN, self).__init__()</div><div class="line">        self.n_layers = n_layers</div><div class="line">        self.hidden_size = hidden_size</div><div class="line"></div><div class="line">        self.embedding = nn.Embedding(output_size, hidden_size)</div><div class="line">        self.gru = nn.GRU(hidden_size, hidden_size)</div><div class="line">        self.out = nn.Linear(hidden_size, output_size)</div><div class="line">        self.softmax = nn.LogSoftmax()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden)</span>:</span></div><div class="line">        output = self.embedding(input).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>)</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_layers):</div><div class="line">            output = F.relu(output)</div><div class="line">            output, hidden = self.gru(output, hidden)</div><div class="line">        output = self.softmax(self.out(output[<span class="number">0</span>]))</div><div class="line">        <span class="keyword">return</span> output, hidden</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></div><div class="line">        result = Variable(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size))</div><div class="line">        <span class="keyword">if</span> use_cuda:</div><div class="line">            <span class="keyword">return</span> result.cuda()</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> result</div></pre></td></tr></table></figure>
<p>我鼓励你训练和观察这个模型的结果，但是为了节约空间我们将直接介绍重要的注意力机制。</p>
<h4 id="Attention-Decoder-注意力解码器"><a href="#Attention-Decoder-注意力解码器" class="headerlink" title="Attention Decoder(注意力解码器)"></a>Attention Decoder(注意力解码器)</h4><p>如果只有上下文向量联通解码和编码器，那么这单个向量承担了编码的整改序列的重担。</p>
<p>注意力模型允许解码器网络在每次输出的时候“集中”在编码输出的不同部分。首先我们计算<em>注意力权值</em>集合。它们将和编码器输出向量相乘去创建一个权值组合。结果(在代码中称作<code>attn_applied</code>)应该包括输入序列具体部分的信息，这样帮助解码器选择正确单词输出。</p>
<p><img src="/image/seq2seq/1152PYf.png"></p>
<p>通过一个前馈层<code>attn</code>计算注意力权值，使用到解码器输入和隐藏层状态作为输入。因为这里的训练集中的句子长度各异，为了实际创建和训练这个层我们不得不选择一个最大的句子长度(输入长度，编码输出)来应用。最大长度的句子将使用所有的权值，短些的句子只使用前面一些。</p>
<p><img src="/image/seq2seq/attention-decoder-network.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttnDecoderRNN</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_size, output_size, n_layers=<span class="number">1</span>, dropout_p=<span class="number">0.1</span>, max_length=MAX_LENGTH)</span>:</span></div><div class="line">        super(AttnDecoderRNN, self).__init__()</div><div class="line">        self.hidden_size = hidden_size</div><div class="line">        self.output_size = output_size</div><div class="line">        self.n_layers = n_layers</div><div class="line">        self.dropout_p = dropout_p</div><div class="line">        self.max_length = max_length</div><div class="line"></div><div class="line">        self.embedding = nn.Embedding(self.output_size, self.hidden_size)</div><div class="line">        self.attn = nn.Linear(self.hidden_size * <span class="number">2</span>, self.max_length)</div><div class="line">        self.attn_combine = nn.Linear(self.hidden_size * <span class="number">2</span>, self.hidden_size)</div><div class="line">        self.dropout = nn.Dropout(self.dropout_p)</div><div class="line">        self.gru = nn.GRU(self.hidden_size, self.hidden_size)</div><div class="line">        self.out = nn.Linear(self.hidden_size, self.output_size)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden, encoder_output, encoder_outputs)</span>:</span></div><div class="line">        embedded = self.embedding(input).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>)</div><div class="line">        embedded = self.dropout(embedded)</div><div class="line"></div><div class="line">        attn_weights = F.softmax(</div><div class="line">            self.attn(torch.cat((embedded[<span class="number">0</span>], hidden[<span class="number">0</span>]), <span class="number">1</span>)))</div><div class="line">        attn_applied = torch.bmm(attn_weights.unsqueeze(<span class="number">0</span>),</div><div class="line">                                 encoder_outputs.unsqueeze(<span class="number">0</span>))</div><div class="line"></div><div class="line">        output = torch.cat((embedded[<span class="number">0</span>], attn_applied[<span class="number">0</span>]), <span class="number">1</span>)</div><div class="line">        output = self.attn_combine(output).unsqueeze(<span class="number">0</span>)</div><div class="line"></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_layers):</div><div class="line">            output = F.relu(output)</div><div class="line">            output, hidden = self.gru(output, hidden)</div><div class="line"></div><div class="line">        output = F.log_softmax(self.out(output[<span class="number">0</span>]))</div><div class="line">        <span class="keyword">return</span> output, hidden, attn_weights</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></div><div class="line">        result = Variable(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size))</div><div class="line">        <span class="keyword">if</span> use_cuda:</div><div class="line">            <span class="keyword">return</span> result.cuda()</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> result</div></pre></td></tr></table></figure>
<p><strong>Note</strong></p>
<blockquote>
<p>这里还有其它形式的注意力机制可以通过相对位置实现以此避免长度的限制。遇到关于“local attention”在<a href="https://arxiv.org/abs/1508.04025" target="_blank" rel="external">Effective Approaches to Attention-based Neural Machine Translation</a>。</p>
</blockquote>
<h2 id="Training-训练"><a href="#Training-训练" class="headerlink" title="Training(训练)"></a>Training(训练)</h2><h3 id="Preparing-Training-Data-训练数据准备"><a href="#Preparing-Training-Data-训练数据准备" class="headerlink" title="Preparing Training Data(训练数据准备)"></a>Preparing Training Data(训练数据准备)</h3><p>为了训练，每个语言对我们需要输入tensor(输入序列词的索引)和目标tensor(目标序列词的索引)。当创建这些向量我们会添加EOS字符在每个序列最后。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">indexesFromSentence</span><span class="params">(lang, sentence)</span>:</span></div><div class="line">    <span class="keyword">return</span> [lang.word2index[word] <span class="keyword">for</span> word <span class="keyword">in</span> sentence.split(<span class="string">' '</span>)]</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">variableFromSentence</span><span class="params">(lang, sentence)</span>:</span></div><div class="line">    indexes = indexesFromSentence(lang, sentence)</div><div class="line">    indexes.append(EOS_token)</div><div class="line">    result = Variable(torch.LongTensor(indexes).view(<span class="number">-1</span>, <span class="number">1</span>))</div><div class="line">    <span class="keyword">if</span> use_cuda:</div><div class="line">        <span class="keyword">return</span> result.cuda()</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> result</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">variablesFromPair</span><span class="params">(pair)</span>:</span></div><div class="line">    input_variable = variableFromSentence(input_lang, pair[<span class="number">0</span>])</div><div class="line">    target_variable = variableFromSentence(output_lang, pair[<span class="number">1</span>])</div><div class="line">    <span class="keyword">return</span> (input_variable, target_variable)</div></pre></td></tr></table></figure>
<h3 id="Training-the-Model-训练模型"><a href="#Training-the-Model-训练模型" class="headerlink" title="Training the Model(训练模型)"></a>Training the Model(训练模型)</h3><p>为了训练我们往编码器中输入数据，然后跟踪每个输入和最新的隐藏状态。然后解码器提供<code>&lt;SOS&gt;</code>标志作为第一个输入，然后解码器的上一个隐藏状态作为这次的隐藏状态。</p>
<p>“Teacher forcing”是一个使用真实目标输出作为每次下一个输入的概念，而不是使用解码器预测的输出作为下一个输入。使用teacher forcing因为它可以更快收敛，但是<a href="http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf" target="_blank" rel="external">当训练网络利用它时，它的表现并不稳定</a>。</p>
<p>你可以观察teacher-forced神经网络的输出序列它们读起来更有连续清晰的语法而不是和正确翻译相差很远混乱的翻译-按照直觉来，它学会了输出语法然后在老师告诉它一些单词后能“挑选”有含义的词，但是并不能正确学习从头创造一个句子。</p>
<p>因为PyTorch的自动梯度计算提供给了我们自由的选择，我们可以随机的选择使用teacher foring或者不使用它。打开<code>teacher_forcing_ratio</code>来更多的使用它。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div></pre></td><td class="code"><pre><div class="line">teacher_forcing_ratio = <span class="number">0.5</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH)</span>:</span></div><div class="line">    encoder_hidden = encoder.initHidden()</div><div class="line"></div><div class="line">    encoder_optimizer.zero_grad()</div><div class="line">    decoder_optimizer.zero_grad()</div><div class="line"></div><div class="line">    input_length = input_variable.size()[<span class="number">0</span>]</div><div class="line">    target_length = target_variable.size()[<span class="number">0</span>]</div><div class="line"></div><div class="line">    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))</div><div class="line">    encoder_outputs = encoder_outputs.cuda() <span class="keyword">if</span> use_cuda <span class="keyword">else</span> encoder_outputs</div><div class="line"></div><div class="line">    loss = <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> ei <span class="keyword">in</span> range(input_length):</div><div class="line">        encoder_output, encoder_hidden = encoder(</div><div class="line">            input_variable[ei], encoder_hidden)</div><div class="line">        encoder_outputs[ei] = encoder_output[<span class="number">0</span>][<span class="number">0</span>]</div><div class="line"></div><div class="line">    decoder_input = Variable(torch.LongTensor([[SOS_token]]))</div><div class="line">    decoder_input = decoder_input.cuda() <span class="keyword">if</span> use_cuda <span class="keyword">else</span> decoder_input</div><div class="line"></div><div class="line">    decoder_hidden = encoder_hidden</div><div class="line"></div><div class="line">    use_teacher_forcing = <span class="keyword">True</span> <span class="keyword">if</span> random.random() &lt; teacher_forcing_ratio <span class="keyword">else</span> <span class="keyword">False</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> use_teacher_forcing:</div><div class="line">        <span class="comment"># Teacher forcing: Feed the target as the next input</span></div><div class="line">        <span class="keyword">for</span> di <span class="keyword">in</span> range(target_length):</div><div class="line">            decoder_output, decoder_hidden, decoder_attention = decoder(</div><div class="line">                decoder_input, decoder_hidden, encoder_output, encoder_outputs)</div><div class="line">            loss += criterion(decoder_output[<span class="number">0</span>], target_variable[di])</div><div class="line">            decoder_input = target_variable[di]  <span class="comment"># Teacher forcing</span></div><div class="line"></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># Without teacher forcing: use its own predictions as the next input</span></div><div class="line">        <span class="keyword">for</span> di <span class="keyword">in</span> range(target_length):</div><div class="line">            decoder_output, decoder_hidden, decoder_attention = decoder(</div><div class="line">                decoder_input, decoder_hidden, encoder_output, encoder_outputs)</div><div class="line">            topv, topi = decoder_output.data.topk(<span class="number">1</span>)</div><div class="line">            ni = topi[<span class="number">0</span>][<span class="number">0</span>]</div><div class="line"></div><div class="line">            decoder_input = Variable(torch.LongTensor([[ni]]))</div><div class="line">            decoder_input = decoder_input.cuda() <span class="keyword">if</span> use_cuda <span class="keyword">else</span> decoder_input</div><div class="line"></div><div class="line">            loss += criterion(decoder_output[<span class="number">0</span>], target_variable[di])</div><div class="line">            <span class="keyword">if</span> ni == EOS_token:</div><div class="line">                <span class="keyword">break</span></div><div class="line"></div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    encoder_optimizer.step()</div><div class="line">    decoder_optimizer.step()</div><div class="line"></div><div class="line">    <span class="keyword">return</span> loss.data[<span class="number">0</span>] / target_length</div></pre></td></tr></table></figure>
<p>这是一个用来帮助打印时间的函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">import</span> math</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">asMinutes</span><span class="params">(s)</span>:</span></div><div class="line">    m = math.floor(s / <span class="number">60</span>)</div><div class="line">    s -= m * <span class="number">60</span></div><div class="line">    <span class="keyword">return</span> <span class="string">'%dm %ds'</span> % (m, s)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeSince</span><span class="params">(since, percent)</span>:</span></div><div class="line">    now = time.time()</div><div class="line">    s = now - since</div><div class="line">    es = s / (percent)</div><div class="line">    rs = es - s</div><div class="line">    <span class="keyword">return</span> <span class="string">'%s (- %s)'</span> % (asMinutes(s), asMinutes(rs))</div></pre></td></tr></table></figure>
<p>整个训练过程如下：</p>
<ul>
<li>打开计时器</li>
<li>初始化优化器和代价函数</li>
<li>创建训练集</li>
<li>创建一个空代价数组用来绘图</li>
</ul>
<p>然后我们调用<code>train</code>多次然后偶尔打印进程(% 循环次数，到目前为止的时间，估计时间)和平均损失。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainEpochs</span><span class="params">(encoder, decoder, n_epochs, print_every=<span class="number">1000</span>, plot_every=<span class="number">100</span>, learning_rate=<span class="number">0.01</span>)</span>:</span></div><div class="line">    start = time.time()</div><div class="line">    plot_losses = []</div><div class="line">    print_loss_total = <span class="number">0</span>  <span class="comment"># Reset every print_every</span></div><div class="line">    plot_loss_total = <span class="number">0</span>  <span class="comment"># Reset every plot_every</span></div><div class="line"></div><div class="line">    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)</div><div class="line">    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)</div><div class="line">    training_pairs = [variablesFromPair(random.choice(pairs))</div><div class="line">                      <span class="keyword">for</span> i <span class="keyword">in</span> range(n_epochs)]</div><div class="line">    criterion = nn.NLLLoss()</div><div class="line"></div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, n_epochs + <span class="number">1</span>):</div><div class="line">        training_pair = training_pairs[epoch - <span class="number">1</span>]</div><div class="line">        input_variable = training_pair[<span class="number">0</span>]</div><div class="line">        target_variable = training_pair[<span class="number">1</span>]</div><div class="line"></div><div class="line">        loss = train(input_variable, target_variable, encoder,</div><div class="line">                     decoder, encoder_optimizer, decoder_optimizer, criterion)</div><div class="line">        print_loss_total += loss</div><div class="line">        plot_loss_total += loss</div><div class="line"></div><div class="line">        <span class="keyword">if</span> epoch % print_every == <span class="number">0</span>:</div><div class="line">            print_loss_avg = print_loss_total / print_every</div><div class="line">            print_loss_total = <span class="number">0</span></div><div class="line">            print(<span class="string">'%s (%d %d%%) %.4f'</span> % (timeSince(start, epoch / n_epochs),</div><div class="line">                                         epoch, epoch / n_epochs * <span class="number">100</span>, print_loss_avg))</div><div class="line"></div><div class="line">        <span class="keyword">if</span> epoch % plot_every == <span class="number">0</span>:</div><div class="line">            plot_loss_avg = plot_loss_total / plot_every</div><div class="line">            plot_losses.append(plot_loss_avg)</div><div class="line">            plot_loss_total = <span class="number">0</span></div><div class="line"></div><div class="line">    showPlot(plot_losses)</div></pre></td></tr></table></figure></p>
<h3 id="Plotting-results-绘制结果"><a href="#Plotting-results-绘制结果" class="headerlink" title="Plotting results(绘制结果)"></a>Plotting results(绘制结果)</h3><p>使用matplotlib来绘制，在训练的时候使用<code>plot_losses</code>损失数组保存损失。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> matplotlib.ticker <span class="keyword">as</span> ticker</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">showPlot</span><span class="params">(points)</span>:</span></div><div class="line">    plt.figure()</div><div class="line">    fig, ax = plt.subplots()</div><div class="line">    <span class="comment"># this locator puts ticks at regular intervals</span></div><div class="line">    loc = ticker.MultipleLocator(base=<span class="number">0.2</span>)</div><div class="line">    ax.yaxis.set_major_locator(loc)</div><div class="line">    plt.plot(points)</div></pre></td></tr></table></figure></p>
<h2 id="Evaluation-评价"><a href="#Evaluation-评价" class="headerlink" title="Evaluation(评价)"></a>Evaluation(评价)</h2><p>评价和训练大部分是一样的，但评价没有目标因此仅仅提供解码器每步预测的词汇给它自己作为下次输入。每次预测的单词我们将它添加到输出字符串，然后当预测到EOS字符的时候停止。我们也存储解码器的注意力输出作为后续的展示使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(encoder, decoder, sentence, max_length=MAX_LENGTH)</span>:</span></div><div class="line">    input_variable = variableFromSentence(input_lang, sentence)</div><div class="line">    input_length = input_variable.size()[<span class="number">0</span>]</div><div class="line">    encoder_hidden = encoder.initHidden()</div><div class="line"></div><div class="line">    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))</div><div class="line">    encoder_outputs = encoder_outputs.cuda() <span class="keyword">if</span> use_cuda <span class="keyword">else</span> encoder_outputs</div><div class="line"></div><div class="line">    <span class="keyword">for</span> ei <span class="keyword">in</span> range(input_length):</div><div class="line">        encoder_output, encoder_hidden = encoder(input_variable[ei],</div><div class="line">                                                 encoder_hidden)</div><div class="line">        encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[<span class="number">0</span>][<span class="number">0</span>]</div><div class="line"></div><div class="line">    decoder_input = Variable(torch.LongTensor([[SOS_token]]))  <span class="comment"># SOS</span></div><div class="line">    decoder_input = decoder_input.cuda() <span class="keyword">if</span> use_cuda <span class="keyword">else</span> decoder_input</div><div class="line"></div><div class="line">    decoder_hidden = encoder_hidden</div><div class="line"></div><div class="line">    decoded_words = []</div><div class="line">    decoder_attentions = torch.zeros(max_length, max_length)</div><div class="line"></div><div class="line">    <span class="keyword">for</span> di <span class="keyword">in</span> range(max_length):</div><div class="line">        decoder_output, decoder_hidden, decoder_attention = decoder(</div><div class="line">            decoder_input, decoder_hidden, encoder_output, encoder_outputs)</div><div class="line">        decoder_attentions[di] = decoder_attention.data</div><div class="line">        topv, topi = decoder_output.data.topk(<span class="number">1</span>)</div><div class="line">        ni = topi[<span class="number">0</span>][<span class="number">0</span>]</div><div class="line">        <span class="keyword">if</span> ni == EOS_token:</div><div class="line">            decoded_words.append(<span class="string">'&lt;EOS&gt;'</span>)</div><div class="line">            <span class="keyword">break</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            decoded_words.append(output_lang.index2word[ni])</div><div class="line"></div><div class="line">        decoder_input = Variable(torch.LongTensor([[ni]]))</div><div class="line">        decoder_input = decoder_input.cuda() <span class="keyword">if</span> use_cuda <span class="keyword">else</span> decoder_input</div><div class="line"></div><div class="line">    <span class="keyword">return</span> decoded_words, decoder_attentions[:di + <span class="number">1</span>]</div></pre></td></tr></table></figure>
<p>我们能从训练集中随机选择句子来进行评估然后打印输入，目标，和输出以便进行主观的模型效果判断。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluateRandomly</span><span class="params">(encoder, decoder, n=<span class="number">10</span>)</span>:</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</div><div class="line">        pair = random.choice(pairs)</div><div class="line">        print(<span class="string">'&gt;'</span>, pair[<span class="number">0</span>])</div><div class="line">        print(<span class="string">'='</span>, pair[<span class="number">1</span>])</div><div class="line">        output_words, attentions = evaluate(encoder, decoder, pair[<span class="number">0</span>])</div><div class="line">        output_sentence = <span class="string">' '</span>.join(output_words)</div><div class="line">        print(<span class="string">'&lt;'</span>, output_sentence)</div><div class="line">        print(<span class="string">''</span>)</div></pre></td></tr></table></figure>
<h2 id="Training-and-Evaluating-训练和评估"><a href="#Training-and-Evaluating-训练和评估" class="headerlink" title="Training and Evaluating(训练和评估)"></a>Training and Evaluating(训练和评估)</h2><p>通过这些有用的函数(这看起来像是额外的工作，但是它更简单的运行–这里翻译有点问题)我们可以实际初始化神经网络并开始训练。</p>
<p>留意输入的句子是进行深度的过滤。对于这些小的数据集我们可以对应使用拥有256个隐藏节点和一个单独的GRU层的小神经网络。在MacBook上使用CPU差不多40分钟后可以获得一个可信赖的结果<br><strong>Note</strong></p>
<blockquote>
<p>如果你运行这个notebook你可以进行训练，断点内核，评估，和继续训练。注释编码器和解码器的初始化部分然后再次运行<code>trainEpochs</code>(感觉不对呢)。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">hidden_size = <span class="number">256</span></div><div class="line">encoder1 = EncoderRNN(input_lang.n_words, hidden_size)</div><div class="line">attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words,</div><div class="line">                               <span class="number">1</span>, dropout_p=<span class="number">0.1</span>)</div><div class="line"></div><div class="line"><span class="keyword">if</span> use_cuda:</div><div class="line">    encoder1 = encoder1.cuda()</div><div class="line">    attn_decoder1 = attn_decoder1.cuda()</div><div class="line"></div><div class="line">trainEpochs(encoder1, attn_decoder1, <span class="number">75000</span>, print_every=<span class="number">5000</span>)</div></pre></td></tr></table></figure>
<p><img src="/image/seq2seq/lossPic.png"></p>
<p><strong>Out</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">1m 49s (- 25m 35s) (5000 6%) 2.9102</div><div class="line">3m 33s (- 23m 9s) (10000 13%) 2.3276</div><div class="line">5m 18s (- 21m 14s) (15000 20%) 2.0305</div><div class="line">7m 2s (- 19m 23s) (20000 26%) 1.7443</div><div class="line">8m 46s (- 17m 33s) (25000 33%) 1.5357</div><div class="line">10m 29s (- 15m 44s) (30000 40%) 1.3806</div><div class="line">12m 13s (- 13m 58s) (35000 46%) 1.2429</div><div class="line">13m 57s (- 12m 13s) (40000 53%) 1.1654</div><div class="line">15m 41s (- 10m 27s) (45000 60%) 1.0286</div><div class="line">17m 25s (- 8m 42s) (50000 66%) 0.9044</div><div class="line">19m 9s (- 6m 57s) (55000 73%) 0.8432</div><div class="line">20m 53s (- 5m 13s) (60000 80%) 0.7452</div><div class="line">22m 36s (- 3m 28s) (65000 86%) 0.7195</div><div class="line">24m 20s (- 1m 44s) (70000 93%) 0.6386</div><div class="line">26m 3s (- 0m 0s) (75000 100%) 0.5978</div></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">evaluateRandomly(encoder1, attn_decoder1)</div></pre></td></tr></table></figure>
<p><strong>Out</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&gt; vous etes celle la .</div><div class="line">= you are the one .</div><div class="line">&lt; you are the one . &lt;EOS&gt;</div><div class="line"></div><div class="line">&gt; nous sommes encore maries .</div><div class="line">= we re still married .</div><div class="line">&lt; we re still married . &lt;EOS&gt;</div><div class="line"></div><div class="line">&gt; tu vas mourir .</div><div class="line">= you re going to die .</div><div class="line">&lt; you re going to die . &lt;EOS&gt;</div><div class="line"></div><div class="line">&gt; il est non fumeur .</div><div class="line">= he s a nonsmoker .</div><div class="line">&lt; he s a nonsmoker . &lt;EOS&gt;</div></pre></td></tr></table></figure></p>
<h3 id="Visualizing-Attention-可视化注意力"><a href="#Visualizing-Attention-可视化注意力" class="headerlink" title="Visualizing Attention(可视化注意力)"></a>Visualizing Attention(可视化注意力)</h3><p>注意力机制一个有用的特性是它的高度可解释性输出。因为它使用权值指出特定输入序列的编码输出，我们可以图像化寻找神经网络在每个时间步集中注意力在哪里。</p>
<p>你可以简单的运行<code>plt.matshow(attentions)</code>去观察按照矩阵形式输出的注意力，列表示输入步骤行表示输出步骤：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">output_words, attentions = evaluate(</div><div class="line">    encoder1, attn_decoder1, <span class="string">"je suis trop froid ."</span>)</div><div class="line">plt.matshow(attentions.numpy())</div></pre></td></tr></table></figure></p>
<p><img src="/image/seq2seq/attentionpic.png"></p>
<p>为了更好的观察我们将添加额外的坐标和标签：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">showAttention</span><span class="params">(input_sentence, output_words, attentions)</span>:</span></div><div class="line">    <span class="comment"># Set up figure with colorbar</span></div><div class="line">    fig = plt.figure()</div><div class="line">    ax = fig.add_subplot(<span class="number">111</span>)</div><div class="line">    cax = ax.matshow(attentions.numpy(), cmap=<span class="string">'bone'</span>)</div><div class="line">    fig.colorbar(cax)</div><div class="line"></div><div class="line">    <span class="comment"># Set up axes</span></div><div class="line">    ax.set_xticklabels([<span class="string">''</span>] + input_sentence.split(<span class="string">' '</span>) +</div><div class="line">                       [<span class="string">'&lt;EOS&gt;'</span>], rotation=<span class="number">90</span>)</div><div class="line">    ax.set_yticklabels([<span class="string">''</span>] + output_words)</div><div class="line"></div><div class="line">    <span class="comment"># Show label at every tick</span></div><div class="line">    ax.xaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</div><div class="line">    ax.yaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</div><div class="line"></div><div class="line">    plt.show()</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluateAndShowAttention</span><span class="params">(input_sentence)</span>:</span></div><div class="line">    output_words, attentions = evaluate(</div><div class="line">        encoder1, attn_decoder1, input_sentence)</div><div class="line">    print(<span class="string">'input ='</span>, input_sentence)</div><div class="line">    print(<span class="string">'output ='</span>, <span class="string">' '</span>.join(output_words))</div><div class="line">    showAttention(input_sentence, output_words, attentions)</div><div class="line"></div><div class="line"></div><div class="line">evaluateAndShowAttention(<span class="string">"elle a cinq ans de moins que moi ."</span>)</div><div class="line"></div><div class="line">evaluateAndShowAttention(<span class="string">"elle est trop petit ."</span>)</div><div class="line"></div><div class="line">evaluateAndShowAttention(<span class="string">"je ne crains pas de mourir ."</span>)</div><div class="line"></div><div class="line">evaluateAndShowAttention(<span class="string">"c est un jeune directeur plein de talent ."</span>)</div></pre></td></tr></table></figure>
<p><img src="/image/seq2seq/attention4pic.png"></p>
<p><strong>Out</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">input = elle a cinq ans de moins que moi .</div><div class="line">output = she s five years younger than me . &lt;EOS&gt;</div><div class="line">input = elle est trop petit .</div><div class="line">output = she is too late . &lt;EOS&gt;</div><div class="line">input = je ne crains pas de mourir .</div><div class="line">output = i m not scared to die . &lt;EOS&gt;</div><div class="line">input = c est un jeune directeur plein de talent .</div><div class="line">output = he s a talented young guy . &lt;EOS&gt;</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
            <category> translate </category>
            
        </categories>
        
        
        <tags>
            
            <tag> rnn </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Generating Names with a Character-Level RNN(翻译)]]></title>
      <url>/2017/05/23/GeneratName/</url>
      <content type="html"><![CDATA[<p>原文链接：<a href="http://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html" target="_blank" rel="external">Generating Names with a Character-Level RNN</a><br>作者：<a href="https://github.com/spro/practical-pytorch" target="_blank" rel="external">Sean Robertson</a><br>在上一篇<a href="http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html" target="_blank" rel="external">指导</a>我们使用RNN来进行名字所属语言的分类。这次我们换一个方向来根据语言生成名字。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">&gt; python sample.py Russian RUS</div><div class="line">Rovakov</div><div class="line">Uantov</div><div class="line">Shavakov</div><div class="line"></div><div class="line">&gt; python sample.py German GER</div><div class="line">Gerren</div><div class="line">Ereng</div><div class="line">Rosher</div><div class="line"></div><div class="line">&gt; python sample.py Spanish SPA</div><div class="line">Salla</div><div class="line">Parer</div><div class="line">Allan</div><div class="line"></div><div class="line">&gt; python sample.py Chinese CHI</div><div class="line">Chan</div><div class="line">Hang</div><div class="line">Iun</div></pre></td></tr></table></figure></p>
<p>我们任然手动完成一个包含几个线性层的小的RNN。最大的不同是我们并不在读入整个名字后预测其所属语言，而是我们输入分类然后每个时间步输出一个字母。循环预测规定语言的下一个字符(也可以是单词或者其它高级结构)经常也称为“语言模型”。</p>
<h2 id="Recommended-Reading-推荐阅读"><a href="#Recommended-Reading-推荐阅读" class="headerlink" title="Recommended Reading(推荐阅读)"></a>Recommended Reading(推荐阅读)</h2><p>我假设你目前至少安装来PyTorch，了解Python，并且理解Tensor：  </p>
<ul>
<li><a href="http://pytorch.org" target="_blank" rel="external">http://pytorch.org</a>从这里获取安装指导</li>
<li><a href="http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="external">Deep Learning with PyTorch: A 60 Minute Blitz</a>一般从这里开始PyToch</li>
<li><a href="http://pytorch.org/tutorials/beginner/pytorch_with_examples.html" target="_blank" rel="external">Learning PyTorch with Examples</a>获取更有广度和深度的概述</li>
<li><a href="http://pytorch.org/tutorials/beginner/former_torchies_tutorial.html" target="_blank" rel="external">PyTorch for former Torch users</a>如果你是Lua Torch用户</li>
</ul>
<p>下面对你了解RNNs和它们如何工作很有用：</p>
<ul>
<li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">The Unreasonable Effectiveness of Recurrent Neural Networks</a>展示了一些实际生活中的例子</li>
<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM Networks</a>是重点关于LSTMs的但也提供了RNNs的一般信息</li>
</ul>
<p>我也建议你阅读前一个指导，<a href="http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html" target="_blank" rel="external">Classifying Names with a Character-Level RNN</a></p>
<h2 id="Preparing-the-Data-数据准备"><a href="#Preparing-the-Data-数据准备" class="headerlink" title="Preparing the Data(数据准备)"></a>Preparing the Data(数据准备)</h2><p><strong>Note</strong></p>
<blockquote>
<p>从<a href="https://download.pytorch.org/tutorial/data.zip" target="_blank" rel="external">这里</a>下载数据然后解压到当前文件夹</p>
</blockquote>
<p>了解更多这个过程的细节可以看上一篇指导。简而言之，这里有一系列以<code>data/names/[Language].txt</code>命名的纯文本文件每行都是一个名字。我们将每行拆分进数组，从Unicode转换为ASCII，最后生成一个字典<code>{language: [names ...]}</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</div><div class="line"><span class="keyword">from</span> io <span class="keyword">import</span> open</div><div class="line"><span class="keyword">import</span> glob</div><div class="line"><span class="keyword">import</span> unicodedata</div><div class="line"><span class="keyword">import</span> string</div><div class="line"></div><div class="line">all_letters = string.ascii_letters + <span class="string">" .,;'-"</span></div><div class="line">n_letters = len(all_letters) + <span class="number">1</span> <span class="comment"># Plus EOS marker</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">findFiles</span><span class="params">(path)</span>:</span> <span class="keyword">return</span> glob.glob(path)</div><div class="line"></div><div class="line"><span class="comment"># Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">unicodeToAscii</span><span class="params">(s)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(</div><div class="line">        c <span class="keyword">for</span> c <span class="keyword">in</span> unicodedata.normalize(<span class="string">'NFD'</span>, s)</div><div class="line">        <span class="keyword">if</span> unicodedata.category(c) != <span class="string">'Mn'</span></div><div class="line">        <span class="keyword">and</span> c <span class="keyword">in</span> all_letters</div><div class="line">    )</div><div class="line"></div><div class="line"><span class="comment"># Read a file and split into lines</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">readLines</span><span class="params">(filename)</span>:</span></div><div class="line">    lines = open(filename, encoding=<span class="string">'utf-8'</span>).read().strip().split(<span class="string">'\n'</span>)</div><div class="line">    <span class="keyword">return</span> [unicodeToAscii(line) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</div><div class="line"></div><div class="line"><span class="comment"># Build the category_lines dictionary, a list of lines per category</span></div><div class="line">category_lines = &#123;&#125;</div><div class="line">all_categories = []</div><div class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> findFiles(<span class="string">'data/names/*.txt'</span>):</div><div class="line">    category = filename.split(<span class="string">'/'</span>)[<span class="number">-1</span>].split(<span class="string">'.'</span>)[<span class="number">0</span>]</div><div class="line">    all_categories.append(category)</div><div class="line">    lines = readLines(filename)</div><div class="line">    category_lines[category] = lines</div><div class="line"></div><div class="line">n_categories = len(all_categories)</div><div class="line"></div><div class="line">print(<span class="string">'# categories:'</span>, n_categories, all_categories)</div><div class="line">print(unicodeToAscii(<span class="string">"O'Néàl"</span>))</div></pre></td></tr></table></figure>
<p>Out:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># categories: 18 [&apos;Arabic&apos;, &apos;Chinese&apos;, &apos;Czech&apos;, &apos;Dutch&apos;, &apos;English&apos;, &apos;French&apos;, &apos;German&apos;, &apos;Greek&apos;, &apos;Irish&apos;, &apos;Italian&apos;, &apos;Japanese&apos;, &apos;Korean&apos;, &apos;Polish&apos;, &apos;Portuguese&apos;, &apos;Russian&apos;, &apos;Scottish&apos;, &apos;Spanish&apos;, &apos;Vietnamese&apos;]</div><div class="line">O&apos;Neal</div></pre></td></tr></table></figure>
<h2 id="Creating-the-Network-创建神经网络"><a href="#Creating-the-Network-创建神经网络" class="headerlink" title="Creating the Network(创建神经网络)"></a>Creating the Network(创建神经网络)</h2><p>这个神经网络是<a href="http://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html#Creating-the-Network" target="_blank" rel="external">上一篇指导</a>的神经网络的扩展多了分类Tensor作为参数，它们和其它参数存在联系。分类Tensor和字母输入一样是one-hot向量。</p>
<p>我们可以理解输出是下一个字母是什么的概率。当进行采样时，最可能输出字母被作为下一个输入字母使用。</p>
<p>我添加了第二个线性层<code>o2o</code>(在组合了隐藏状态和输出)让这个神经网络更好的工作。这里还有个dropout层，用来根据概率(这里是0.1)随机生成0通常用来稀疏输入数据避免过拟合。这里我们使用它在神经网络来增加一些噪音然后增加采样。</p>
<p><img src="/image/GenNameRnn/genName.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, output_size)</span>:</span></div><div class="line">        super(RNN, self).__init__()</div><div class="line">        self.hidden_size = hidden_size</div><div class="line"></div><div class="line">        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)</div><div class="line">        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)</div><div class="line">        self.o2o = nn.Linear(hidden_size + output_size, output_size)</div><div class="line">        self.dropout = nn.Dropout(<span class="number">0.1</span>)</div><div class="line">        self.softmax = nn.LogSoftmax()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, category, input, hidden)</span>:</span></div><div class="line">        input_combined = torch.cat((category, input, hidden), <span class="number">1</span>)</div><div class="line">        hidden = self.i2h(input_combined)</div><div class="line">        output = self.i2o(input_combined)</div><div class="line">        output_combined = torch.cat((hidden, output), <span class="number">1</span>)</div><div class="line">        output = self.o2o(output_combined)</div><div class="line">        output = self.dropout(output)</div><div class="line">        output = self.softmax(output)</div><div class="line">        <span class="keyword">return</span> output, hidden</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> Variable(torch.zeros(<span class="number">1</span>, self.hidden_size))</div></pre></td></tr></table></figure>
<h2 id="Training-训练"><a href="#Training-训练" class="headerlink" title="Training(训练)"></a>Training(训练)</h2><h3 id="Preparing-for-Training-训练准备"><a href="#Preparing-for-Training-训练准备" class="headerlink" title="Preparing for Training(训练准备)"></a>Preparing for Training(训练准备)</h3><p>首先，一个帮助函数用来获得随机的组合(category,line):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> random</div><div class="line"></div><div class="line"><span class="comment"># Random item from a list</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomChoice</span><span class="params">(l)</span>:</span></div><div class="line">    <span class="keyword">return</span> l[random.randint(<span class="number">0</span>, len(l) - <span class="number">1</span>)]</div><div class="line"></div><div class="line"><span class="comment"># Get a random category and random line from that category</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomTrainingPair</span><span class="params">()</span>:</span></div><div class="line">    category = randomChoice(all_categories)</div><div class="line">    line = randomChoice(category_lines[category])</div><div class="line">    <span class="keyword">return</span> category, line</div></pre></td></tr></table></figure>
<p>对于每个时间步(指每个训练单词中的字母)输入神经网络的是<code>(category, current letter, hidden state)</code>输出是<code>(next letter, next hidden state)</code>。因此对于每个训练集，我们需要分类，输入字母集，和输出集/目标集。</p>
<p>因此在每个时间步骤我们从当前字母去预测下一个字母，字母组合是一组从每行名字得到的连续字母-e.g.对于<code>ABCD&lt;EOS&gt;</code>我们会创建(“A”, “B”),(“B”,”C”),(“C”,”D”),(“D”,”EOS”)。</p>
<p><img src="/image/GenNameRnn/letter.png"></p>
<p>分类tensor是<a href="https://en.wikipedia.org/wiki/One-hot" target="_blank" rel="external">one-hot tensor</a>大小是<code>&lt;1 x n_categories&gt;</code>。当训练的时候在每个时间步骤我们将它输入到神经网络-这个是一个设计选择，它可能作为初始化的隐藏状态活着一些其它策略的一部分。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># One-hot vector for category</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">categoryTensor</span><span class="params">(category)</span>:</span></div><div class="line">    li = all_categories.index(category)</div><div class="line">    tensor = torch.zeros(<span class="number">1</span>, n_categories)</div><div class="line">    tensor[<span class="number">0</span>][li] = <span class="number">1</span></div><div class="line">    <span class="keyword">return</span> tensor</div><div class="line"></div><div class="line"><span class="comment"># One-hot matrix of first to last letters (not including EOS) for input</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">inputTensor</span><span class="params">(line)</span>:</span></div><div class="line">    tensor = torch.zeros(len(line), <span class="number">1</span>, n_letters)</div><div class="line">    <span class="keyword">for</span> li <span class="keyword">in</span> range(len(line)):</div><div class="line">        letter = line[li]</div><div class="line">        tensor[li][<span class="number">0</span>][all_letters.find(letter)] = <span class="number">1</span></div><div class="line">    <span class="keyword">return</span> tensor</div><div class="line"></div><div class="line"><span class="comment"># LongTensor of second letter to end (EOS) for target</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">targetTensor</span><span class="params">(line)</span>:</span></div><div class="line">    letter_indexes = [all_letters.find(line[li]) <span class="keyword">for</span> li <span class="keyword">in</span> range(<span class="number">1</span>, len(line))]</div><div class="line">    letter_indexes.append(n_letters - <span class="number">1</span>) <span class="comment"># EOS</span></div><div class="line">    <span class="keyword">return</span> torch.LongTensor(letter_indexes)</div></pre></td></tr></table></figure>
<p>为了方便在训练中我们将使用<code>randomTrainingSet</code>函数获取(category,line)训练组合然后将其转换为要求(category,input,target)的tensor。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Make category, input, and target tensors from a random category, line pair</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomTrainingSet</span><span class="params">()</span>:</span></div><div class="line">    category, line = randomTrainingPair()</div><div class="line">    category_tensor = Variable(categoryTensor(category))</div><div class="line">    input_line_tensor = Variable(inputTensor(line))</div><div class="line">    target_line_tensor = Variable(targetTensor(line))</div><div class="line">    <span class="keyword">return</span> category_tensor, input_line_tensor, target_line_tensor</div></pre></td></tr></table></figure></p>
<h3 id="Training-the-Network-训练神经网络"><a href="#Training-the-Network-训练神经网络" class="headerlink" title="Training the Network(训练神经网络)"></a>Training the Network(训练神经网络)</h3><p>不同于分类我们只需使用最后一个输出，这里我们需要预测每一时间步，因此我们需要计算每个时间步的损失。</p>
<p>神奇的是自动梯度计算允许你简单的求和每个步骤的损失然后调用反向传播。但是不要问我为什么初始化loss为0。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">criterion = nn.NLLLoss()</div><div class="line"></div><div class="line">learning_rate = <span class="number">0.0005</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(category_tensor, input_line_tensor, target_line_tensor)</span>:</span></div><div class="line">    hidden = rnn.initHidden()</div><div class="line"></div><div class="line">    rnn.zero_grad()</div><div class="line"></div><div class="line">    loss = <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(input_line_tensor.size()[<span class="number">0</span>]):</div><div class="line">        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)</div><div class="line">        loss += criterion(output, target_line_tensor[i])</div><div class="line"></div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> rnn.parameters():</div><div class="line">        p.data.add_(-learning_rate, p.grad.data)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> output, loss.data[<span class="number">0</span>] / input_line_tensor.size()[<span class="number">0</span>]</div></pre></td></tr></table></figure>
<p>为了保持跟踪我训练了多长时间我添加了一个<code>timeSince(timestamp)</code>函数它会返回一个可读的字符串。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">import</span> math</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeSince</span><span class="params">(since)</span>:</span></div><div class="line">    now = time.time()</div><div class="line">    s = now - since</div><div class="line">    m = math.floor(s / <span class="number">60</span>)</div><div class="line">    s -= m * <span class="number">60</span></div><div class="line">    <span class="keyword">return</span> <span class="string">'%dm %ds'</span> % (m, s)</div></pre></td></tr></table></figure></p>
<p>训练和前面一样-调用训练后等待一段时间，打印当前时间和损失在每个<code>print_every</code>步，保存每<code>plot_every</code>步的平均损失在<code>all_losses</code>打印。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">rnn = RNN(n_letters, <span class="number">128</span>, n_letters)</div><div class="line"></div><div class="line">n_epochs = <span class="number">100000</span></div><div class="line">print_every = <span class="number">5000</span></div><div class="line">plot_every = <span class="number">500</span></div><div class="line">all_losses = []</div><div class="line">total_loss = <span class="number">0</span> <span class="comment"># Reset every plot_every epochs</span></div><div class="line"></div><div class="line">start = time.time()</div><div class="line"></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, n_epochs + <span class="number">1</span>):</div><div class="line">    output, loss = train(*randomTrainingSet())</div><div class="line">    total_loss += loss</div><div class="line"></div><div class="line">    <span class="keyword">if</span> epoch % print_every == <span class="number">0</span>:</div><div class="line">        print(<span class="string">'%s (%d %d%%) %.4f'</span> % (timeSince(start), epoch, epoch / n_epochs * <span class="number">100</span>, loss))</div><div class="line"></div><div class="line">    <span class="keyword">if</span> epoch % plot_every == <span class="number">0</span>:</div><div class="line">        all_losses.append(total_loss / plot_every)</div><div class="line">        total_loss = <span class="number">0</span></div></pre></td></tr></table></figure></p>
<p>Out:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">0m 12s (5000 5%) 2.3159</div><div class="line">0m 24s (10000 10%) 2.6862</div><div class="line">0m 36s (15000 15%) 1.8658</div><div class="line">0m 49s (20000 20%) 2.5288</div><div class="line">1m 1s (25000 25%) 2.2218</div><div class="line">1m 14s (30000 30%) 3.0179</div><div class="line">1m 26s (35000 35%) 2.2104</div><div class="line">1m 39s (40000 40%) 2.2386</div><div class="line">1m 51s (45000 45%) 2.0167</div><div class="line">2m 3s (50000 50%) 2.3285</div><div class="line">2m 16s (55000 55%) 1.8585</div><div class="line">2m 28s (60000 60%) 2.2254</div><div class="line">2m 40s (65000 65%) 2.6131</div><div class="line">2m 53s (70000 70%) 2.1845</div><div class="line">3m 5s (75000 75%) 1.9502</div><div class="line">3m 17s (80000 80%) 1.7935</div><div class="line">3m 30s (85000 85%) 1.9056</div><div class="line">3m 42s (90000 90%) 2.4642</div><div class="line">3m 54s (95000 95%) 1.8065</div><div class="line">4m 7s (100000 100%) 2.0977</div></pre></td></tr></table></figure></p>
<h3 id="Plotting-the-Losses-绘制损失"><a href="#Plotting-the-Losses-绘制损失" class="headerlink" title="Plotting the Losses(绘制损失)"></a>Plotting the Losses(绘制损失)</h3><p>从神经网络学习中的all_losses绘制历史损失变化。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> matplotlib.ticker <span class="keyword">as</span> ticker</div><div class="line"></div><div class="line">plt.figure()</div><div class="line">plt.plot(all_losses)</div></pre></td></tr></table></figure></p>
<p><img src="/image/GenNameRnn/loss.png"></p>
<h2 id="Sampling-the-Network-神经网络抽样"><a href="#Sampling-the-Network-神经网络抽样" class="headerlink" title="Sampling the Network(神经网络抽样)"></a>Sampling the Network(神经网络抽样)</h2><p>为了抽样我们提供给神经网络一个字母然后要求下一个，再将它作为下一个输入传递给神经网络，如此循环直到EOS标志出现。</p>
<ul>
<li>创建一个tensor以便输入分类，开始单词，和空的隐藏状态</li>
<li>创建一个开始字母的<code>output_name</code></li>
<li>直到最大的输出长度<ul>
<li>返回输出字母给神经网络</li>
<li>获取下一个最大可能的输出，和下一个隐藏层状态</li>
<li>如果字母是EOS就停止循环</li>
<li>如果是正常字母就添加到<code>output_name</code>然后继续</li>
</ul>
</li>
<li>返回最终的名字</li>
</ul>
<p><strong>Note</strong></p>
<blockquote>
<p>有些策略不是提供一个开始字母而是在训练时包括了一个”开始”标志，然后神经网络自行选择开始的字母</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">max_length = <span class="number">20</span></div><div class="line"></div><div class="line"><span class="comment"># Sample from a category and starting letter</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(category, start_letter=<span class="string">'A'</span>)</span>:</span></div><div class="line">    category_tensor = Variable(categoryTensor(category))</div><div class="line">    input = Variable(inputTensor(start_letter))</div><div class="line">    hidden = rnn.initHidden()</div><div class="line"></div><div class="line">    output_name = start_letter</div><div class="line"></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_length):</div><div class="line">        output, hidden = rnn(category_tensor, input[<span class="number">0</span>], hidden)</div><div class="line">        topv, topi = output.data.topk(<span class="number">1</span>)</div><div class="line">        topi = topi[<span class="number">0</span>][<span class="number">0</span>]</div><div class="line">        <span class="keyword">if</span> topi == n_letters - <span class="number">1</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            letter = all_letters[topi]</div><div class="line">            output_name += letter</div><div class="line">        input = Variable(inputTensor(letter))</div><div class="line"></div><div class="line">    <span class="keyword">return</span> output_name</div><div class="line"></div><div class="line"><span class="comment"># Get multiple samples from one category and multiple starting letters</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">samples</span><span class="params">(category, start_letters=<span class="string">'ABC'</span>)</span>:</span></div><div class="line">    <span class="keyword">for</span> start_letter <span class="keyword">in</span> start_letters:</div><div class="line">        print(sample(category, start_letter))</div><div class="line"></div><div class="line">samples(<span class="string">'Russian'</span>, <span class="string">'RUS'</span>)</div><div class="line"></div><div class="line">samples(<span class="string">'German'</span>, <span class="string">'GER'</span>)</div><div class="line"></div><div class="line">samples(<span class="string">'Spanish'</span>, <span class="string">'SPA'</span>)</div><div class="line"></div><div class="line">samples(<span class="string">'Chinese'</span>, <span class="string">'CHI'</span>)</div></pre></td></tr></table></figure>
<p>Out:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">Rovallov</div><div class="line">Uarinov</div><div class="line">Shavanov</div><div class="line">Garter</div><div class="line">Eren</div><div class="line">Romall</div><div class="line">Sallan</div><div class="line">Perran</div><div class="line">Allan</div><div class="line">Chang</div><div class="line">Ha</div><div class="line">Iua</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
            <category> translate </category>
            
        </categories>
        
        
        <tags>
            
            <tag> rnn </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Classifying Names with a Character-Level RNN(翻译)]]></title>
      <url>/2017/05/22/classifyName/</url>
      <content type="html"><![CDATA[<p>原文链接：<a href="http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html" target="_blank" rel="external">Classifying Names with a Character-Level RNN</a><br>作者：<a href="https://github.com/spro/practical-pytorch" target="_blank" rel="external">Sean Robertson</a></p>
<p>我们将构建和训练一个基础的字符级别的RNN网络去进行词汇分类。一个字符级别的RNN将词汇作为字符序列读入-在每步输出预测值和“隐藏状态”，将上一步的隐藏状态传递给下一步。我们将最后的预测作为输出，i.e.就是词汇所属的分类。<br><a id="more"></a><br>具体来说，我们会从18种语言中训练几千个姓，然后通过一个姓的拼写来预测它属于哪种语言：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">$ python predict.py Hinton</div><div class="line">(<span class="number">-0.47</span>) Scottish</div><div class="line">(<span class="number">-1.52</span>) English</div><div class="line">(<span class="number">-3.57</span>) Irish</div><div class="line"></div><div class="line">$ python predict.py Schmidhuber</div><div class="line">(<span class="number">-0.19</span>) German</div><div class="line">(<span class="number">-2.48</span>) Czech</div><div class="line">(<span class="number">-2.68</span>) Dutch</div></pre></td></tr></table></figure></p>
<h2 id="Recommended-Reading-推荐阅读"><a href="#Recommended-Reading-推荐阅读" class="headerlink" title="Recommended Reading(推荐阅读)"></a>Recommended Reading(推荐阅读)</h2><p>我假设你目前至少安装来PyTorch，了解Python，并且理解Tensor：  </p>
<ul>
<li><a href="http://pytorch.org" target="_blank" rel="external">http://pytorch.org</a>从这里获取安装指导</li>
<li><a href="http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="external">Deep Learning with PyTorch: A 60 Minute Blitz</a>一般从这里开始PyToch</li>
<li><a href="http://pytorch.org/tutorials/beginner/pytorch_with_examples.html" target="_blank" rel="external">Learning PyTorch with Examples</a>获取更有广度和深度的概述</li>
<li><a href="http://pytorch.org/tutorials/beginner/former_torchies_tutorial.html" target="_blank" rel="external">PyTorch for former Torch users</a>如果你是Lua Torch用户</li>
</ul>
<p>下面对你了解RNNs和它们如何工作很有用：</p>
<ul>
<li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">The Unreasonable Effectiveness of Recurrent Neural Networks</a>展示了一些实际生活中的例子</li>
<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM Networks</a>是重点关于LSTMs的但也提供了RNNs的一般信息</li>
</ul>
<h2 id="Preparing-the-Data-数据准备"><a href="#Preparing-the-Data-数据准备" class="headerlink" title="Preparing the Data(数据准备)"></a>Preparing the Data(数据准备)</h2><p><strong>Note</strong></p>
<blockquote>
<p>从<a href="https://download.pytorch.org/tutorial/data.zip" target="_blank" rel="external">这里</a>下载数据并解压到当前文件</p>
</blockquote>
<p>包含在<code>data/names</code>目录下的是18个以“[Language].txt”格式命名的文件。每个文件包含了一些名字，一个名字一行，大部分是罗马字母(但是我们任然需要将编码格式从Unicode转换为ASCII)。</p>
<p>我们最终得到的是一个以语言名称为键以名字列表为值的字典，<code>{language:[names ...]}</code>。通常的变量“category”和“line”(在我们的例子中对应语言和名字)是用来后续扩展。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</div><div class="line"><span class="keyword">from</span> io <span class="keyword">import</span> open</div><div class="line"><span class="keyword">import</span> glob</div><div class="line"><span class="comment">#glob是python自己带的一个文件操作相关模块，用它可以查找符合自己目的的文件，只会</span></div><div class="line"><span class="comment">#当前查询目录下匹配到的文件路径，不关注子文件目录</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">findFiles</span><span class="params">(path)</span>:</span> <span class="keyword">return</span> glob.glob(path)</div><div class="line"></div><div class="line">print(findFiles(<span class="string">'data/names/*.txt'</span>))</div><div class="line"></div><div class="line"><span class="keyword">import</span> unicodedata</div><div class="line"><span class="keyword">import</span> string</div><div class="line"></div><div class="line">all_letters = string.ascii_letters + <span class="string">" .,;'"</span></div><div class="line">n_letters = len(all_letters)</div><div class="line"></div><div class="line"><span class="comment"># Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">unicodeToAscii</span><span class="params">(s)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(</div><div class="line">        c <span class="keyword">for</span> c <span class="keyword">in</span> unicodedata.normalize(<span class="string">'NFD'</span>, s)</div><div class="line">        <span class="keyword">if</span> unicodedata.category(c) != <span class="string">'Mn'</span></div><div class="line">        <span class="keyword">and</span> c <span class="keyword">in</span> all_letters</div><div class="line">    )</div><div class="line"></div><div class="line">print(unicodeToAscii(<span class="string">'Ślusàrski'</span>))</div><div class="line"></div><div class="line"><span class="comment"># Build the category_lines dictionary, a list of names per language</span></div><div class="line">category_lines = &#123;&#125;</div><div class="line">all_categories = []</div><div class="line"></div><div class="line"><span class="comment"># Read a file and split into lines</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">readLines</span><span class="params">(filename)</span>:</span></div><div class="line">    lines = open(filename, encoding=<span class="string">'utf-8'</span>).read().strip().split(<span class="string">'\n'</span>)</div><div class="line">    <span class="keyword">return</span> [unicodeToAscii(line) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</div><div class="line"><span class="comment"># 也可以使用os.path.basename('data/names/test.txt')得到test.txt</span></div><div class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> findFiles(<span class="string">'data/names/*.txt'</span>):</div><div class="line">    category = filename.split(<span class="string">'/'</span>)[<span class="number">-1</span>].split(<span class="string">'.'</span>)[<span class="number">0</span>]</div><div class="line">    all_categories.append(category)</div><div class="line">    lines = readLines(filename)</div><div class="line">    category_lines[category] = lines</div><div class="line"></div><div class="line">n_categories = len(all_categories)</div></pre></td></tr></table></figure>
<p>Out:</p>
<blockquote>
<p>[‘data/names/Arabic.txt’, ‘data/names/Chinese.txt’, ‘data/names/Czech.txt’, ‘data/names/Dutch.txt’, ‘data/names/English.txt’, ‘data/names/French.txt’, ‘data/names/German.txt’, ‘data/names/Greek.txt’, ‘data/names/Irish.txt’, ‘data/names/Italian.txt’, ‘data/names/Japanese.txt’, ‘data/names/Korean.txt’, ‘data/names/Polish.txt’, ‘data/names/Portuguese.txt’, ‘data/names/Russian.txt’, ‘data/names/Scottish.txt’, ‘data/names/Spanish.txt’, ‘data/names/Vietnamese.txt’]<br>Slusarski</p>
</blockquote>
<p>现在我们有<code>category_lines</code>，一个从每个分类(语言)到行的列表(名字)映射的字典。我们也记下了<code>all_categories</code>(语言列表)和<code>n_categories</code>后续使用。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">print(category_lines[<span class="string">'Italian'</span>][:<span class="number">5</span>])</div></pre></td></tr></table></figure></p>
<p>Out:</p>
<blockquote>
<p>[‘Abandonato’, ‘Abatangelo’, ‘Abatantuono’, ‘Abate’, ‘Abategiovanni’]</p>
</blockquote>
<h3 id="Turning-Names-into-Tensors-转换名字为Tensor"><a href="#Turning-Names-into-Tensors-转换名字为Tensor" class="headerlink" title="Turning Names into Tensors(转换名字为Tensor)"></a>Turning Names into Tensors(转换名字为Tensor)</h3><p>现在我们已经有了所有组织好的名字，我们需要将它们转换为Tensor以便利用。</p>
<p>为了表示字母，我们使用<code>&lt;1 x n_letters&gt;</code>大小的“one-hot vector”。一个one-hot vector除当前字母的位置填为1外，使用0来填充其它位置，e.g.<code>&quot;b&quot;=&lt;0 1 0 0 0 ...&gt;</code>。</p>
<p>为了表示一个词语我们把一串这些字母整合进一个2维矩阵<code>&lt;line_length x 1 x n_letter&gt;</code>。<br>这个多出来的一维是因为PyTorch假设任何东西是在一个批次中，我们仅使用一个大小为1的批次在这里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"></div><div class="line"><span class="comment"># Find letter index from all_letters, e.g. "a" = 0</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">letterToIndex</span><span class="params">(letter)</span>:</span></div><div class="line">    <span class="keyword">return</span> all_letters.find(letter)</div><div class="line"></div><div class="line"><span class="comment"># Just for demonstration, turn a letter into a &lt;1 x n_letters&gt; Tensor</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">letterToTensor</span><span class="params">(letter)</span>:</span></div><div class="line">    tensor = torch.zeros(<span class="number">1</span>, n_letters)</div><div class="line">    tensor[<span class="number">0</span>][letterToIndex(letter)] = <span class="number">1</span></div><div class="line">    <span class="keyword">return</span> tensor</div><div class="line"></div><div class="line"><span class="comment"># Turn a line into a &lt;line_length x 1 x n_letters&gt;,</span></div><div class="line"><span class="comment"># or an array of one-hot letter vectors</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lineToTensor</span><span class="params">(line)</span>:</span></div><div class="line">    tensor = torch.zeros(len(line), <span class="number">1</span>, n_letters)</div><div class="line">    <span class="keyword">for</span> li, letter <span class="keyword">in</span> enumerate(line):</div><div class="line">        tensor[li][<span class="number">0</span>][letterToIndex(letter)] = <span class="number">1</span></div><div class="line">    <span class="keyword">return</span> tensor</div><div class="line"></div><div class="line">print(letterToTensor(<span class="string">'J'</span>))</div><div class="line"></div><div class="line">print(lineToTensor(<span class="string">'Jones'</span>).size())</div></pre></td></tr></table></figure>
<p>Out:</p>
<blockquote>
<p>Columns 0 to 12<br>    0     0     0     0     0     0     0     0     0     0     0     0     0</p>
<p>Columns 13 to 25<br>    0     0     0     0     0     0     0     0     0     0     0     0     0</p>
<p>Columns 26 to 38<br>    0     0     0     0     0     0     0     0     0     1     0     0     0</p>
<p>Columns 39 to 51<br>    0     0     0     0     0     0     0     0     0     0     0     0     0</p>
<p>Columns 52 to 56<br>    0     0     0     0     0<br>[torch.FloatTensor of size 1x57]</p>
<p>torch.Size([5, 1, 57])</p>
</blockquote>
<h2 id="Creating-the-Network-创建神经网络"><a href="#Creating-the-Network-创建神经网络" class="headerlink" title="Creating the Network(创建神经网络)"></a>Creating the Network(创建神经网络)</h2><p>在自动梯度计算前，使用Torch创建一个循环神经网络需要按照时间步骤克隆每层的参数。每个层拥有隐藏状态和梯度现在整个的都在计算图中。这意味你可以很“纯净”的方式实现一个RNN，就像普通的前向层网络。</p>
<p>这个RNN模型(大部分拷贝自<a href="https://github.com/pytorch/tutorials/blob/master/Introduction%20to%20PyTorch%20for%20former%20Torchies.ipynb" target="_blank" rel="external">the PyTorch for Torch users tutorial</a>)是一个用来操作输入和隐藏层状态的两层线性层，和处理输出的LogSoftmax层组成。</p>
<p><img src="/image/NameRnn/rnnModel.png"><br>这里说明logsoftmax的公式是$f_i(x)=log(\frac{e^{x_i}}{\sum_{j}e^{x_j}})$<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, output_size)</span>:</span></div><div class="line">        super(RNN, self).__init__()</div><div class="line"></div><div class="line">        self.hidden_size = hidden_size</div><div class="line"></div><div class="line">        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)</div><div class="line">        self.i2o = nn.Linear(input_size + hidden_size, output_size)</div><div class="line">        <span class="comment"># 带有log处理的softmax</span></div><div class="line">        self.softmax = nn.LogSoftmax()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden)</span>:</span></div><div class="line">        <span class="comment"># 按照dim=1列进行拼接</span></div><div class="line">        combined = torch.cat((input, hidden), <span class="number">1</span>)</div><div class="line">        hidden = self.i2h(combined)</div><div class="line">        output = self.i2o(combined)</div><div class="line">        output = self.softmax(output)</div><div class="line">        <span class="keyword">return</span> output, hidden</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> Variable(torch.zeros(<span class="number">1</span>, self.hidden_size))</div><div class="line"></div><div class="line">n_hidden = <span class="number">128</span></div><div class="line">rnn = RNN(n_letters, n_hidden, n_categories)</div></pre></td></tr></table></figure></p>
<p>为了运行这样的网络一步我们需要通过输入数据(在我们的例子中是表示当前字母的Tensor)和上一个隐层状态(我们先使用0来初始化隐层状态)。我们将获得输出(分类语言的概率)和下一个隐层状态(保存以便下一步使用)</p>
<p>需要留意PyTorch模型是在Variable上操作而不是直接操作Tensor。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">input = Variable(letterToTensor(<span class="string">'A'</span>))</div><div class="line">hidden = Variable(torch.zeros(<span class="number">1</span>, n_hidden))</div><div class="line"></div><div class="line">output, next_hidden = rnn(input, hidden)</div></pre></td></tr></table></figure></p>
<p>从效率考虑我们不想每步都创建一个Tensor，因此我们使用<code>lineToTensor</code>而不是<code>letterToTensor</code>然后使用切片操作。这个通过预计算Tensor的批次来进一步优化。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">input = Variable(lineToTensor(<span class="string">'Albert'</span>))</div><div class="line">hidden = Variable(torch.zeros(<span class="number">1</span>, n_hidden))</div><div class="line"></div><div class="line">output, next_hidden = rnn(input[<span class="number">0</span>], hidden)</div><div class="line">print(output)</div></pre></td></tr></table></figure></p>
<p>Out:</p>
<blockquote>
<p>Variable containing:</p>
<p>Columns 0 to 9<br>-2.9407 -3.0152 -2.7955 -2.8986 -2.9942 -2.8066 -2.7735 -2.8927 -2.8127 -2.9285</p>
<p>Columns 10 to 17<br>-2.9342 -2.9568 -2.9258 -2.8479 -2.9587 -2.8280 -2.8510 -2.9090<br>[torch.FloatTensor of size 1x18]</p>
</blockquote>
<p>正如你看到的输出是一个<code>&lt;1 x n_categories&gt;</code>Tensor，它的每一个小项目都是名字所属语言分类的似然估计(越大越可能)</p>
<h2 id="Training-训练"><a href="#Training-训练" class="headerlink" title="Training(训练)"></a>Training(训练)</h2><h3 id="Preparing-for-Training-训练准备"><a href="#Preparing-for-Training-训练准备" class="headerlink" title="Preparing for Training(训练准备)"></a>Preparing for Training(训练准备)</h3><p>在进行训练前我们应该完成一些有用的函数。第一个是用来解析输出结果的函数，我们知道它们是属于某个分类的似然估计。我们使用<code>Tensor.topk</code>去获取最大值的索引。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">categoryFromOutput</span><span class="params">(output)</span>:</span></div><div class="line">    <span class="comment"># topk(1)返回每一行数据最大的一个值的取值和在每行的位置信息。</span></div><div class="line">    top_n, top_i = output.data.topk(<span class="number">1</span>) <span class="comment"># Tensor out of Variable with .data</span></div><div class="line">    category_i = top_i[<span class="number">0</span>][<span class="number">0</span>]</div><div class="line">    <span class="keyword">return</span> all_categories[category_i], category_i</div><div class="line"></div><div class="line">print(categoryFromOutput(output))</div></pre></td></tr></table></figure></p>
<p>Out：</p>
<blockquote>
<p>(‘German’, 6)</p>
</blockquote>
<p>我们也需要一个快速获取训练样例的方法(名字加所属分类)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> random</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomChoice</span><span class="params">(l)</span>:</span></div><div class="line">    <span class="keyword">return</span> l[random.randint(<span class="number">0</span>, len(l) - <span class="number">1</span>)]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomTrainingPair</span><span class="params">()</span>:</span></div><div class="line">    category = randomChoice(all_categories)</div><div class="line">    line = randomChoice(category_lines[category])</div><div class="line">    category_tensor = Variable(torch.LongTensor([all_categories.index(category)]))</div><div class="line">    line_tensor = Variable(lineToTensor(line))</div><div class="line">    <span class="keyword">return</span> category, line, category_tensor, line_tensor</div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">    category, line, category_tensor, line_tensor = randomTrainingPair()</div><div class="line">    print(<span class="string">'category ='</span>, category, <span class="string">'/ line ='</span>, line)</div></pre></td></tr></table></figure></p>
<p>Out：</p>
<blockquote>
<p>category = Dutch / line = Oomen<br>category = Arabic / line = Kassis<br>category = Irish / line = Mclain<br>category = Czech / line = Kerner<br>category = Chinese / line = She<br>category = Scottish / line = Anderson<br>category = Polish / line = Sokolof<br>category = Irish / line = Daly<br>category = Vietnamese / line = An<br>category = Russian / line = Tsagareli</p>
</blockquote>
<h3 id="Training-the-Network-训练神经网络"><a href="#Training-the-Network-训练神经网络" class="headerlink" title="Training the Network(训练神经网络)"></a>Training the Network(训练神经网络)</h3><p>现在所有要做的通过展示大量的训练例子给神经网络进行训练，让它进行预测，然后告诉它正确与否。</p>
<p>使用<code>nn.NLLLoss</code>作为损失函数便足够了，因为RNNs的最后一层事<code>nn.LogSoftmax</code>它加上<code>nn.NLLLoss</code>便是交叉熵损失函数的表达式。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">criterion = nn.NLLLoss()</div></pre></td></tr></table></figure></p>
<p>每个训练中的循环会完成：</p>
<ul>
<li>创建输入和目标tensor</li>
<li>使用0初始化隐藏状态</li>
<li>读取每个字母<ul>
<li>并为下轮保持隐藏状态</li>
</ul>
</li>
<li>计算输出层输出结果</li>
<li>进行反向传播</li>
<li>返回输出结果和误差</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">learning_rate = <span class="number">0.005</span> <span class="comment"># If you set this too high, it might explode. If too low, it might not learn</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(category_tensor, line_tensor)</span>:</span></div><div class="line">    hidden = rnn.initHidden()</div><div class="line"></div><div class="line">    rnn.zero_grad()</div><div class="line"></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(line_tensor.size()[<span class="number">0</span>]):</div><div class="line">        output, hidden = rnn(line_tensor[i], hidden)</div><div class="line"></div><div class="line">    loss = criterion(output, category_tensor)</div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    <span class="comment"># Add parameters' gradients to their values, multiplied by learning rate</span></div><div class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> rnn.parameters():</div><div class="line">        p.data.add_(-learning_rate, p.grad.data)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> output, loss.data[<span class="number">0</span>]</div></pre></td></tr></table></figure>
<p>现在我们需要训练大量的训练样例。因为<code>train</code>函数返回了输出和损失我们可以打印预测值和保存损失以便绘图。因为这里有1000s训练例子我们按照<code>print_every</code>一次时间步骤打印，然后取loss的平均值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">import</span> math</div><div class="line"></div><div class="line">n_epochs = <span class="number">100000</span></div><div class="line">print_every = <span class="number">5000</span></div><div class="line">plot_every = <span class="number">1000</span></div><div class="line"></div><div class="line">rnn = RNN(n_letters, n_hidden, n_categories)</div><div class="line"></div><div class="line"><span class="comment"># Keep track of losses for plotting</span></div><div class="line">current_loss = <span class="number">0</span></div><div class="line">all_losses = []</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeSince</span><span class="params">(since)</span>:</span></div><div class="line">    now = time.time()</div><div class="line">    s = now - since</div><div class="line">    m = math.floor(s / <span class="number">60</span>)</div><div class="line">    s -= m * <span class="number">60</span></div><div class="line">    <span class="keyword">return</span> <span class="string">'%dm %ds'</span> % (m, s)</div><div class="line"></div><div class="line">start = time.time()</div><div class="line"></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, n_epochs + <span class="number">1</span>):</div><div class="line">    category, line, category_tensor, line_tensor = randomTrainingPair()</div><div class="line">    output, loss = train(category_tensor, line_tensor)</div><div class="line">    current_loss += loss</div><div class="line"></div><div class="line">    <span class="comment"># Print epoch number, loss, name and guess</span></div><div class="line">    <span class="keyword">if</span> epoch % print_every == <span class="number">0</span>:</div><div class="line">        guess, guess_i = categoryFromOutput(output)</div><div class="line">        correct = <span class="string">'✓'</span> <span class="keyword">if</span> guess == category <span class="keyword">else</span> <span class="string">'✗ (%s)'</span> % category</div><div class="line">        print(<span class="string">'%d %d%% (%s) %.4f %s / %s %s'</span> % (epoch, epoch / n_epochs * <span class="number">100</span>, timeSince(start), loss, line, guess, correct))</div><div class="line"></div><div class="line">    <span class="comment"># Add current loss avg to list of losses</span></div><div class="line">    <span class="keyword">if</span> epoch % plot_every == <span class="number">0</span>:</div><div class="line">        all_losses.append(current_loss / plot_every)</div><div class="line">        current_loss = <span class="number">0</span></div></pre></td></tr></table></figure></p>
<p>Out:</p>
<blockquote>
<p>5000 5% (0m 5s) 2.8818 Aodh / Vietnamese ✗ (Irish)<br>10000 10% (0m 10s) 2.5445 Radford / Japanese ✗ (English)<br>15000 15% (0m 15s) 2.6274 Viteri / Italian ✗ (Spanish)<br>20000 20% (0m 20s) 3.4456 Molloy / Scottish ✗ (Irish)<br>25000 25% (0m 25s) 0.3957 Demakis / Greek ✓<br>30000 30% (0m 30s) 3.5076 Jez / Chinese ✗ (Polish)<br>35000 35% (0m 35s) 2.1709 Docherty / English ✗ (Scottish)<br>40000 40% (0m 40s) 0.9810 Paszek / Polish ✓<br>45000 45% (0m 45s) 0.7359 Ryu / Korean ✓<br>50000 50% (0m 50s) 2.6953 Delaney / Spanish ✗ (Irish)<br>55000 55% (0m 55s) 1.4272 Lieu / Vietnamese ✓<br>60000 60% (1m 0s) 1.8603 Holzer / Polish ✗ (German)<br>65000 65% (1m 5s) 2.6793 Costa / Czech ✗ (Portuguese)<br>70000 70% (1m 10s) 0.6097 Dalach / Irish ✓<br>75000 75% (1m 15s) 0.1429 Kowalczyk / Polish ✓<br>80000 80% (1m 20s) 0.8101 Cha / Korean ✓<br>85000 85% (1m 25s) 0.2391 Travert / French ✓<br>90000 90% (1m 30s) 1.8437 Nuremberg / Dutch ✗ (German)<br>95000 95% (1m 35s) 1.2834 Faucher / French ✓<br>100000 100% (1m 40s) 0.5508 Urogataya / Japanese ✓</p>
</blockquote>
<h3 id="Plotting-the-Results-绘制结果"><a href="#Plotting-the-Results-绘制结果" class="headerlink" title="Plotting the Results(绘制结果)"></a>Plotting the Results(绘制结果)</h3><p>根据<code>all_losses</code>绘制历史损失展示深度网络学习。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> matplotlib.ticker <span class="keyword">as</span> ticker</div><div class="line"></div><div class="line">plt.figure()</div><div class="line">plt.plot(all_losses)</div></pre></td></tr></table></figure></p>
<p><img src="/image/NameRnn/loss.png"></p>
<h2 id="Evaluating-the-Results（验证结果）"><a href="#Evaluating-the-Results（验证结果）" class="headerlink" title="Evaluating the Results（验证结果）"></a>Evaluating the Results（验证结果）</h2><p>为了观察神经网络在不同分类熵的表现，我们会创建一个混和的矩阵，标示出实际语言(行)和模型预测语言(列)。为了计算混合矩阵一系列的样本将在神经网络中运行通过<code>evaluate()</code>，它和<code>train()</code>除去backprop一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Keep track of correct guesses in a confusion matrix</span></div><div class="line">confusion = torch.zeros(n_categories, n_categories)</div><div class="line">n_confusion = <span class="number">10000</span></div><div class="line"></div><div class="line"><span class="comment"># Just return an output given a line</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(line_tensor)</span>:</span></div><div class="line">    hidden = rnn.initHidden()</div><div class="line"></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(line_tensor.size()[<span class="number">0</span>]):</div><div class="line">        output, hidden = rnn(line_tensor[i], hidden)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> output</div><div class="line"></div><div class="line"><span class="comment"># Go through a bunch of examples and record which are correctly guessed</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_confusion):</div><div class="line">    category, line, category_tensor, line_tensor = randomTrainingPair()</div><div class="line">    output = evaluate(line_tensor)</div><div class="line">    guess, guess_i = categoryFromOutput(output)</div><div class="line">    category_i = all_categories.index(category)</div><div class="line">    confusion[category_i][guess_i] += <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># Normalize by dividing every row by its sum</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_categories):</div><div class="line">    confusion[i] = confusion[i] / confusion[i].sum()</div><div class="line"></div><div class="line"><span class="comment"># Set up plot</span></div><div class="line">fig = plt.figure()</div><div class="line">ax = fig.add_subplot(<span class="number">111</span>)</div><div class="line">cax = ax.matshow(confusion.numpy())</div><div class="line">fig.colorbar(cax)</div><div class="line"></div><div class="line"><span class="comment"># Set up axes</span></div><div class="line">ax.set_xticklabels([<span class="string">''</span>] + all_categories, rotation=<span class="number">90</span>)</div><div class="line">ax.set_yticklabels([<span class="string">''</span>] + all_categories)</div><div class="line"></div><div class="line"><span class="comment"># Force label at every tick</span></div><div class="line">ax.xaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</div><div class="line">ax.yaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</div><div class="line"></div><div class="line"><span class="comment"># sphinx_gallery_thumbnail_number = 2</span></div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<image src="/image/NameRnn/group.png">

<p>你从非对角线上挑选一些亮点出来，它们表示语言预测不对，e.g.中文对应韩文，西班牙语对应意大利语。看起来希腊语预测比较好，英语预测的不好(可能英语和其它语言重叠笔记多)</p>
<h3 id="Running-on-User-Input-运行用户输入"><a href="#Running-on-User-Input-运行用户输入" class="headerlink" title="Running on User Input(运行用户输入)"></a>Running on User Input(运行用户输入)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(input_line, n_predictions=<span class="number">3</span>)</span>:</span></div><div class="line">    print(<span class="string">'\n&gt; %s'</span> % input_line)</div><div class="line">    output = evaluate(Variable(lineToTensor(input_line)))</div><div class="line"></div><div class="line">    <span class="comment"># Get top N categories</span></div><div class="line">    topv, topi = output.data.topk(n_predictions, <span class="number">1</span>, <span class="keyword">True</span>)</div><div class="line">    predictions = []</div><div class="line"></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n_predictions):</div><div class="line">        value = topv[<span class="number">0</span>][i]</div><div class="line">        category_index = topi[<span class="number">0</span>][i]</div><div class="line">        print(<span class="string">'(%.2f) %s'</span> % (value, all_categories[category_index]))</div><div class="line">        predictions.append([value, all_categories[category_index]])</div><div class="line"></div><div class="line">predict(<span class="string">'Dovesky'</span>)</div><div class="line">predict(<span class="string">'Jackson'</span>)</div><div class="line">predict(<span class="string">'Satoshi'</span>)</div></pre></td></tr></table></figure>
<p>Out:</p>
<blockquote>
<p>Dovesky<br>(-0.46) Russian<br>(-1.51) Czech<br>(-2.63) English</p>
<p>Jackson<br>(-0.83) Scottish<br>(-1.10) English<br>(-2.34) Russian</p>
<p>Satoshi<br>(-0.96) Italian<br>(-1.61) Arabic<br>(-1.90) Japanese</p>
</blockquote>
<p>最终的版本的脚本在<a href="">Practical PyTorch repo</a>拆分成了下面的几个文件</p>
<ul>
<li><code>data.py</code>(加载文件)</li>
<li><code>model.py</code>(定义RNN)</li>
<li><code>train.py</code>(进行训练)</li>
<li><code>predict.py</code>(运行<code>predict()</code>使用命令行)</li>
<li><code>server.py</code>（在JSON API使用bottel.py运行预测服务）</li>
</ul>
<p>运行<code>train.py</code>去训练和保存神经网络。<br>运行<code>predict.py</code>带名字可以看预测结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ python predict.py Hazaki</div><div class="line">(<span class="number">-0.42</span>) Japanese</div><div class="line">(<span class="number">-1.39</span>) Polish</div><div class="line">(<span class="number">-3.51</span>) Czech</div></pre></td></tr></table></figure>
<p>运行<code>server.py</code>然后访问<a href="http://localhost:5533/Yourname" target="_blank" rel="external">http://localhost:5533/Yourname</a>获取JSON输出的预测结果。</p>
</image>]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
            <category> translate </category>
            
        </categories>
        
        
        <tags>
            
            <tag> rnn </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Transfer Learning tutorial(翻译)]]></title>
      <url>/2017/05/21/TransLearning/</url>
      <content type="html"><![CDATA[<p>原文链接：<a href="http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html" target="_blank" rel="external">Transfer Learning tutorial</a><br>作者：<a href="https://chsasank.github.io" target="_blank" rel="external">Sasank Chilamkurthy</a><br>在这篇文章中，你将会学到如何通过迁移学习训练你的神经网络。你可以在<a href="http://cs231n.github.io/transfer-learning/" target="_blank" rel="external">cs231n notes</a>上了解更过关于迁移学习的内容。<br>引用这个笔记：</p>
<blockquote>
<p>在实践中，很少人会从零开始训练一个卷积神经网络(使用随机初始化)，因为拥有一个相当数量的训练数据是不常见的。作为替代，通常做法是对一个卷积神经网络在大数据上(e.g. ImageNet，包括了120万图片和1000分类)做预训练，然后使用这个卷积神经网络作为权值初始化使用或者任务关注的固定特征提取器。</p>
</blockquote>
<a id="more"></a>
<p>迁移学习包含了下面两个重点：   </p>
<ul>
<li><strong>卷积网络微调</strong>：不是随机的去初始化神经网络，我们使用那些就像在imagenet的1000个数据集上预训练的神经网络去初始化。剩下的训练过程和正常训练过程一致。   </li>
<li><strong>卷积神经网络作为固定特征提取器</strong>：这里，我们将除最后一个全连接层外的其它层权值参数都固定。将最后的全连接层替换为一个随机初始化权值后的层，也就只有这个层被训练。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># License: BSD</span></div><div class="line"><span class="comment"># Author: Sasank Chilamkurthy</span></div><div class="line"><span class="comment"># from __future__是在当前版本到处一些未来会定型的特性出来使用比如变成函数的print</span></div><div class="line"><span class="comment"># print_function 和 真实世界的除法 division 不会进行向下取整，如果需要向下取整</span></div><div class="line"><span class="comment"># 就使用//</span></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function, division</div><div class="line"></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> torchvision</div><div class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, models, transforms</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">import</span> copy</div><div class="line"><span class="keyword">import</span> os</div><div class="line"></div><div class="line">plt.ion()   <span class="comment"># interactive mode</span></div></pre></td></tr></table></figure>
<h2 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h2><p>我们会使用torchvision和torch.utils.data包来加载数据。<br>我们现在的任务是训练一个蚂蚁和蜜蜂的分类模型。我们对于每个蚂蚁和蜜蜂的分类各有120个训练图片。对应每个分类有75个验证图片。如果是从头开始训练的话，这点数据显得很少。因此我们使用迁移学习，这样可以达到一个好的效果。</p>
<p>这是imagenet的一非常小的数据集。<br><strong>Note</strong></p>
<blockquote>
<p>通过<a href="https://download.pytorch.org/tutorial/hymenoptera_data.zip" target="_blank" rel="external">这里</a>来下载数据并解压到当前文件。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Data augmentation and normalization for training</span></div><div class="line"><span class="comment"># Just normalization for validation</span></div><div class="line">data_transforms = &#123;</div><div class="line">    <span class="string">'train'</span>: transforms.Compose([</div><div class="line">        transforms.RandomSizedCrop(<span class="number">224</span>),</div><div class="line">        transforms.RandomHorizontalFlip(),</div><div class="line">        transforms.ToTensor(),</div><div class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</div><div class="line">    ]),</div><div class="line">    <span class="string">'val'</span>: transforms.Compose([</div><div class="line">        transforms.Scale(<span class="number">256</span>),</div><div class="line">        transforms.CenterCrop(<span class="number">224</span>),</div><div class="line">        transforms.ToTensor(),</div><div class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</div><div class="line">    ]),</div><div class="line">&#125;</div><div class="line"></div><div class="line">data_dir = <span class="string">'hymenoptera_data'</span></div><div class="line">dsets = &#123;x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])</div><div class="line">         <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</div><div class="line">dset_loaders = &#123;x: torch.utils.data.DataLoader(dsets[x], batch_size=<span class="number">4</span>,</div><div class="line">                                               shuffle=<span class="keyword">True</span>, num_workers=<span class="number">4</span>)</div><div class="line">                <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</div><div class="line">dset_sizes = &#123;x: len(dsets[x]) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</div><div class="line">dset_classes = dsets[<span class="string">'train'</span>].classes</div><div class="line"></div><div class="line">use_gpu = torch.cuda.is_available()</div></pre></td></tr></table></figure>
<h3 id="可视化一些图片"><a href="#可视化一些图片" class="headerlink" title="可视化一些图片"></a>可视化一些图片</h3><p>让我们可视化一些训练数据以便理解数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(inp, title=None)</span>:</span></div><div class="line">    <span class="string">"""Imshow for Tensor."""</span></div><div class="line">    inp = inp.numpy().transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</div><div class="line">    mean = np.array([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>])</div><div class="line">    std = np.array([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</div><div class="line">    inp = std * inp + mean</div><div class="line">    plt.imshow(inp)</div><div class="line">    <span class="keyword">if</span> title <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        plt.title(title)</div><div class="line">    plt.pause(<span class="number">0.001</span>)  <span class="comment"># pause a bit so that plots are updated</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># Get a batch of training data</span></div><div class="line">inputs, classes = next(iter(dset_loaders[<span class="string">'train'</span>]))</div><div class="line"></div><div class="line"><span class="comment"># Make a grid from batch</span></div><div class="line">out = torchvision.utils.make_grid(inputs)</div><div class="line"></div><div class="line">imshow(out, title=[dset_classes[x] <span class="keyword">for</span> x <span class="keyword">in</span> classes])</div></pre></td></tr></table></figure></p>
<p><img src="/image/transLearn/translenVisimage.png"></p>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>现在，让我们写一个通用的函数去训练模型。这里会做出说明：</p>
<ul>
<li>安排学习率</li>
<li>保存(深拷贝)最好的模型<br>接下来，参数<code>lr_scheduler(optimizer, epoch)</code>是一个用来修改optimizer的函数以便可以根据计划改变学习率。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model, criterion, optimizer, lr_scheduler, num_epochs=<span class="number">25</span>)</span>:</span></div><div class="line">    since = time.time()</div><div class="line"></div><div class="line">    best_model = model</div><div class="line">    best_acc = <span class="number">0.0</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</div><div class="line">        print(<span class="string">'Epoch &#123;&#125;/&#123;&#125;'</span>.format(epoch, num_epochs - <span class="number">1</span>))</div><div class="line">        print(<span class="string">'-'</span> * <span class="number">10</span>)</div><div class="line"></div><div class="line">        <span class="comment"># Each epoch has a training and validation phase</span></div><div class="line">        <span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]:</div><div class="line">            <span class="keyword">if</span> phase == <span class="string">'train'</span>:</div><div class="line">                optimizer = lr_scheduler(optimizer, epoch)</div><div class="line">                model.train(<span class="keyword">True</span>)  <span class="comment"># Set model to training mode</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                model.train(<span class="keyword">False</span>)  <span class="comment"># Set model to evaluate mode</span></div><div class="line"></div><div class="line">            running_loss = <span class="number">0.0</span></div><div class="line">            running_corrects = <span class="number">0</span></div><div class="line"></div><div class="line">            <span class="comment"># Iterate over data.</span></div><div class="line">            <span class="keyword">for</span> data <span class="keyword">in</span> dset_loaders[phase]:</div><div class="line">                <span class="comment"># get the inputs</span></div><div class="line">                inputs, labels = data</div><div class="line"></div><div class="line">                <span class="comment"># wrap them in Variable</span></div><div class="line">                <span class="keyword">if</span> use_gpu:</div><div class="line">                    inputs, labels = Variable(inputs.cuda()), \</div><div class="line">                        Variable(labels.cuda())</div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    inputs, labels = Variable(inputs), Variable(labels)</div><div class="line"></div><div class="line">                <span class="comment"># zero the parameter gradients</span></div><div class="line">                optimizer.zero_grad()</div><div class="line"></div><div class="line">                <span class="comment"># forward</span></div><div class="line">                outputs = model(inputs)</div><div class="line">                _, preds = torch.max(outputs.data, <span class="number">1</span>)</div><div class="line">                loss = criterion(outputs, labels)</div><div class="line"></div><div class="line">                <span class="comment"># backward + optimize only if in training phase</span></div><div class="line">                <span class="keyword">if</span> phase == <span class="string">'train'</span>:</div><div class="line">                    loss.backward()</div><div class="line">                    optimizer.step()</div><div class="line"></div><div class="line">                <span class="comment"># statistics</span></div><div class="line">                running_loss += loss.data[<span class="number">0</span>]</div><div class="line">                running_corrects += torch.sum(preds == labels.data)</div><div class="line"></div><div class="line">            epoch_loss = running_loss / dset_sizes[phase]</div><div class="line">            epoch_acc = running_corrects / dset_sizes[phase]</div><div class="line"></div><div class="line">            print(<span class="string">'&#123;&#125; Loss: &#123;:.4f&#125; Acc: &#123;:.4f&#125;'</span>.format(</div><div class="line">                phase, epoch_loss, epoch_acc))</div><div class="line"></div><div class="line">            <span class="comment"># deep copy the model</span></div><div class="line">            <span class="keyword">if</span> phase == <span class="string">'val'</span> <span class="keyword">and</span> epoch_acc &gt; best_acc:</div><div class="line">                best_acc = epoch_acc</div><div class="line">                best_model = copy.deepcopy(model)</div><div class="line"></div><div class="line">        print()</div><div class="line"></div><div class="line">    time_elapsed = time.time() - since</div><div class="line">    print(<span class="string">'Training complete in &#123;:.0f&#125;m &#123;:.0f&#125;s'</span>.format(</div><div class="line">        time_elapsed // <span class="number">60</span>, time_elapsed % <span class="number">60</span>))</div><div class="line">    print(<span class="string">'Best val Acc: &#123;:4f&#125;'</span>.format(best_acc))</div><div class="line">    <span class="keyword">return</span> best_model</div></pre></td></tr></table></figure>
<h3 id="学习率计划"><a href="#学习率计划" class="headerlink" title="学习率计划"></a>学习率计划</h3><p>让我们创建一个学习率计划，在每个外层循环步骤我们会呈几何的减小学习率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">exp_lr_scheduler</span><span class="params">(optimizer, epoch, init_lr=<span class="number">0.001</span>, lr_decay_epoch=<span class="number">7</span>)</span>:</span></div><div class="line">    <span class="string">"""Decay learning rate by a factor of 0.1 every lr_decay_epoch epochs."""</span></div><div class="line">    lr = init_lr * (<span class="number">0.1</span>**(epoch // lr_decay_epoch))</div><div class="line"></div><div class="line">    <span class="keyword">if</span> epoch % lr_decay_epoch == <span class="number">0</span>:</div><div class="line">        print(<span class="string">'LR is set to &#123;&#125;'</span>.format(lr))</div><div class="line"></div><div class="line">    <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</div><div class="line">        param_group[<span class="string">'lr'</span>] = lr</div><div class="line"></div><div class="line">    <span class="keyword">return</span> optimizer</div></pre></td></tr></table></figure>
<h3 id="可视化模型预测"><a href="#可视化模型预测" class="headerlink" title="可视化模型预测"></a>可视化模型预测</h3><p>一个通用的函数用来展示一些图片预测。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_model</span><span class="params">(model, num_images=<span class="number">6</span>)</span>:</span></div><div class="line">    images_so_far = <span class="number">0</span></div><div class="line">    fig = plt.figure()</div><div class="line"></div><div class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(dset_loaders[<span class="string">'val'</span>]):</div><div class="line">        inputs, labels = data</div><div class="line">        <span class="keyword">if</span> use_gpu:</div><div class="line">            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            inputs, labels = Variable(inputs), Variable(labels)</div><div class="line"></div><div class="line">        outputs = model(inputs)</div><div class="line">        _, preds = torch.max(outputs.data, <span class="number">1</span>)</div><div class="line"></div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(inputs.size()[<span class="number">0</span>]):</div><div class="line">            images_so_far += <span class="number">1</span></div><div class="line">            ax = plt.subplot(num_images//<span class="number">2</span>, <span class="number">2</span>, images_so_far)</div><div class="line">            ax.axis(<span class="string">'off'</span>)</div><div class="line">            ax.set_title(<span class="string">'predicted: &#123;&#125;'</span>.format(dset_classes[labels.data[j]]))</div><div class="line">            imshow(inputs.cpu().data[j])</div><div class="line"></div><div class="line">            <span class="keyword">if</span> images_so_far == num_images:</div><div class="line">                <span class="keyword">return</span></div></pre></td></tr></table></figure></p>
<h2 id="卷积网络微调"><a href="#卷积网络微调" class="headerlink" title="卷积网络微调"></a>卷积网络微调</h2><p>加载预训练的模型然后重置最后一层全连接层。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">model_ft = models.resnet18(pretrained=<span class="keyword">True</span>)</div><div class="line">num_ftrs = model_ft.fc.in_features</div><div class="line">model_ft.fc = nn.Linear(num_ftrs, <span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">if</span> use_gpu:</div><div class="line">    model_ft = model_ft.cuda()</div><div class="line"></div><div class="line">criterion = nn.CrossEntropyLoss()</div><div class="line"></div><div class="line"><span class="comment"># Observe that all parameters are being optimized</span></div><div class="line">optimizer_ft = optim.SGD(model_ft.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</div></pre></td></tr></table></figure></p>
<h3 id="训练和评判"><a href="#训练和评判" class="headerlink" title="训练和评判"></a>训练和评判</h3><p>在CPU上会花费15-25分钟。在GPU上的花费少于1分钟。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25</div></pre></td></tr></table></figure></p>
<p>OUT：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div></pre></td><td class="code"><pre><div class="line">Epoch 0/24</div><div class="line">----------</div><div class="line">LR is set to 0.001</div><div class="line">train Loss: 0.1565 Acc: 0.6844</div><div class="line">val Loss: 0.1242 Acc: 0.7908</div><div class="line"></div><div class="line">Epoch 1/24</div><div class="line">----------</div><div class="line">train Loss: 0.1269 Acc: 0.7582</div><div class="line">val Loss: 0.0473 Acc: 0.9150</div><div class="line"></div><div class="line">Epoch 2/24</div><div class="line">----------</div><div class="line">train Loss: 0.1062 Acc: 0.8156</div><div class="line">val Loss: 0.0714 Acc: 0.8562</div><div class="line"></div><div class="line">Epoch 3/24</div><div class="line">----------</div><div class="line">train Loss: 0.0993 Acc: 0.8156</div><div class="line">val Loss: 0.0566 Acc: 0.9085</div><div class="line"></div><div class="line">Epoch 4/24</div><div class="line">----------</div><div class="line">train Loss: 0.0962 Acc: 0.8730</div><div class="line">val Loss: 0.2831 Acc: 0.6863</div><div class="line"></div><div class="line">Epoch 5/24</div><div class="line">----------</div><div class="line">train Loss: 0.1778 Acc: 0.7787</div><div class="line">val Loss: 0.4696 Acc: 0.6667</div><div class="line"></div><div class="line">Epoch 6/24</div><div class="line">----------</div><div class="line">train Loss: 0.1765 Acc: 0.8238</div><div class="line">val Loss: 0.0994 Acc: 0.8758</div><div class="line"></div><div class="line">Epoch 7/24</div><div class="line">----------</div><div class="line">LR is set to 0.0001</div><div class="line">train Loss: 0.1600 Acc: 0.8156</div><div class="line">val Loss: 0.0850 Acc: 0.8824</div><div class="line"></div><div class="line">Epoch 8/24</div><div class="line">----------</div><div class="line">train Loss: 0.0876 Acc: 0.8730</div><div class="line">val Loss: 0.0841 Acc: 0.8889</div><div class="line"></div><div class="line">Epoch 9/24</div><div class="line">----------</div><div class="line">train Loss: 0.1105 Acc: 0.8279</div><div class="line">val Loss: 0.0777 Acc: 0.8954</div><div class="line"></div><div class="line">Epoch 10/24</div><div class="line">----------</div><div class="line">train Loss: 0.0976 Acc: 0.8607</div><div class="line">val Loss: 0.0719 Acc: 0.9020</div><div class="line"></div><div class="line">Epoch 11/24</div><div class="line">----------</div><div class="line">train Loss: 0.0644 Acc: 0.9057</div><div class="line">val Loss: 0.0637 Acc: 0.9085</div><div class="line"></div><div class="line">Epoch 12/24</div><div class="line">----------</div><div class="line">train Loss: 0.1003 Acc: 0.8279</div><div class="line">val Loss: 0.0678 Acc: 0.9085</div><div class="line"></div><div class="line">Epoch 13/24</div><div class="line">----------</div><div class="line">train Loss: 0.0755 Acc: 0.8770</div><div class="line">val Loss: 0.0576 Acc: 0.9150</div><div class="line"></div><div class="line">Epoch 14/24</div><div class="line">----------</div><div class="line">LR is set to 1.0000000000000003e-05</div><div class="line">train Loss: 0.0642 Acc: 0.9139</div><div class="line">val Loss: 0.0608 Acc: 0.9150</div><div class="line"></div><div class="line">Epoch 15/24</div><div class="line">----------</div><div class="line">train Loss: 0.0870 Acc: 0.8484</div><div class="line">val Loss: 0.0618 Acc: 0.9085</div><div class="line"></div><div class="line">Epoch 16/24</div><div class="line">----------</div><div class="line">train Loss: 0.0704 Acc: 0.8975</div><div class="line">val Loss: 0.0629 Acc: 0.9085</div><div class="line"></div><div class="line">Epoch 17/24</div><div class="line">----------</div><div class="line">train Loss: 0.0714 Acc: 0.8730</div><div class="line">val Loss: 0.0627 Acc: 0.9150</div><div class="line"></div><div class="line">Epoch 18/24</div><div class="line">----------</div><div class="line">train Loss: 0.0739 Acc: 0.8730</div><div class="line">val Loss: 0.0637 Acc: 0.9020</div><div class="line"></div><div class="line">Epoch 19/24</div><div class="line">----------</div><div class="line">train Loss: 0.0865 Acc: 0.8320</div><div class="line">val Loss: 0.0701 Acc: 0.8954</div><div class="line"></div><div class="line">Epoch 20/24</div><div class="line">----------</div><div class="line">train Loss: 0.0868 Acc: 0.8566</div><div class="line">val Loss: 0.0654 Acc: 0.9216</div><div class="line"></div><div class="line">Epoch 21/24</div><div class="line">----------</div><div class="line">LR is set to 1.0000000000000002e-06</div><div class="line">train Loss: 0.0868 Acc: 0.8525</div><div class="line">val Loss: 0.0619 Acc: 0.9216</div><div class="line"></div><div class="line">Epoch 22/24</div><div class="line">----------</div><div class="line">train Loss: 0.0867 Acc: 0.8730</div><div class="line">val Loss: 0.0742 Acc: 0.8824</div><div class="line"></div><div class="line">Epoch 23/24</div><div class="line">----------</div><div class="line">train Loss: 0.0860 Acc: 0.8566</div><div class="line">val Loss: 0.0603 Acc: 0.9150</div><div class="line"></div><div class="line">Epoch 24/24</div><div class="line">----------</div><div class="line">train Loss: 0.1004 Acc: 0.8402</div><div class="line">val Loss: 0.0652 Acc: 0.9085</div><div class="line"></div><div class="line">Training complete in 1m 8s</div><div class="line">Best val Acc: 0.921569</div></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">visualize_model(model_ft)</div></pre></td></tr></table></figure>
<p><img src="/image/transLearn/translenPre.png"></p>
<h2 id="卷积神经网络作为固定特征提取器"><a href="#卷积神经网络作为固定特征提取器" class="headerlink" title="卷积神经网络作为固定特征提取器"></a>卷积神经网络作为固定特征提取器</h2><p>这里，我们需要固定除开最后一个全连接层以外的其它层。我们需要设置<code>requires_grad == False</code>去固定参数这样在进行<code>backward()</code>时他们不会被计算更新。</p>
<p>你可以阅读这里更详细的<a href="http://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-backward" target="_blank" rel="external">文档</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">model_conv = torchvision.models.resnet18(pretrained=<span class="keyword">True</span>)</div><div class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model_conv.parameters():</div><div class="line">    param.requires_grad = <span class="keyword">False</span></div><div class="line"></div><div class="line"><span class="comment"># Parameters of newly constructed modules have requires_grad=True by default</span></div><div class="line">num_ftrs = model_conv.fc.in_features</div><div class="line">model_conv.fc = nn.Linear(num_ftrs, <span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">if</span> use_gpu:</div><div class="line">    model_conv = model_conv.cuda()</div><div class="line"></div><div class="line">criterion = nn.CrossEntropyLoss()</div><div class="line"></div><div class="line"><span class="comment"># Observe that only parameters of final layer are being optimized as</span></div><div class="line"><span class="comment"># opoosed to before.</span></div><div class="line">optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</div></pre></td></tr></table></figure></p>
<h3 id="训练和评判-1"><a href="#训练和评判-1" class="headerlink" title="训练和评判"></a>训练和评判</h3><p>在CPU上会花费上前一个例子一半的时间。这里神经网络的大部分梯度都不需要计算。当然前向还是需要计算。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">model_conv = train_model(model_conv, criterion, optimizer_conv,</div><div class="line">                         exp_lr_scheduler, num_epochs=<span class="number">25</span>)</div></pre></td></tr></table></figure></p>
<p>OUT：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div></pre></td><td class="code"><pre><div class="line">Epoch 0/24</div><div class="line">----------</div><div class="line">LR is set to 0.001</div><div class="line">train Loss: 0.1731 Acc: 0.6025</div><div class="line">val Loss: 0.0868 Acc: 0.8627</div><div class="line"></div><div class="line">Epoch 1/24</div><div class="line">----------</div><div class="line">train Loss: 0.1219 Acc: 0.7336</div><div class="line">val Loss: 0.0470 Acc: 0.9542</div><div class="line"></div><div class="line">Epoch 2/24</div><div class="line">----------</div><div class="line">train Loss: 0.1205 Acc: 0.8033</div><div class="line">val Loss: 0.0505 Acc: 0.9412</div><div class="line"></div><div class="line">Epoch 3/24</div><div class="line">----------</div><div class="line">train Loss: 0.0967 Acc: 0.8156</div><div class="line">val Loss: 0.0736 Acc: 0.8954</div><div class="line"></div><div class="line">Epoch 4/24</div><div class="line">----------</div><div class="line">train Loss: 0.1452 Acc: 0.7664</div><div class="line">val Loss: 0.0447 Acc: 0.9346</div><div class="line"></div><div class="line">Epoch 5/24</div><div class="line">----------</div><div class="line">train Loss: 0.1170 Acc: 0.7992</div><div class="line">val Loss: 0.0791 Acc: 0.8758</div><div class="line"></div><div class="line">Epoch 6/24</div><div class="line">----------</div><div class="line">train Loss: 0.1260 Acc: 0.7787</div><div class="line">val Loss: 0.0551 Acc: 0.9346</div><div class="line"></div><div class="line">Epoch 7/24</div><div class="line">----------</div><div class="line">LR is set to 0.0001</div><div class="line">train Loss: 0.0580 Acc: 0.9016</div><div class="line">val Loss: 0.0473 Acc: 0.9412</div><div class="line"></div><div class="line">Epoch 8/24</div><div class="line">----------</div><div class="line">train Loss: 0.0950 Acc: 0.8115</div><div class="line">val Loss: 0.0579 Acc: 0.9216</div><div class="line"></div><div class="line">Epoch 9/24</div><div class="line">----------</div><div class="line">train Loss: 0.0868 Acc: 0.8689</div><div class="line">val Loss: 0.0500 Acc: 0.9477</div><div class="line"></div><div class="line">Epoch 10/24</div><div class="line">----------</div><div class="line">train Loss: 0.1027 Acc: 0.8033</div><div class="line">val Loss: 0.0476 Acc: 0.9412</div><div class="line"></div><div class="line">Epoch 11/24</div><div class="line">----------</div><div class="line">train Loss: 0.0911 Acc: 0.8443</div><div class="line">val Loss: 0.0500 Acc: 0.9477</div><div class="line"></div><div class="line">Epoch 12/24</div><div class="line">----------</div><div class="line">train Loss: 0.0968 Acc: 0.8361</div><div class="line">val Loss: 0.0520 Acc: 0.9412</div><div class="line"></div><div class="line">Epoch 13/24</div><div class="line">----------</div><div class="line">train Loss: 0.0930 Acc: 0.8361</div><div class="line">val Loss: 0.0501 Acc: 0.9412</div><div class="line"></div><div class="line">Epoch 14/24</div><div class="line">----------</div><div class="line">LR is set to 1.0000000000000003e-05</div><div class="line">train Loss: 0.1072 Acc: 0.7992</div><div class="line">val Loss: 0.0631 Acc: 0.9150</div><div class="line"></div><div class="line">Epoch 15/24</div><div class="line">----------</div><div class="line">train Loss: 0.1021 Acc: 0.8197</div><div class="line">val Loss: 0.0458 Acc: 0.9477</div><div class="line"></div><div class="line">Epoch 16/24</div><div class="line">----------</div><div class="line">train Loss: 0.1014 Acc: 0.8279</div><div class="line">val Loss: 0.0467 Acc: 0.9477</div><div class="line"></div><div class="line">Epoch 17/24</div><div class="line">----------</div><div class="line">train Loss: 0.0645 Acc: 0.8934</div><div class="line">val Loss: 0.0517 Acc: 0.9477</div><div class="line"></div><div class="line">Epoch 18/24</div><div class="line">----------</div><div class="line">train Loss: 0.0704 Acc: 0.8811</div><div class="line">val Loss: 0.0511 Acc: 0.9346</div><div class="line"></div><div class="line">Epoch 19/24</div><div class="line">----------</div><div class="line">train Loss: 0.0821 Acc: 0.8648</div><div class="line">val Loss: 0.0469 Acc: 0.9412</div><div class="line"></div><div class="line">Epoch 20/24</div><div class="line">----------</div><div class="line">train Loss: 0.0838 Acc: 0.8730</div><div class="line">val Loss: 0.0431 Acc: 0.9477</div><div class="line"></div><div class="line">Epoch 21/24</div><div class="line">----------</div><div class="line">LR is set to 1.0000000000000002e-06</div><div class="line">train Loss: 0.0964 Acc: 0.8443</div><div class="line">val Loss: 0.0433 Acc: 0.9477</div><div class="line"></div><div class="line">Epoch 22/24</div><div class="line">----------</div><div class="line">train Loss: 0.0823 Acc: 0.8525</div><div class="line">val Loss: 0.0462 Acc: 0.9477</div><div class="line"></div><div class="line">Epoch 23/24</div><div class="line">----------</div><div class="line">train Loss: 0.0769 Acc: 0.8525</div><div class="line">val Loss: 0.0452 Acc: 0.9542</div><div class="line"></div><div class="line">Epoch 24/24</div><div class="line">----------</div><div class="line">train Loss: 0.0953 Acc: 0.8320</div><div class="line">val Loss: 0.0594 Acc: 0.9281</div><div class="line"></div><div class="line">Training complete in 0m 35s</div><div class="line">Best val Acc: 0.954248</div></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">visualize_model(model_conv)</div><div class="line"></div><div class="line">plt.ioff()</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><image src="/image/transLearn/translenFeat.png"><br>整个训练脚本花费时间:(1分 48.038秒)</image></p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
            <category> translate </category>
            
        </categories>
        
        
        <tags>
            
            <tag> transfer learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Learning PyTorch with Example(翻译)]]></title>
      <url>/2017/05/20/pytorch1/</url>
      <content type="html"><![CDATA[<p>原文链接：<a href="http://pytorch.org/tutorials/beginner/pytorch_with_examples.html" target="_blank" rel="external">Learning PyTorch with Examples</a><br>作者：<a href="https://github.com/jcjohnson/pytorch-examples" target="_blank" rel="external">Justin Johnos</a></p>
<p>这份学习指导通过下面的例子介绍<a href="https://github.com/pytorch/pytorch" target="_blank" rel="external">PyTorch</a>的一些基本概念。<br>PyTorch提供了以下两个核心的特性：   </p>
<ul>
<li>一个和numpy相似的多维Tensor(张量)，但可在GPU上执行</li>
<li>对构建的神经网络进行自动求导<a id="more"></a>
</li>
</ul>
<p>我们讲解的例子为激活函数是ReLU的全连接神经网络。这个神经网络将只包含一个隐藏层，使用欧式距离作为输出值和实际值度量，我们将使用梯度下降去最小化欧式距离代价函数。</p>
<h2 id="Tensor-张量"><a href="#Tensor-张量" class="headerlink" title="Tensor(张量)"></a>Tensor(张量)</h2><h3 id="热身-numpy"><a href="#热身-numpy" class="headerlink" title="热身:numpy"></a>热身:numpy</h3><p>在正式介绍PyTorch前，我们先通过numpy来实现这个神经网络。<br>Numpy提供了一个n维数组对象，并提供了许多函数来操作这个数组。Numpy是一个通用科学运算框架。它本身对计算图，深度学习和梯度是一无所知。然而我们可以轻松的通过随机数据去训练一个使用numpy实现了前向和后向传播的神经网络。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random input and output data</span></div><div class="line">x = np.random.randn(N, D_in)</div><div class="line">y = np.random.randn(N, D_out)</div><div class="line"></div><div class="line"><span class="comment"># Randomly initialize weights</span></div><div class="line">w1 = np.random.randn(D_in, H)</div><div class="line">w2 = np.random.randn(H, D_out)</div><div class="line"></div><div class="line">learning_rate = <span class="number">1e-6</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: compute predicted y</span></div><div class="line">    h = x.dot(w1)</div><div class="line">    h_relu = np.maximum(h, <span class="number">0</span>)</div><div class="line">    y_pred = h_relu.dot(w2)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss</span></div><div class="line">    loss = np.square(y_pred - y).sum()</div><div class="line">    print(t, loss)</div><div class="line"></div><div class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></div><div class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</div><div class="line">    grad_w2 = h_relu.T.dot(grad_y_pred)</div><div class="line">    grad_h_relu = grad_y_pred.dot(w2.T)</div><div class="line">    grad_h = grad_h_relu.copy()</div><div class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></div><div class="line">    grad_w1 = x.T.dot(grad_h)</div><div class="line"></div><div class="line">    <span class="comment"># Update weights</span></div><div class="line">    w1 -= learning_rate * grad_w1</div><div class="line">    w2 -= learning_rate * grad_w2</div></pre></td></tr></table></figure></p>
<h3 id="PyTorch-Tensor"><a href="#PyTorch-Tensor" class="headerlink" title="PyTorch:Tensor"></a>PyTorch:Tensor</h3><p>Numpy是一个伟大的框架，但是它并不能利用GPUs来对数学计算加速。对于现代的深度神经网络，GPUs通常可以提供50倍甚至更多的提速，因此很可惜numpy不能满足现代深度学习的需求。</p>
<p>接下来我们介绍一个最重要的PyTorch概念:<strong>Tensor(张量)</strong>。在概念上PyTorch的Tensor和numpy的array相似：Tensor是一个n维数组，PyTorch提供了很多函数去操作它。和numpy的array一样，PyTorch Tensor同样对深度学习或者计算图或者梯度等概念一无所知；它同样是一个通用的科学运算工具。</p>
<p>然而不似numpy，PyTorch Tensor 能利用GPUs去加速他们的数学运算。为了可以在GPU上运行PyTorch Tensor，你只需要将它简单的转换为一个新的数据类型。</p>
<p>下面我们使用Pytorch Tensor去训练一个两层的神经网络。像上面的numpy的例子我们需要手动实现神经网络的前向和后向传播。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"></div><div class="line"></div><div class="line">dtype = torch.FloatTensor</div><div class="line"><span class="comment"># 取消下面的注释，程序运行在GPU上</span></div><div class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random input and output data</span></div><div class="line">x = torch.randn(N, D_in).type(dtype)</div><div class="line">y = torch.randn(N, D_out).type(dtype)</div><div class="line"></div><div class="line"><span class="comment"># Randomly initialize weights</span></div><div class="line">w1 = torch.randn(D_in, H).type(dtype)</div><div class="line">w2 = torch.randn(H, D_out).type(dtype)</div><div class="line"></div><div class="line">learning_rate = <span class="number">1e-6</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: compute predicted y</span></div><div class="line">    h = x.mm(w1)</div><div class="line">    h_relu = h.clamp(min=<span class="number">0</span>)</div><div class="line">    y_pred = h_relu.mm(w2)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss</span></div><div class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</div><div class="line">    print(t, loss)</div><div class="line"></div><div class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></div><div class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</div><div class="line">    grad_w2 = h_relu.t().mm(grad_y_pred)</div><div class="line">    grad_h_relu = grad_y_pred.mm(w2.t())</div><div class="line">    grad_h = grad_h_relu.clone()</div><div class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></div><div class="line">    grad_w1 = x.t().mm(grad_h)</div><div class="line"></div><div class="line">    <span class="comment"># Update weights using gradient descent</span></div><div class="line">    w1 -= learning_rate * grad_w1</div><div class="line">    w2 -= learning_rate * grad_w2</div></pre></td></tr></table></figure>
<h2 id="Autograd-自动求导"><a href="#Autograd-自动求导" class="headerlink" title="Autograd(自动求导)"></a>Autograd(自动求导)</h2><h3 id="PyTorch-Variables-and-autograd"><a href="#PyTorch-Variables-and-autograd" class="headerlink" title="PyTorch:Variables and autograd"></a>PyTorch:Variables and autograd</h3><p>在上面的例子中，我们不得不实现神经网络的前向和后向传播。手动实现后向传播对于一个简单的两层神经网络并不是一个困难的事情，但对于一个大型复杂的神经网络将变的非常困难。</p>
<p>幸运的是，我们可以使用<a href="https://en.wikipedia.org/wiki/Automatic_differentiation" target="_blank" rel="external">自动求导</a>来自动的进行神经网络的反向传播过程。在PyTorch中的<strong>autograd</strong>包提供了此功能。当使用自动求导时，神经网络的前向传播将会定义<strong>计算图</strong>；在计算图中的节点就是Tensor，然后连线就是根据输入Tensor产生输出Tensor的函数。反向传播过程通过这个计算图可以很容易计算梯度。</p>
<p>这听起来似乎很复杂，但在练习中使用起来却很简单。我们将PyTorch的Tensor包含在<strong>Variable</strong>对象中；Variable在计算图中实际表示一个节点。如果x是一个Variable那么x.data是Tensor，x.grad是另外一个保存了x的标量梯度值的Variable。</p>
<p>PyTorch Variable拥有和PyTorch Tensor相同的API：(大部分)任何你在Tensor上可以做的操作同样可以在Variable生效；不同点在于，使用Variable可以定义计算图，允许你自动的计算梯度。</p>
<p>下面是我们使用PyTorch Variable和自动梯度计算实现的两层神经网络：现在我们不再需要手动实现神经网络的反向传播。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line">dtype = torch.FloatTensor</div><div class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold input and outputs, and wrap them in Variables.</span></div><div class="line"><span class="comment"># Setting requires_grad=False indicates that we do not need to compute gradients</span></div><div class="line"><span class="comment"># with respect to these Variables during the backward pass.</span></div><div class="line">x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=<span class="keyword">False</span>)</div><div class="line">y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors for weights, and wrap them in Variables.</span></div><div class="line"><span class="comment"># Setting requires_grad=True indicates that we want to compute gradients with</span></div><div class="line"><span class="comment"># respect to these Variables during the backward pass.</span></div><div class="line"><span class="comment"># 需要计算梯度的Variable需要将requires_grad置为True</span></div><div class="line">w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=<span class="keyword">True</span>)</div><div class="line">w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">learning_rate = <span class="number">1e-6</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: compute predicted y using operations on Variables; these</span></div><div class="line">    <span class="comment"># are exactly the same operations we used to compute the forward pass using</span></div><div class="line">    <span class="comment"># Tensors, but we do not need to keep references to intermediate values since</span></div><div class="line">    <span class="comment"># we are not implementing the backward pass by hand.</span></div><div class="line">    <span class="comment"># mm表示矩阵乘法</span></div><div class="line">    y_pred = x.mm(w1).clamp(min=<span class="number">0</span>).mm(w2)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss using operations on Variables.</span></div><div class="line">    <span class="comment"># Now loss is a Variable of shape (1,) and loss.data is a Tensor of shape</span></div><div class="line">    <span class="comment"># (1,); loss.data[0] is a scalar value holding the loss.</span></div><div class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Use autograd to compute the backward pass. This call will compute the</span></div><div class="line">    <span class="comment"># gradient of loss with respect to all Variables with requires_grad=True.</span></div><div class="line">    <span class="comment"># After this call w1.grad and w2.grad will be Variables holding the gradient</span></div><div class="line">    <span class="comment"># of the loss with respect to w1 and w2 respectively.</span></div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    <span class="comment"># Update weights using gradient descent; w1.data and w2.data are Tensors,</span></div><div class="line">    <span class="comment"># w1.grad and w2.grad are Variables and w1.grad.data and w2.grad.data are</span></div><div class="line">    <span class="comment"># Tensors.</span></div><div class="line">    w1.data -= learning_rate * w1.grad.data</div><div class="line">    w2.data -= learning_rate * w2.grad.data</div><div class="line"></div><div class="line">    <span class="comment"># Manually zero the gradients after updating weights</span></div><div class="line">    w1.grad.data.zero_()</div><div class="line">    w2.grad.data.zero_()</div></pre></td></tr></table></figure>
<h3 id="PyTorch-定义一个新的自动梯度计算函数"><a href="#PyTorch-定义一个新的自动梯度计算函数" class="headerlink" title="PyTorch:定义一个新的自动梯度计算函数"></a>PyTorch:定义一个新的自动梯度计算函数</h3><p>掩盖在自动计算梯度下，实际都包含着两个关于Tensor的基本函数操作。一是<strong>前向</strong>函数通过输入的Tensor计算输出Tensor，一是<strong>反向</strong>函数接受输出Tensor的标量梯度，然后根据这些值计算输入Tensor的梯度。</p>
<p>在PyTorch中我们可以轻易的定义我们自己的自动计算梯度操作，通过重定义<strong>torch.autograd.Fuction</strong>然后实现<strong>forward</strong>和<strong>backward</strong>函数。然后我们可以构造一个实例，然后像函数一样去调用它，通过Variable保存输入数据。</p>
<p>在下面的例子中我们自定义了一个ReLu的自动梯度计算函数，使用它来实现我们的两层神经网络：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyReLU</span><span class="params">(torch.autograd.Function)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    We can implement our own custom autograd Functions by subclassing</div><div class="line">    torch.autograd.Function and implementing the forward and backward passes</div><div class="line">    which operate on Tensors.</div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        In the forward pass we receive a Tensor containing the input and return a</div><div class="line">        Tensor containing the output. You can cache arbitrary Tensors for use in the</div><div class="line">        backward pass using the save_for_backward method.</div><div class="line">        """</div><div class="line">        self.save_for_backward(input)</div><div class="line">        <span class="keyword">return</span> input.clamp(min=<span class="number">0</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad_output)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        In the backward pass we receive a Tensor containing the gradient of the loss</div><div class="line">        with respect to the output, and we need to compute the gradient of the loss</div><div class="line">        with respect to the input.</div><div class="line">        """</div><div class="line">        <span class="comment"># 这里说明下ReLU在进行反向传播的时候对于是0的部分是没有梯度的。</span></div><div class="line">        input, = self.saved_tensors</div><div class="line">        grad_input = grad_output.clone()</div><div class="line">        grad_input[input &lt; <span class="number">0</span>] = <span class="number">0</span></div><div class="line">        <span class="keyword">return</span> grad_input</div><div class="line"></div><div class="line"></div><div class="line">dtype = torch.FloatTensor</div><div class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold input and outputs, and wrap them in Variables.</span></div><div class="line">x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=<span class="keyword">False</span>)</div><div class="line">y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors for weights, and wrap them in Variables.</span></div><div class="line">w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=<span class="keyword">True</span>)</div><div class="line">w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">learning_rate = <span class="number">1e-6</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Construct an instance of our MyReLU class to use in our network</span></div><div class="line">    relu = MyReLU()</div><div class="line"></div><div class="line">    <span class="comment"># Forward pass: compute predicted y using operations on Variables; we compute</span></div><div class="line">    <span class="comment"># ReLU using our custom autograd operation.</span></div><div class="line">    y_pred = relu(x.mm(w1)).mm(w2)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss</span></div><div class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Use autograd to compute the backward pass.</span></div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    <span class="comment"># Update weights using gradient descent</span></div><div class="line">    w1.data -= learning_rate * w1.grad.data</div><div class="line">    w2.data -= learning_rate * w2.grad.data</div><div class="line"></div><div class="line">    <span class="comment"># Manually zero the gradients after updating weights</span></div><div class="line">    w1.grad.data.zero_()</div><div class="line">    w2.grad.data.zero_()</div></pre></td></tr></table></figure>
<h3 id="TensorFlow-Static-Graphs"><a href="#TensorFlow-Static-Graphs" class="headerlink" title="TensorFlow:Static Graphs"></a>TensorFlow:Static Graphs</h3><p>PyTorch的autograd看起来和TensorFlow很相似：在这两个框架我们都定义计算图，然后使用自动微分技术计算梯度。两者计算图最大的区别是TenorFlow的计算图是<strong>静态</strong>的PyTorch是<strong>动图</strong>的计算图。</p>
<p>在TensorFlow中，我们定义好计算图后可以一边又一边的执行计算图，输入不同的输入数据给它。在PyTorch中，每个前向操作定义一个新的计算图。</p>
<p>静态图优势在于你可以先期进行图的优化；举个例讲一个框架可以决定有效的融合一些图操作，或者提出跨机器或者GPUs的分布式计算图策略。如果你一次次的重复使用一个计算图，那么这个潜在的前期图优化代价，将分散到每次的重复上(这段翻译的有点问题)。</p>
<p>一方面静态和动态的计算图结构差别还在控制流上。在一些模型中我们希望对每个数据点做梯度计算;比如在循环神经网络中一个数据点我们会按照时间步骤来展开；这些展开可以作为循环来计算。静态图需要将这个循环结构变成计算图的一部分；因此TensorFlow提供来一个<strong>tf.scan</strong>操作来将循环结构嵌入到计算图中。在这个情况下使用动态图处理上会更简单：因为我们是在运行时创建计算图，我们使用一般必要的流控方式来计算不同输入数据。</p>
<p>同上面的PyTorch自动梯度计算例子相比，下面是我们使用TensorFlow来实现一个简单的两层网络：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># First we set up the computational graph:</span></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create placeholders for the input and target data; these will be filled</span></div><div class="line"><span class="comment"># with real data when we execute the graph.</span></div><div class="line">x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, D_in))</div><div class="line">y = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, D_out))</div><div class="line"></div><div class="line"><span class="comment"># Create Variables for the weights and initialize them with random data.</span></div><div class="line"><span class="comment"># A TensorFlow Variable persists its value across executions of the graph.</span></div><div class="line">w1 = tf.Variable(tf.random_normal((D_in, H)))</div><div class="line">w2 = tf.Variable(tf.random_normal((H, D_out)))</div><div class="line"></div><div class="line"><span class="comment"># Forward pass: Compute the predicted y using operations on TensorFlow Tensors.</span></div><div class="line"><span class="comment"># Note that this code does not actually perform any numeric operations; it</span></div><div class="line"><span class="comment"># merely sets up the computational graph that we will later execute.</span></div><div class="line">h = tf.matmul(x, w1)</div><div class="line">h_relu = tf.maximum(h, tf.zeros(<span class="number">1</span>))</div><div class="line">y_pred = tf.matmul(h_relu, w2)</div><div class="line"></div><div class="line"><span class="comment"># Compute loss using operations on TensorFlow Tensors</span></div><div class="line">loss = tf.reduce_sum((y - y_pred) ** <span class="number">2.0</span>)</div><div class="line"></div><div class="line"><span class="comment"># Compute gradient of the loss with respect to w1 and w2.</span></div><div class="line">grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])</div><div class="line"></div><div class="line"><span class="comment"># Update the weights using gradient descent. To actually update the weights</span></div><div class="line"><span class="comment"># we need to evaluate new_w1 and new_w2 when executing the graph. Note that</span></div><div class="line"><span class="comment"># in TensorFlow the the act of updating the value of the weights is part of</span></div><div class="line"><span class="comment"># the computational graph; in PyTorch this happens outside the computational</span></div><div class="line"><span class="comment"># graph.</span></div><div class="line">learning_rate = <span class="number">1e-6</span></div><div class="line">new_w1 = w1.assign(w1 - learning_rate * grad_w1)</div><div class="line">new_w2 = w2.assign(w2 - learning_rate * grad_w2)</div><div class="line"></div><div class="line"><span class="comment"># Now we have built our computational graph, so we enter a TensorFlow session to</span></div><div class="line"><span class="comment"># actually execute the graph.</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    <span class="comment"># Run the graph once to initialize the Variables w1 and w2.</span></div><div class="line">    sess.run(tf.global_variables_initializer())</div><div class="line"></div><div class="line">    <span class="comment"># Create numpy arrays holding the actual data for the inputs x and targets</span></div><div class="line">    <span class="comment"># y</span></div><div class="line">    x_value = np.random.randn(N, D_in)</div><div class="line">    y_value = np.random.randn(N, D_out)</div><div class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">        <span class="comment"># Execute the graph many times. Each time it executes we want to bind</span></div><div class="line">        <span class="comment"># x_value to x and y_value to y, specified with the feed_dict argument.</span></div><div class="line">        <span class="comment"># Each time we execute the graph we want to compute the values for loss,</span></div><div class="line">        <span class="comment"># new_w1, and new_w2; the values of these Tensors are returned as numpy</span></div><div class="line">        <span class="comment"># arrays.</span></div><div class="line">        loss_value, _, _ = sess.run([loss, new_w1, new_w2],</div><div class="line">                                    feed_dict=&#123;x: x_value, y: y_value&#125;)</div><div class="line">        print(loss_value)</div></pre></td></tr></table></figure></p>
<h2 id="nn-module-神经网络模块"><a href="#nn-module-神经网络模块" class="headerlink" title="nn module(神经网络模块)"></a>nn module(神经网络模块)</h2><h3 id="PyTorch-nn"><a href="#PyTorch-nn" class="headerlink" title="PyTorch:nn"></a>PyTorch:nn</h3><p>计算图和自动梯度计算是非常有用的范式去定义复杂的操作和自动的执行导数计算；然而对于大型神经网络来说这样未加处理的自动梯度计算就显的太低层了。</p>
<p>当构建一个神经网络时，我们经常考虑在<strong>layer(层)</strong>中安排计算，这些层中有些包含了需要在学习过程中需要进行优化的<strong>leanable parameters(可学习参数)</strong>。</p>
<p>在TensorFlow中，提供了像<strong>Keras</strong>,<strong>TensorFlow-Slim</strong>,和<strong>TFLearn</strong>包他们对原始计算图方式进行了高层抽象，对于神经网络的构建非常有用。</p>
<p>在PyTorch，nn包为了相同的目的诞生。nn包定义了一个<strong>Modules</strong>集，它门大致等于神经网络的层。一个Module接受输入的Variable然后计算输出Variable，但也可以保存中间状态就像Variable包含可学习参数。nn包也定义了在训练神经网络时常用的损失函数集。</p>
<p>在这个例子中我们使用nn包去实现我们的两层神经网络：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables.</span></div><div class="line">x = Variable(torch.randn(N, D_in))</div><div class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Use the nn package to define our model as a sequence of layers. nn.Sequential</span></div><div class="line"><span class="comment"># is a Module which contains other Modules, and applies them in sequence to</span></div><div class="line"><span class="comment"># produce its output. Each Linear Module computes output from input using a</span></div><div class="line"><span class="comment"># linear function, and holds internal Variables for its weight and bias.</span></div><div class="line">model = torch.nn.Sequential(</div><div class="line">    torch.nn.Linear(D_in, H),</div><div class="line">    torch.nn.ReLU(),</div><div class="line">    torch.nn.Linear(H, D_out),</div><div class="line">)</div><div class="line"></div><div class="line"><span class="comment"># The nn package also contains definitions of popular loss functions; in this</span></div><div class="line"><span class="comment"># case we will use Mean Squared Error (MSE) as our loss function.</span></div><div class="line">loss_fn = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</div><div class="line"></div><div class="line">learning_rate = <span class="number">1e-4</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: compute predicted y by passing x to the model. Module objects</span></div><div class="line">    <span class="comment"># override the __call__ operator so you can call them like functions. When</span></div><div class="line">    <span class="comment"># doing so you pass a Variable of input data to the Module and it produces</span></div><div class="line">    <span class="comment"># a Variable of output data.</span></div><div class="line">    y_pred = model(x)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss. We pass Variables containing the predicted and true</span></div><div class="line">    <span class="comment"># values of y, and the loss function returns a Variable containing the</span></div><div class="line">    <span class="comment"># loss.</span></div><div class="line">    loss = loss_fn(y_pred, y)</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Zero the gradients before running the backward pass.</span></div><div class="line">    model.zero_grad()</div><div class="line"></div><div class="line">    <span class="comment"># Backward pass: compute gradient of the loss with respect to all the learnable</span></div><div class="line">    <span class="comment"># parameters of the model. Internally, the parameters of each Module are stored</span></div><div class="line">    <span class="comment"># in Variables with requires_grad=True, so this call will compute gradients for</span></div><div class="line">    <span class="comment"># all learnable parameters in the model.</span></div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    <span class="comment"># Update the weights using gradient descent. Each parameter is a Variable, so</span></div><div class="line">    <span class="comment"># we can access its data and gradients like we did before.</span></div><div class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</div><div class="line">        param.data -= learning_rate * param.grad.data</div></pre></td></tr></table></figure></p>
<h3 id="PyTorch-optim"><a href="#PyTorch-optim" class="headerlink" title="PyTorch:optim"></a>PyTorch:optim</h3><p>到目前为止我们通过手动的转换Vraiable保存的可学习参数<strong>.data</strong>来更新模型权重。这个并不是一个大的负担对于这个简单的随机梯度下降算法，但是在练习中我们通常使用更复杂的优化器比如AdaGrad，RMSProp，Adam等。</p>
<p>在PyTorch中optim包抽象了优化算法的想法并提供了通常使用的优化算法。</p>
<p>下面的例子中我们像先前一样使用nn包去定义我们的模型，但是我们使用optim包的Adam算法去优化我们的模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables.</span></div><div class="line">x = Variable(torch.randn(N, D_in))</div><div class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Use the nn package to define our model and loss function.</span></div><div class="line">model = torch.nn.Sequential(</div><div class="line">    torch.nn.Linear(D_in, H),</div><div class="line">    torch.nn.ReLU(),</div><div class="line">    torch.nn.Linear(H, D_out),</div><div class="line">)</div><div class="line">loss_fn = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Use the optim package to define an Optimizer that will update the weights of</span></div><div class="line"><span class="comment"># the model for us. Here we will use Adam; the optim package contains many other</span></div><div class="line"><span class="comment"># optimization algoriths. The first argument to the Adam constructor tells the</span></div><div class="line"><span class="comment"># optimizer which Variables it should update.</span></div><div class="line">learning_rate = <span class="number">1e-4</span></div><div class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: compute predicted y by passing x to the model.</span></div><div class="line">    y_pred = model(x)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss.</span></div><div class="line">    loss = loss_fn(y_pred, y)</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Before the backward pass, use the optimizer object to zero all of the</span></div><div class="line">    <span class="comment"># gradients for the variables it will update (which are the learnable weights</span></div><div class="line">    <span class="comment"># of the model)</span></div><div class="line">    optimizer.zero_grad()</div><div class="line"></div><div class="line">    <span class="comment"># Backward pass: compute gradient of the loss with respect to model</span></div><div class="line">    <span class="comment"># parameters</span></div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    <span class="comment"># Calling the step function on an Optimizer makes an update to its</span></div><div class="line">    <span class="comment"># parameters</span></div><div class="line">    optimizer.step()</div></pre></td></tr></table></figure>
<h3 id="PyTorch-Custom-nn-Modules"><a href="#PyTorch-Custom-nn-Modules" class="headerlink" title="PyTorch:Custom nn Modules"></a>PyTorch:Custom nn Modules</h3><p>有些时候你可能像实现一个比一系列已经存在的Modules更复杂的模型；基于此你可以通过继承nn.Module然后定义forward来接受输入的Variable，使用其它模块或者别的自动梯度计算方式来生成Variable。</p>
<p>在这个例子中我们实现了我们两层的神经网络作为一个定制的Module子类：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayerNet</span><span class="params">(torch.nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        In the constructor we instantiate two nn.Linear modules and assign them as</div><div class="line">        member variables.</div><div class="line">        """</div><div class="line">        super(TwoLayerNet, self).__init__()</div><div class="line">        self.linear1 = torch.nn.Linear(D_in, H)</div><div class="line">        self.linear2 = torch.nn.Linear(H, D_out)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        In the forward function we accept a Variable of input data and we must return</div><div class="line">        a Variable of output data. We can use Modules defined in the constructor as</div><div class="line">        well as arbitrary operators on Variables.</div><div class="line">        """</div><div class="line">        h_relu = self.linear1(x).clamp(min=<span class="number">0</span>)</div><div class="line">        y_pred = self.linear2(h_relu)</div><div class="line">        <span class="keyword">return</span> y_pred</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables</span></div><div class="line">x = Variable(torch.randn(N, D_in))</div><div class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Construct our model by instantiating the class defined above</span></div><div class="line">model = TwoLayerNet(D_in, H, D_out)</div><div class="line"></div><div class="line"><span class="comment"># Construct our loss function and an Optimizer. The call to model.parameters()</span></div><div class="line"><span class="comment"># in the SGD constructor will contain the learnable parameters of the two</span></div><div class="line"><span class="comment"># nn.Linear modules which are members of the model.</span></div><div class="line">criterion = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</div><div class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-4</span>)</div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: Compute predicted y by passing x to the model</span></div><div class="line">    y_pred = model(x)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss</span></div><div class="line">    loss = criterion(y_pred, y)</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Zero gradients, perform a backward pass, and update the weights.</span></div><div class="line">    optimizer.zero_grad()</div><div class="line">    loss.backward()</div><div class="line">    optimizer.step()</div></pre></td></tr></table></figure></p>
<h3 id="PyTorch-Control-Flow-Weight-Sharing"><a href="#PyTorch-Control-Flow-Weight-Sharing" class="headerlink" title="PyTorch:Control Flow + Weight Sharing"></a>PyTorch:Control Flow + Weight Sharing</h3><p>作为一个动态图和共享权重的例子，我们实现了一个非常奇怪的模型：一个ReLU作为激活函数的全连接层神经网络，它的每个前向传播选择0到4之间的一个随机数作为需要使用的中间层数，重复使用相同的权重多次去计算隐藏层(这段翻译的不好没有怎么理解)。</p>
<p>对这样的模型我们使用普通的Python流控去实现循环，然后在定义前向传播时在最深处的层通过简单的重复使用相同模块多次实现权重共享。</p>
<p>我们能轻易的实现这个模型通过继承Module。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DynamicNet</span><span class="params">(torch.nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        In the constructor we construct three nn.Linear instances that we will use</div><div class="line">        in the forward pass.</div><div class="line">        """</div><div class="line">        super(DynamicNet, self).__init__()</div><div class="line">        self.input_linear = torch.nn.Linear(D_in, H)</div><div class="line">        self.middle_linear = torch.nn.Linear(H, H)</div><div class="line">        self.output_linear = torch.nn.Linear(H, D_out)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3</div><div class="line">        and reuse the middle_linear Module that many times to compute hidden layer</div><div class="line">        representations.</div><div class="line"></div><div class="line">        Since each forward pass builds a dynamic computation graph, we can use normal</div><div class="line">        Python control-flow operators like loops or conditional statements when</div><div class="line">        defining the forward pass of the model.</div><div class="line"></div><div class="line">        Here we also see that it is perfectly safe to reuse the same Module many</div><div class="line">        times when defining a computational graph. This is a big improvement from Lua</div><div class="line">        Torch, where each Module could be used only once.</div><div class="line">        """</div><div class="line">        h_relu = self.input_linear(x).clamp(min=<span class="number">0</span>)</div><div class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(random.randint(<span class="number">0</span>, <span class="number">3</span>)):</div><div class="line">            h_relu = self.middle_linear(h_relu).clamp(min=<span class="number">0</span>)</div><div class="line">        y_pred = self.output_linear(h_relu)</div><div class="line">        <span class="keyword">return</span> y_pred</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables</span></div><div class="line">x = Variable(torch.randn(N, D_in))</div><div class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Construct our model by instantiating the class defined above</span></div><div class="line">model = DynamicNet(D_in, H, D_out)</div><div class="line"></div><div class="line"><span class="comment"># Construct our loss function and an Optimizer. Training this strange model with</span></div><div class="line"><span class="comment"># vanilla stochastic gradient descent is tough, so we use momentum</span></div><div class="line">criterion = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</div><div class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-4</span>, momentum=<span class="number">0.9</span>)</div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: Compute predicted y by passing x to the model</span></div><div class="line">    y_pred = model(x)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss</span></div><div class="line">    loss = criterion(y_pred, y)</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Zero gradients, perform a backward pass, and update the weights.</span></div><div class="line">    optimizer.zero_grad()</div><div class="line">    loss.backward()</div><div class="line">    optimizer.step()</div></pre></td></tr></table></figure>]]></content>
      
        <categories>
            
            <category> frame </category>
            
            <category> translate </category>
            
        </categories>
        
        
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[pytorch学习笔记]]></title>
      <url>/2017/05/18/pytorch/</url>
      <content type="html"><![CDATA[<h2 id="pytorch基本操作"><a href="#pytorch基本操作" class="headerlink" title="pytorch基本操作"></a>pytorch基本操作</h2><p>在pytorch中会使用到Tensor张量，它可以用来表示多维数组并且如果存在GPU的话可以进行利用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch   </div><div class="line">x = torch.Tensor(<span class="number">5</span>, <span class="number">3</span>) <span class="comment">#构造一个未初始化的矩阵，注意是未初始化的矩阵.    </span></div><div class="line"><span class="comment">#矩阵加法</span></div><div class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>) <span class="comment">#构造一个使用随机数初始化的矩阵，注意返回的这个矩阵是Tensor</span></div><div class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</div><div class="line"><span class="comment">#以下两种方式都可以</span></div><div class="line">x + y</div><div class="line">torch.add(x, y)</div><div class="line"><span class="comment">#如果需要输出tensor</span></div><div class="line">result = torch.Tensor(<span class="number">5</span>, <span class="number">3</span>)</div><div class="line">torch.add(x, y, out=result)</div><div class="line"><span class="comment">#这种方法就会直接将y和x相加后的值更新到y中</span></div><div class="line"><span class="comment">#其中带有“_”的运算上的操作都带有相似的含义</span></div><div class="line">y.add_(x)</div></pre></td></tr></table></figure>
<a id="more"></a>
<p><em>注意：如果是维数相同的矩阵相加那么就是矩阵对应元素相加，如果是矩阵和标量相加那么就是矩阵每个元素都和这个标量相加。</em></p>
<table>
<thead>
<tr>
<th style="text-align:left">操作</th>
<th style="text-align:left">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">x+y</td>
<td style="text-align:left">相加，需要定义另外的变量接受返回值</td>
</tr>
<tr>
<td style="text-align:left">torch_add(x, y)</td>
<td style="text-align:left">含义同上</td>
</tr>
<tr>
<td style="text-align:left">x.add_(y)</td>
<td style="text-align:left">将y加到x上并更新x的值</td>
</tr>
<tr>
<td style="text-align:left">torch.add(x, y, out=result)</td>
<td style="text-align:left">相加后的值更新到自定义的Tensor变量result</td>
</tr>
</tbody>
</table>
<h3 id="Tensor与array转换"><a href="#Tensor与array转换" class="headerlink" title="Tensor与array转换"></a>Tensor与array转换</h3><p>在Torch中进行tensor和numpy的array的转换非常方便。转换后两者会共享同一块存储空间，意思是<strong>一个发生改变另外一个也会改变</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="comment">#生成一个tensor矩阵a</span></div><div class="line">a = torch.ones(<span class="number">5</span>)</div><div class="line"><span class="comment"># 利用Tensor对象a的numpy()方法，完成tensor到numpy的转换</span></div><div class="line">b = a.numpy()</div><div class="line"><span class="comment">#生成一个numpy矩阵c</span></div><div class="line">c = np.ones(<span class="number">5</span>)</div><div class="line"><span class="comment"># 完成从numpy array到tensor的转换</span></div><div class="line">d = torch.from_numpy(c)</div></pre></td></tr></table></figure>
<p><em>注意：使用x.cuda()可以将tensor移动到GPU上进行运算，这个是说得通过这样才能保障这个部分在GPU上进行运算吗？</em></p>
<h2 id="Autograd-自动求导"><a href="#Autograd-自动求导" class="headerlink" title="Autograd(自动求导)"></a>Autograd(自动求导)</h2><p>在pytorch中提供了一个自动求导的功能，这个功能依赖于torch中的autograd.Variable这个类。一般通过一个tensor和requires_grad的表示是否需要计算梯度的标志来实例化。Variable这个的内部简单看包括了data对应传入的tensor，grad对应计算得到的梯度，creator对应这个Variable的创建操作，如果是开发人员自定义的话这个Creator的值为None。</p>
<p><img src="/image/Variable.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line">x = Variable(torch.ones(<span class="number">2</span>, <span class="number">2</span>), requires_grad=<span class="keyword">True</span>)</div><div class="line">print(x.creator)</div><div class="line">&gt;<span class="keyword">None</span></div><div class="line">y = x + <span class="number">2</span></div><div class="line">print(y.creator)</div><div class="line"><span class="comment"># 在经过一个x+2这个操作得到的Variable变量y，y的creator属性表示它是通过一个加操作得到。</span></div><div class="line">&gt;&lt;torch.autograd._functions.basic_ops.AddConstant object at <span class="number">0x104ba7908</span>&gt;</div></pre></td></tr></table></figure>
<p>通过Variable中的data属性和creator属性就可以构建出一个计算图出来。从一个creator为None的地方输入，根据creator来衔接最后得到输出结果。以下绘制了一个简略图以说明<br><img src="/image/autograd.png"><br>在定义了Variable并围绕它进行一系列的操作后得到z，然后通过z.backward()调用次方法进行反向传播自动得到x的梯度，通过x.grad属性就可以获取到。这个注意到creator不为None的对象在反向传播过程中不会返回梯度。<br><em>注意：这里通过z=Mean(Y)得到是一个标量，也就是一些列的计算最后的输出只有一个那么进行backward()时就默认从这里开始，但是如果最后得到的结果有多个元素组成，那么就需要指定一个和输出结果tensor匹配的grad_output参数backward(grad_output)</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line">A = torch.ones(<span class="number">4</span>)</div><div class="line">x = Variable(A, requires_grad=<span class="keyword">True</span>)</div><div class="line">y = x + <span class="number">2</span></div><div class="line">gradients = torch.FloatTensor([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>])</div><div class="line">y.backward(gradients)</div></pre></td></tr></table></figure></p>
<p>另外这个grad_output参数还有个作用举个<a href="http://quabr.com/43451125/pytorch-what-are-the-gradient-arguments" target="_blank" rel="external">例子</a>如果输入的数据x，然后得到了一个中间结果y=[y1, y2, y3]，中间结果被使用来计算最终输出结果z那么：<br>$\frac{\partial z}{\partial x}=\frac{\partial z}{\partial y1}\frac{\partial y1}{\partial x}+\frac{\partial z}{\partial y2}\frac{\partial y2}{\partial x}+\frac{\partial z}{\partial y3}\frac{\partial y3}{\partial x}$这样在grad_output=[$\frac{\partial z}{\partial y1}$, $\frac{\partial z}{\partial y2}$, $\frac{\partial z}{\partial y3}$]再y.backward(grad_output)就可以得到最终的$\frac{\partial z}{\partial x}$了。</p>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>接下来是通过pytorch来实现一个简易卷积神经网络来对minist手写数字进行分类。对于卷积神经网络可以通过<a href="http://www.memorycrash.cn/2017/05/10/cnnNote/" target="_blank" rel="external">前面的文章</a>进行了解。<br><img src="/image/pytorchCnn.png"><br>对于一个卷积神经网络一般有以下模块组成：</p>
<ul>
<li>输入数据集合</li>
<li>若干卷积层+激活层+池化层</li>
<li>全连接层</li>
<li>softmax输出层</li>
</ul>
<p>对于一个卷积神经网络的训练一般是这样：</p>
<ol>
<li>forward前向传播，最后通过交叉熵代价函数得到代价也就是预测值和期望值之间的差距</li>
<li>backward反向传播，将误差信息往会传递获得神经元间连接的权值对应的偏导数</li>
<li>基于SGD(随机梯度下降),根据这些得到的偏导数刷新这些权值(weight=weight-learning_rate*gradient)，返回1如此循环以期望降低代价</li>
</ol>
<p>接下来定义一个卷积神经网络<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(Net, self).__init__() <span class="comment"># 调用Net的父类的__init__()函数进行初始化</span></div><div class="line">        <span class="comment"># 虽然我们使用的是SGD但是在实际中的SGD每次输入的并不是单个样本，而是一个小批次的样本</span></div><div class="line">        <span class="comment"># 1 图片输入通道，6个输出通道(这个6个可以理解为一个本卷积层有6个核)，5x5的感受野</span></div><div class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</div><div class="line">        <span class="comment"># 第2类的卷积层承接上一层的信息所以有6个输入通道，同时有16个核，5x5的感受野</span></div><div class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</div><div class="line">        <span class="comment"># 全连接层接受卷积层的数据y=Wx+b</span></div><div class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</div><div class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</div><div class="line">        <span class="comment"># 最后的输出是10个节点因为要识别0到9的数字</span></div><div class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="comment"># 使用最大池化的池化层，每次池化的窗口是2x2。这里先通过卷积层进行卷积计算后通过</span></div><div class="line">        <span class="comment"># relu来作为激活层进行激活后通过最大池化层池化(采样)</span></div><div class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</div><div class="line">        <span class="comment"># 如果池化的窗口是一个方阵可以只指定一个数字</span></div><div class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</div><div class="line">        <span class="comment"># torch 的 view 类似 numpy 的reshape用来调整形状，指定某个位置为-1表示将这个位置</span></div><div class="line">        <span class="comment"># 应该填写的内容交给系统自行计算。这里是为了方便输入数据到全连接层将调整后列数设置为</span></div><div class="line">        <span class="comment"># 16*5*5的大小，行数由系统计算得到</span></div><div class="line">        x = x.view(<span class="number">-1</span>, self.num_flat_features(x))</div><div class="line">        <span class="comment"># 使用relu作为激活函数的全连接层</span></div><div class="line">        x = F.relu(self.fc1(x))</div><div class="line">        x = F.relu(self.fc2(x))</div><div class="line">        x = self.fc3(x)</div><div class="line">        <span class="keyword">return</span> x</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="comment"># x[TensorSamples,nChannels,Height,Width]</span></div><div class="line">        <span class="comment"># x.size()[1:]得到x[nChannels,Height,Width]</span></div><div class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># 获取除开批次后的其它维度</span></div><div class="line">        num_features = <span class="number">1</span></div><div class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</div><div class="line">            num_features *= s</div><div class="line">        <span class="keyword">return</span> num_features</div><div class="line">        </div><div class="line">net = Net()</div><div class="line">print(net)</div></pre></td></tr></table></figure></p>
<h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h3><p>这里考虑用来计算模型预测值和期望值之间度量的代价函数，nn.MSELoss是一个均方误差代价函数$E=\frac{1}{N}\sum_{i}(\hat{y_i}-y_i)^2$还有需要注意的是我们需要训练的参数可以通过net.parameters()获得，得到的，然后转换成list后可以按照定义的顺序来获取到训练参数。下面的链式顺序表示从输入到获得loss的过程。</p>
<blockquote>
<p>input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d<br>     -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear<br>     -&gt; MSELoss<br>     -&gt; loss  </p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"><span class="comment"># 以下代码只是举例如何使用代价函数</span></div><div class="line"><span class="comment"># 手动构造一个测试数据</span></div><div class="line">input = Variable(torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>))</div><div class="line"><span class="comment"># 使用数据初始化一个神经网络</span></div><div class="line">output = net(input)</div><div class="line"><span class="comment"># 手动初始化一个目标值</span></div><div class="line">target = Variable(torch.range(<span class="number">1</span>, <span class="number">10</span>))  </div><div class="line"><span class="comment"># 定义一个均方误差代价函数</span></div><div class="line">criterion = nn.MSELoss()</div><div class="line"><span class="comment"># 获得代价值</span></div><div class="line">loss = criterion(output, target)</div><div class="line">loss.backward()</div></pre></td></tr></table></figure>
<h3 id="更新权值"><a href="#更新权值" class="headerlink" title="更新权值"></a>更新权值</h3><p>这里更新权值的方式是调用已经封装好的函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch, optim <span class="keyword">as</span> optim</div><div class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</div><div class="line"></div><div class="line"> <span class="comment">#选择我们进行优化的方式SGD随机梯度下降，传入需要处理的参数已经学习率</span></div><div class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</div><div class="line"></div><div class="line"> <span class="comment">#构建一个训练的循环</span></div><div class="line">optimizer.zero_grad()   <span class="comment"># 将梯度先值0，因为还没有开始进行训练</span></div><div class="line">output = net(input)</div><div class="line">loss = criterion(output, target)</div><div class="line">loss.backward()</div><div class="line">optimizer.step()    <span class="comment"># 一个循环中完成一次权值的更新</span></div></pre></td></tr></table></figure></p>
<h2 id="关于训练数据"><a href="#关于训练数据" class="headerlink" title="关于训练数据"></a>关于训练数据</h2><p>对于训练数据来说主要分为图片数据，语言数据以及文本信息。处理方式一般是先通过某些包将其加载到numpy的数组中，再转换为tensor。</p>
<ul>
<li>处理图像比较好的包是Pillow，OpenCV</li>
<li>处理语音比较好的包是sicpy，librosa</li>
<li>处理文本一般python的加载功能也可以</li>
</ul>
<p>对于图像来说，torchvision这个包中可以加载常用的图像数据，比如Imagenet(数据量大)，CIFAR10(有十种分类)，MNIST(简单的手写0到9数字灰度图)接下来的练习会选择CIFAR10数据集包含了这些分类:‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’。3X32X32是3个颜色通道大小32X32的图片。</p>
<p><img src="/image/cifar10.png"><br>接下来的代码基本和上面已经讲过的一致只是多了使用torchvision来加载数据处理数据的过程和训练模型已经验证模型。以下代码注释内容引用了<a href="www.jianshu.com/p/8da9b24b2fb6">这篇文章</a>。图像预处理可以参考<a href="https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit" target="_blank" rel="external">cs231n的课堂笔记</a></p>
<blockquote>
<p>ToTensor是指把PIL.Image(RGB)或者numpy.ndarray(H x W x C)从0到255的值映射到0到1的范围内并转化为Tensor格式(C x H x W)这里表示channel的C位置发生了改变<br>Normalize(mean, std)是通过下面的公式实现数据的归一化提供一个表示RGB标准差(R,G,B)和表示均值的(R,G,B)，mean是均值std是标准差$ std=\sqrt{ \frac{1}{N}\sum_{i=1}^{N}(x_i-mean)^2}$<br>channel=(channel-mean)/std</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torchvision</div><div class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</div><div class="line"><span class="comment"># transform可以对数据进行预处理,compose函数是去组合需要进行预处理的项目，比如归一化等</span></div><div class="line"><span class="comment"># 经过下面处理的数据取值变为[-1,1]</span></div><div class="line">transform = transforms.Compose([transforms.ToTensor(),</div><div class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</div><div class="line"><span class="comment"># 表示加载训练数据 train=True</span></div><div class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="keyword">True</span>,</div><div class="line">                                        download=<span class="keyword">True</span>, transform=transform)</div><div class="line"><span class="comment"># 将训练数据50000张图片划分为12500份，每份4张。shffule=True表示不同批次的数据遍历需打乱顺序</span></div><div class="line"><span class="comment"># num_workers表示使用两个子进程来加载数据                                    </span></div><div class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>,</div><div class="line">                                          shuffle=<span class="keyword">True</span>, num_workers=<span class="number">2</span>)</div><div class="line"><span class="comment"># 表示加载验证数据 train=False</span></div><div class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="keyword">False</span>,</div><div class="line">                                       download=<span class="keyword">True</span>, transform=transform)</div><div class="line">                                       </div><div class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>,</div><div class="line">                                         shuffle=<span class="keyword">False</span>, num_workers=<span class="number">2</span>)</div><div class="line"></div><div class="line">classes = (<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>,<span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</div></pre></td></tr></table></figure>
<p>momentum是梯度下降法中一种常用的加速技术。对于一般的SGD，其表达式为$x=x-\alpha*dx$,沿负梯度方向下降。而带momentum项的SGD则写生如下形式$v=\beta*v - \alpha*dx;x=x+v$：其中即momentum系数，通俗的理解上面式子就是，如果上一次的momentum（即）与这一次的负梯度方向是相同的，那这次下降的幅度就会加大，所以这样做能够达到加速收敛的过程。下面是这次用到的卷积神经网络代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(Net, self).__init__()</div><div class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</div><div class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</div><div class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</div><div class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</div><div class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</div><div class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = self.pool(F.relu(self.conv1(x)))</div><div class="line">        x = self.pool(F.relu(self.conv2(x)))</div><div class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</div><div class="line">        x = F.relu(self.fc1(x))</div><div class="line">        x = F.relu(self.fc2(x))</div><div class="line">        x = self.fc3(x)</div><div class="line">        <span class="keyword">return</span> x</div><div class="line"></div><div class="line"></div><div class="line">net = Net()</div><div class="line">criterion = nn.CrossEntropyLoss()</div><div class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</div><div class="line"></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">2</span>):  <span class="comment"># loop over the dataset multiple times</span></div><div class="line"></div><div class="line">    running_loss = <span class="number">0.0</span></div><div class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</div><div class="line">        <span class="comment"># 获取输入数据</span></div><div class="line">        inputs, labels = data</div><div class="line"></div><div class="line">        <span class="comment"># 转换输入数据以及标签为Variable</span></div><div class="line">        inputs, labels = Variable(inputs), Variable(labels)</div><div class="line"></div><div class="line">        <span class="comment"># 重置梯度为0</span></div><div class="line">        optimizer.zero_grad()</div><div class="line"></div><div class="line">        <span class="comment"># 前向 + 后向 + SGD更新</span></div><div class="line">        outputs = net(inputs)</div><div class="line">        loss = criterion(outputs, labels)</div><div class="line">        loss.backward()</div><div class="line">        optimizer.step()</div><div class="line"></div><div class="line">        <span class="comment"># print statistics</span></div><div class="line">        running_loss += loss.data[<span class="number">0</span>]</div><div class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:    <span class="comment"># print every 2000 mini-batches</span></div><div class="line">            print(<span class="string">'[%d, %5d] loss: %.3f'</span> %</div><div class="line">                  (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</div><div class="line">            running_loss = <span class="number">0.0</span></div><div class="line"></div><div class="line">print(<span class="string">'Finished Training'</span>)</div></pre></td></tr></table></figure></p>
<p>参考：<br>[1]<a href="http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="external">Deep Learning with PyTorch: A 60 Minute Blitz</a><br>[2]<a href="https://zhuanlan.zhihu.com/p/25572330" target="_blank" rel="external">PyTorch深度学习：60分钟入门(Translation)</a></p>
]]></content>
      
        <categories>
            
            <category> frame </category>
            
        </categories>
        
        
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[循环神经网络简略笔记]]></title>
      <url>/2017/05/11/rnnNote/</url>
      <content type="html"><![CDATA[<h2 id="循环神经网络和卷积神经网络"><a href="#循环神经网络和卷积神经网络" class="headerlink" title="循环神经网络和卷积神经网络"></a>循环神经网络和卷积神经网络</h2><p>我们可以使用卷积神经网络来处理图片的分类，从目前看其包含的特点是输入到卷积层的图片是大小是固定的，待分类的图片与图片之间是相对独立的存在。通过对目前我们了解到的卷积神经网络的描述可以了解到它的模型处理的场景和我们的现实生活还有有比较大的差异。</p>
<a id="more"></a>
<p>现实的生活中我们做出来的判断可能不能单靠一张图片来进行，比如我们需要和其它人交流了解到他说的整段话以后才能明白含义，看一篇文章，看一部视频也是一样的道理。我们因为可以记下交流中过去时刻的信息，最后基于这些记忆的内容来做出判断。</p>
<p>了解到这些后，有必要再分析下卷积神经网络的模型。在卷积神经网络中神经元模型是一个接受输入再输入的形式，可以看出在这样的模型中并不能记忆过去时刻的信息。而接下来的循环神经网络的部分层的神经元模型实现了以往信息对当前状态影响结果的记录。</p>
<h2 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h2><p>循环神经网络的基本机构如下图<br><img src="/image/rnnStru.png"><br>第一次看到这个图可能会感到很困惑，可以暂时不看w和它所在的圆形箭头那这个图便是一个常见的神经元模型，x作为输入进行权值矩阵U后输入到s，然后s通过激活函数激活后输出，输出后的值同样经过一个权值矩阵后输入到o。这里我们暂时忽略偏移bias，这不影响理解。</p>
<p>现在我们来考虑w的存在，假设某一个t时刻，从x输入了一个数据，它经过了权值矩阵后进入s，然后同样是经过激活函数的激活。接着输出的数据会通过v传递下去，同时会经过w这个权值矩阵处理后保存下来。等到t+1时刻有数据从x传入的时候，s就会吸收新传入的数据和上一时刻通过w处理后的信息了。这样就构造了一个可以保存记忆基于记忆来进行处理的神经元模型。</p>
<p>如上所述，在不同时刻数据通过这个结构都在影响着输入，下面按照时间将展开这个结构。<br><img src="/image/rnnUnflod.jpg" width="70%" height="70%"></p>
<ul>
<li>首先图中的W，V，U都是同一个，只是在时间维度上展开了。</li>
<li>从左向右传递上一个结点状态s会传递给下一个结点。</li>
<li>这里o在每个时刻都会依据当前的状态进行判断病输出。</li>
</ul>
<p>这里我们看看展开后的循环神经网络结构。这里的x可以理解为代表输入层，W代表循环层的权值矩阵，o表示最终的输出层。一般会使用sigmoid,tanh或者relu作为激活函数，循环层中我们使用tanh作为激活函数，最终使用softmax作为输出层。<br>$ o_{t}=g(Vs_{t}) $<br>$ s_{t}=f(Ux_{t}+Ws_{t-1}) $</p>
<h2 id="循环神经网络实现"><a href="#循环神经网络实现" class="headerlink" title="循环神经网络实现"></a>循环神经网络实现</h2><p><img src="/image/rnnLayer.png"><br>注意到这里多一个$ h_{t} $符号，它表示t时刻隐藏层的输入$ h_{t}=Ux_{t}+Ws_{t-1} $。还有$ \widehat{y}_{t} $表示t时候模型预测的最终输出值。如果我们要实现的是一个三层结构的循环神经网络并使用它来进行句子中词语的预测。并希望最终达到这样的效果。<br><img src="/image/rnnWords.png"><br>这里的s和e代表一句话的开始和结束。上图展示出来的情况表达的含义是我在每个时间点输入一个句子中的单词然后模型会预测出我下一个单词是什么，比如在t2时刻我输入了x2(我)，这个时候我期望得到的单词是“昨天”。这里举例的这个场景是rnn应用的一个小点，其它还包括机器翻译等。下面基于预测单词的要求来看看完成循环神经网络是什么样的。</p>
<h3 id="输入数据"><a href="#输入数据" class="headerlink" title="输入数据"></a>输入数据</h3><p>既然是预测单词，那我们的输入也是单词的内容。这里以一个句子作为一条训练样本，以句子中的单词作为具体的输入信息。可以看做单词是句子这个训练样本中的元素。按照上面的说法我们还需要给句子标注出开始和结束标志。我们把句子中的没有包含结束标志的部分可以当作是输入数据，把句子中没有包含开始标志的部分看做是验证模型预测输出数据是否正确的标准。因为我们并不能直接把一个单词作为输入或者输出。所以我们需要建立一个词汇表，就像是字典一样，在词汇表中将位置信息和具体的词汇进行映射。将映射后的信息比如[0 0 0 0 1 0 0]就代表“我”，传入到模型中去。</p>
<h3 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h3><p>在隐藏层中我们包含了一个循环，处于隐藏层的神经元接受当前时候的输入$ x_{t} $和上一个时刻的神经元的状态$ s_{t-1} $作为本次的输入，然后经过tanh激活函数进行激活后输出，同时也作为下一个时刻神经元的输入$ s_{t} $。</p>
<p>$ tanh(z) = \frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$<br>$ h_{t}=Ux_{t}+Ws_{t-1} $<br>$ s_{t}=tanh(Ux_{t}+Ws_{t-1}) $</p>
<p>tanh函数的图像是这样，和sigmoid很类似，但是tanh是关于0旋转对称的，它的取值范围是[-1,1]而sigmoid的取值范围是[0,1]。sigmoid的导数是$ {f}’(z)=f(z)(1-f(z)) $而tanh的导数$ {f}’(z)=1-(f(z))^{2} $</p>
<p><img src="/image/tanh.png"></p>
<h3 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h3><p>输出层的输入信息是从隐藏层来的，隐藏层输出的状态通过权值矩阵V处理以后输入到输出层softmax，这里V的主要作用是将隐藏层输出的矩阵信息转换为输出层可以处理的向量信息信息，因为输出层输出的是向量。softmax的公式是<br>$$ \sigma (z)_{j}=\frac{e^{z_{j}}}{\sum_{k=1}^{K}e^{z_{k}}} $$<br>可以看出通过softmax层输出后的是模型认为在这个位置应该出现的单词的概率。也就是说softmax输出的数据中值最大的那个就是模型预测的值。</p>
<h2 id="循环神经网络的训练"><a href="#循环神经网络的训练" class="headerlink" title="循环神经网络的训练"></a>循环神经网络的训练</h2><p>在训练循环神经网络的时候我们使用的是bptt(Backprogation Through Time)算法，基本思想和bp算法是一致的。bptt多考虑了在时间维度上进行循环的影响。一般来说训练一个神经网络我们先获取到经过模型的预测值和实际值之间的差异，这个过程属于一个前向传播的过程(这个差异就是代价函数我们这里使用交叉熵作为代价函数)。然后对预测值和实际值的差异从减小差异的方向来调整各个层的权值，这个过程属于一个后向传播的过程。之后重复这个过程直到模型预测结果复合要求。</p>
<p>后续的进行bptt算法的推导可以参考<a href="http://www.cnblogs.com/wacc/p/5341670.html" target="_blank" rel="external">此篇文档</a>这里只简单描述下，如上所述的</p>
<blockquote>
<p>“处于隐藏层的神经元接受当前时候的输入$ x_{t} $和上一个时刻的神经元的状态$ s_{t-1} $作为本次的输入”</p>
</blockquote>
<p>所以对于反向传播也会存在两个方向的传播，其实也好理解因为有两个方向的传播才能兼顾到U和W两个权值矩阵并对他们进行更新。而一般的非循环神经网络只有一个方向的传递路径。<br><img src="/image/rnnBptt.jpg"></p>
<ul>
<li>红色方向涉及权值矩阵U，表示将梯度传递到上一层网络(从隐藏层的角度看)</li>
<li>绿色方向涉及到权值矩阵W，表示将梯度沿着时间方向传递</li>
</ul>
<p>推导bptt公式还可以参考<a href="https://zybuluo.com/hanbingtao/note/541458" target="_blank" rel="external">此篇文档</a>,这里直接给出来(这里我感觉这个文章的推导中只计算了t时刻的梯度)。在时间维度展开以后对于代价函数我们可以表示为$ E=\sum_{t=0}^{T}E_{t} $我们需要获取的是E对V，W，U的求导结果。那根据上面的公式可以的得到。<br>$ \frac{\partial E}{\partial V}=\sum_{t=0}^{T}\frac{\partial E_{t}}{\partial V} $<br>$ \frac{\partial E}{\partial W}=\sum_{t=0}^{T}\frac{\partial E_{t}}{\partial W} $<br>$ \frac{\partial E}{\partial U}=\sum_{t=0}^{T}\frac{\partial E_{t}}{\partial U} $<br>下面是基于上图和这三个式子的进一步推导。<br>$ \frac{\partial E_{t}}{\partial V}=(\hat{y_{t}}-y_{t})\otimes h_{t} $<br>$ \frac{\partial E}{\partial V}=\sum_{t=0}^{T}(\hat{y_{t}}-y_{t})\otimes h_{t} $<br>以上表示的是权值矩阵V的导数计算公式，这里需要注意$ \frac{\partial E_{t}}{\partial V} $只是和当前的t时刻有关系。这个可以上面的图中看出来对于每个当前时刻的代价函数在将反向传播的时候只会经过对V这个权值矩阵只会经过一次。<br>$ \delta_{k}=(U^{T}\delta_{k+1})\odot (1-h_{k}\odot h_{k}) $<br>$ \delta_{t}=(V^{T}(\hat{y_{t}}-y_{t}))\odot (1-h_{t}\odot h_{t}) $<br>以上两个公式中$\delta_{k}$表示的是中间节点的误差(隐藏层到隐藏层或者隐藏层到输入层)，但是在传递误差的开始节点$\delta_{t}$和$\delta_{k}$是相等的。<br>$ \frac{\partial E_{t}}{\partial W}=\sum_{k=0}^{t}\delta_{k}\otimes h_{k-1} $<br>$ \frac{\partial E}{\partial W}=\sum_{t=0}^{T}\sum_{k=0}^{t}\delta_{k}\otimes h_{k-1} $<br>以上公式表示隐藏层到隐藏层在时间维度展开后计算导数。这里我们根据上图可以看出来$ \frac{\partial E_{t}}{\partial W} $对于$E_{t}$不单单收到当前t时刻的影响，也受到t-1时刻及其以前的时刻的影响，所以我们会把t时刻的起始误差循环的往过去传递以此来获取过去的时刻的梯度，因为W在各个时刻都是共享的。所以对于$E_{t}$来说它的起始误差对W的影响就是把从t开始往前时刻的影响都加起来。从这里的描述可以看出来这个也是当前t时刻的可以使用以前时刻的记忆的来源，因为依然受到了以前时刻的影响。不过根据梯度消失的情况我们越早时间的点对于当前t时刻的影响就越小，获取话说就是记不清太久以前的事情。<br>$ \frac{\partial E_{t}}{\partial U}=\sum_{k=0}^{t}\delta_{k}\otimes x_{k} $<br>$ \frac{\partial E}{\partial U}=\sum_{t=0}^{T}\sum_{k=0}^{t}\delta_{k}\otimes x_{k} $<br>以上公式是表示隐藏层到输入层在时间维度展开后计算导数。误差的传递根据上面的图可以看出来是会往U的方向传递的。因为循环层不仅受到前一个状态的影响也搜到各个时刻的当前输入影响。</p>
<h3 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h3><p>梯度消失在循环神经网络中更容易出现，我们在进行反向传播的时候，比如计算当前时刻t对于W的梯度，本质上对于W的影响是存在于过往的每一个时刻的。比如我们去理解一个句子的信息，在当前t时刻获取的字符要理解其含义可能需要联系下先前任意时刻的信息。但是在我们刚才的描述中也会发现对于t-n时刻，这个t-n的梯度就会比t-n+1时刻小，最终在某个时刻可能就没有梯度了也就是完全遗忘了以前的记忆。对于当前也就失去了影响。</p>
<h3 id="numpy矩阵操作"><a href="#numpy矩阵操作" class="headerlink" title="numpy矩阵操作"></a>numpy矩阵操作</h3><p>这里简单记录下numpy部分矩阵操作。其中A.dot(B)表示A和B进行矩阵乘法，A*B在numpy表示A和B的对应元素相乘，np.outer(A,B)表示A和B进行外积计算，np.dot(A,B)表示A和B进行点积计算</p>
<table>
<thead>
<tr>
<th>numpy操作</th>
<th style="text-align:left">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>A.dot(B)</td>
<td style="text-align:left">A和B进行矩阵乘法</td>
</tr>
<tr>
<td>np.dot(A,B)</td>
<td style="text-align:left">A和B进行矩阵乘法</td>
</tr>
<tr>
<td>A*B</td>
<td style="text-align:left">A和B的对应元素相乘</td>
</tr>
<tr>
<td>np.multiply(U3, U4)</td>
<td style="text-align:left">A和B的对应元素相乘</td>
</tr>
<tr>
<td>np.outer(A,B)</td>
<td style="text-align:left">A和B进行外积计算</td>
</tr>
<tr>
<td>A.T</td>
<td style="text-align:left">A矩阵的转置</td>
</tr>
</tbody>
</table>
<h2 id="长短期记忆"><a href="#长短期记忆" class="headerlink" title="长短期记忆"></a>长短期记忆</h2><p>长短期记忆 Long Short Term Memory(LSTM)可以很好的解决上面提到的循环神经网络中梯度消失问题。LSTM将原来直接操作输入的数据和上一个时刻状态的方式修改为，通过三个门来控制输入数据和上一个时刻的状态的信息。这三个门分别是<strong>遗忘门(forget gate)</strong>，<strong>输入门(input gate)</strong>，<strong>输出门(output gate)</strong>。</p>
<p>LSTM部分详细解释可以参考colah的博客<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM Networks</a>及其<a href="https://www.yunaitong.cn/understanding-lstm-networks.html" target="_blank" rel="external">中文翻译</a></p>
<p>以及这篇文章<a href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/" target="_blank" rel="external">WILDML</a>及其<a href="https://zhuanlan.zhihu.com/p/22371429" target="_blank" rel="external">中文翻译</a></p>
<p>对于LSTM的梯度推导过程可以看下<a href="https://zybuluo.com/hanbingtao/note/581764" target="_blank" rel="external">零基础入门深度学习(6) - 长短时记忆网络(LSTM)</a></p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> rnn </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[卷积神经网络简略笔记]]></title>
      <url>/2017/05/10/cnnNote/</url>
      <content type="html"><![CDATA[<h2 id="卷积神经网络与神经网络"><a href="#卷积神经网络与神经网络" class="headerlink" title="卷积神经网络与神经网络"></a>卷积神经网络与神经网络</h2><p>神经网络和卷积神经网络的关系可以理解过卷积神经网络是为了可以更好处理图片而对神经网络进行了相应的改造和特殊化。这里以卷积来为这个新的神经网络进行命名，可能是卷积神经网络的卷积层在对原始数据的处理上类似于卷积操作。但是如果本身对数学上的卷积操作不怎么了解，其实也不影响对卷积神经网络的理解。<br><a id="more"></a></p>
<h2 id="神经网络处理图片的问题"><a href="#神经网络处理图片的问题" class="headerlink" title="神经网络处理图片的问题"></a>神经网络处理图片的问题</h2><p>我们可以使用神经网络来处理图片，对图片进行学习分类。但是神经网络可能对处理尺寸较小的，不复杂的图片处理更擅长。对于具有相反特点的图片神经网络的学习效果就不理想了。其中的原因包括有神经网络会将图片的所有像素信息都传入到神经网络的输入层中，当图片过大或者需要进行更复杂的分类时会导致模型中出现大量的参数。这样直接导致的是计算量变得极大最终难以计算。而且为了进行更复杂的判断新增的隐藏又可能导致梯度消失的情况。</p>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>为了减少上面问题的影响，发展出了卷积神经网络。最主要的特点是添加了卷积层来对图片进行处理。卷积神经网络模型的样子如下：</p>
<p><img src="/image/simple_conv.png" width="50%" height="50%/"></p>
<p>简单说下上图中convolutionallayer就是卷积层，主要作用是通过过滤器来过滤出原始图片的特征，一般一个卷积层有多个过滤器所以也就能得到多个特征来。接下来是poolinglayer是池化层主要是将卷积层产生的特征信息进行采样以进一步减小图片的尺寸。接着将池化后的数据输入到全连接层，这里的全连接层就和神经网络是一样，接着是通过输出层得到分类结果。</p>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>在神经网络中我们一般使用sigmod作为激活函数，它的图像如下</p>
<p><img src="/image/sigmoid.png" width="50%" height="50%/"></p>
<p>这里可以看出来在偏离图像的中心点0越远则图像的变换的趋势越加缓慢，如果在初始化w和b的过程可能某些神经元获得了一个更大的值。这就引起了梯度的下降变的缓慢解决的方式可以选择换一个激活函数比如relu函数，它的图像如下：</p>
<p><img src="/image/relu.png" width="50%" height="50%"></p>
<h3 id="代价函数-sigmoid-交叉熵"><a href="#代价函数-sigmoid-交叉熵" class="headerlink" title="代价函数 sigmoid+交叉熵"></a>代价函数 sigmoid+交叉熵</h3><p>在先前使用sigmod作为激活函数的神经网络中我们使用代价函数的模式一般是平方差的形式。代价函数的形式是会依赖对激活函数求导，我们知道求导对应到曲线可以理解为曲线在某点的变化率。以下通过一个一个输入一个输出的单个神经元模型来进行解释。以下输入假设x是1，期待y输出为0可以看出求导的结果确实依赖了对sigmod的求导，这样就会出现上面讨论的梯度下降缓慢的问题。<br>$$ C=\frac{(y-a)^{2}}{2} $$</p>
<p>$$ \frac{\partial C}{\partial w}=(a-y){\sigma}’(z)x=a{\sigma}’(z) $$</p>
<p>$$ \frac{\partial C}{\partial b}=(a-y){\sigma}’(z)=a{\sigma}’(z) $$</p>
<p>通过使用交叉熵来作为代价函数：</p>
<p>$$ C=-\frac{1}{n}\sum_{x}[ylna+(1-y)ln(1-a)] $$</p>
<p>可以对w和b分别求导发现结果是和激活函数的导数没有关系的。</p>
<h3 id="softmax-log-likelihood"><a href="#softmax-log-likelihood" class="headerlink" title="softmax+log-likelihood"></a>softmax+log-likelihood</h3><p>除了交叉熵以外还可以通过softmax的方式来解决学习速度衰减的问题。我们仅将输出层从普通的sigmod作为激活函数的层替换为softmax层。softmax输出层同样接受z=wx+b然后通过以下公式来计算输出结果</p>
<p>$$ a_{j}^{L}=\frac{e^{z_{j}^{L}}}{\sum_{k}e^{z_{k}^{L}}} $$</p>
<p>可以看出来这里得到的是某个值占总体的一个比例。配合softmax我们的代价函数需要替换成log-likelihood</p>
<p>$$ C\equiv-lna_{y}^{L} $$</p>
<p>这里表示的是单个输入样本的代价，如果有多个样本的可以对他们的代价求均值，作为总的代价函数。通过代价函数对w和b求导得到公式</p>
<p>$$ \frac{\partial C}{\partial b_{j}^{L}}=a_{j}^{L}-y_{j} $$</p>
<p>$$ \frac{\partial C}{\partial w_{jk}^{L}}=a_{k}^{L-1}(a_{k}^{L}-y_{j}) $$</p>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>一般有l1,l2,dropout的方式来对模型进行正则化，主要目的还是防止模型的过拟合，其中l2正则化方法是对所有的w进行如下处理并把这个部分添加到代价函数中去需要注意的是l2正则化只需包括w不需要包括b。</p>
<p>$$ C=-\frac{1}{n}\sum_{xj}[y_{j}lna_{j}^{L}+(1-y_{j})ln(1-a_{j}^{L})]+\frac{\lambda}{2n}\sum_{w}w^{2} $$</p>
<p>这样起到的作用使的优化这个代价函数，会更倾向于获取一个w并不是那么复杂的模型。一般直观的来看过拟合的模型都是在训练集中表现过于好的函数，而表现的过好的模型一般w都是比较复杂的。我们对于l1正则化不作解释。</p>
<p>接下还有一种方式是dropout，可以理解为丢弃。处理的思想类似我们以前看的集成学习法。这dropout中会随机的丢弃一些神经元(非输入和输出层)，也就是我们每次迭代更新梯度只对模型中的部分神经元进行处理。在一个batch的样本训练并更新完w和b以后我们会重新再进行一次dropout。这样的感觉就像训练了多个子神经网络，最后将他们组合成一个大的神经元来进行使用。是不是和集成学习思想类似？</p>
<h3 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h3><p>在先前的神经网络中一般采用的是符合一个均值是0，标准差是1的高斯分布的随机数去初始化w和b，但是这个并不适合初始化隐藏层比较多的神经网络。一般是将标准差表示为 $1/\sqrt{n_{in}}$ 这里的 $n_{in}$ 表示具有输入权重的神经元个数。</p>
<h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>卷积层是卷积神经网络不同于神经网络的一个重要特点。首先我们输入到卷积神经网络的依然是一个图片，但是作为接受这个数据的卷积层并不是如果神经网络一样将图片的所有像素信息都输入，而是使用一个比输入图片小的多的过滤器来扫描原始输入的图片数据。过滤器可以理解为将原始图片的一小部分作为输入并通过权值和偏移进行计算后输入激活函数得到在新的数据。这个过滤器会按照设置每次滑动并重复刚才的过程。最终得到的就是对应一个过滤器得到的一个特征map。这里需要特别说明单独过滤器滑动的任何区域计算使用的w和b都是一样的，一般的讲这个可以称为<strong>参数共享</strong>。这样的好处是识别某个特征将和这个特征在原始图片出现的位置无关。</p>
<p><img src="/image/no_padding.gif" width="50%" height="50%/"></p>
<p>可以直观的看到经过过滤器处理以后的图像的大小变小了，同时多个过滤器也可以获得到图片的多种特征。同时可以看出来我们需要仔细设计过滤器的大小以及每次滑动的距离以使的在原始图像中每个像素都可以涉及到。但是可能真就存在冲突的情况或者我希望过滤器处理后图片的大小不发生改变，这个时候可以适当的在待过滤的图片边缘添加0来实现。</p>
<p><img src="/image/padding.gif" width="50%" height="50%/"></p>
<p>这些处理得到特征map一般还需要通过池化来进行采样，其中一个目的就是进一步减小图片大小。一般的池化方法就是最大池化。比如使用2*2大小的框格在特制map上每次移动2格，将22在特征map范围上的最大的数据返回。这样处理以后的数据可能是作为下一个卷积层的输入也可能作为全连接层的输入。</p>
<p>在实际中卷积层的输入可能是多个通道的输入，输出也可能是多个通道的。这个时候针对输入的每个通道都会有一个卷积核然后各个输入通道卷积核每次滑动得到的结果相加得到输出的某一层的特征值。如果是多个输出通道那么也就需要多组输入的卷积核。</p>
<p><img src="/image/mulMap.png"></p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> cnn </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
